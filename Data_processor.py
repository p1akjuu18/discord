import pandas as pd
import numpy as np
import os
import re
from pathlib import Path
import ast
from datetime import datetime
import json
import glob
import argparse
import pytz

# åˆ—åæ˜ å°„å­—å…¸
COLUMN_NAME_MAPPING = {
    # ä¸»è¦åˆ†ç±»
    'entry_results': 'å…¥åœºç»“æœ',
    'entry_points_info': 'å…¥åœºç‚¹ä¿¡æ¯',
    'total_profit_details': 'æ€»æ”¶ç›Šè¯¦æƒ…',
    'total': 'æ€»è®¡',
    
    # å…¥åœºç»“æœç›¸å…³
    'entry_point': 'å…¥åœºç‚¹',
    'entry_price': 'å…¥åœºä»·æ ¼',
    'weight': 'æƒé‡',
    'status': 'çŠ¶æ€',
    'entry_hit': 'å…¥åœºå‘½ä¸­',
    'actual_entry_time': 'å®é™…å…¥åœºæ—¶é—´',
    'outcome': 'ç»“æœ',
    'tp_results': 'æ­¢ç›ˆç»“æœ',
    'tp_price': 'æ­¢ç›ˆä»·æ ¼',
    'tp_weight': 'æ­¢ç›ˆæƒé‡',
    'profit_pct': 'æ”¶ç›Šç‡',
    'exit_time': 'é€€å‡ºæ—¶é—´',
    'step': 'æ­¥éª¤',
    'total_profit_pct': 'æ€»æ”¶ç›Šç‡',
    'remaining_weight': 'å‰©ä½™æƒé‡',
    'exit_price': 'é€€å‡ºä»·æ ¼',
    'weighted_profit': 'åŠ æƒæ”¶ç›Š',
    'holding_period_minutes': 'æŒä»“æ—¶é—´',
    'risk_reward_ratio': 'é£é™©æ”¶ç›Šæ¯”',
    
    # å…¥åœºç‚¹ä¿¡æ¯ç›¸å…³
    'target_price': 'ç›®æ ‡ä»·æ ¼',
    'actual_entry': 'å®é™…å…¥åœº',
    'entry_time': 'å…¥åœºæ—¶é—´',
    'actual_price': 'å®é™…ä»·æ ¼',
    
    # æ”¶ç›Šè¯¦æƒ…ç›¸å…³
    'tp': 'æ­¢ç›ˆ',
    'sl': 'æ­¢æŸ',
    'open': 'å¼€ä»“',
    'profit': 'æ”¶ç›Š',
    'details': 'è¯¦æƒ…',
    
    # é€šç”¨å­—æ®µ
    'price': 'ä»·æ ¼',
    'time': 'æ—¶é—´',
    'volume': 'æˆäº¤é‡',
    'side': 'æ–¹å‘',
    'position': 'ä»“ä½'
}

def get_chinese_column_name(eng_name):
    """
    å°†è‹±æ–‡åˆ—åè½¬æ¢ä¸ºä¸­æ–‡åˆ—å
    """
    parts = eng_name.split('_')
    translated_parts = []
    i = 0
    
    while i < len(parts):
        # å¤„ç†æ•°å­—éƒ¨åˆ†
        if parts[i].isdigit():
            translated_parts.append(parts[i])
            i += 1
            continue
            
        # å°è¯•ç»„åˆå¤šä¸ªéƒ¨åˆ†è¿›è¡Œç¿»è¯‘
        for j in range(len(parts), i, -1):
            combined = '_'.join(parts[i:j])
            if combined in COLUMN_NAME_MAPPING:
                translated_parts.append(COLUMN_NAME_MAPPING[combined])
                i = j
                break
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»„åˆåŒ¹é…ï¼Œå°è¯•å•ä¸ªéƒ¨åˆ†
            if parts[i] in COLUMN_NAME_MAPPING:
                translated_parts.append(COLUMN_NAME_MAPPING[parts[i]])
            else:
                translated_parts.append(parts[i])
            i += 1
    
    return ''.join(translated_parts)

def clean_numeric_value(value):
    """
    æ¸…ç†æ•°å€¼ï¼Œå»é™¤å¯èƒ½æ®‹ç•™çš„å­—å…¸æˆ–åˆ—è¡¨ç»“æŸç¬¦å·
    """
    if isinstance(value, str):
        # å»é™¤ç»“å°¾å¯èƒ½çš„ } æˆ– ]
        value = value.rstrip('}]')
        try:
            return float(value)
        except:
            return value
    return value

def process_backtest_results(file_path):
    """
    è¯»å–å›æµ‹ç»“æœæ–‡ä»¶å¹¶å¤„ç†åµŒå¥—æ•°æ®ç»“æ„
    """
    # ç¡®å®šæ–‡ä»¶ç±»å‹å¹¶è¯»å–
    if file_path.endswith('.csv'):
        df = pd.read_csv(file_path)
    elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):
        df = pd.read_excel(file_path)
    else:
        raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·æä¾›CSVæˆ–Excelæ–‡ä»¶")
    
    # å¤åˆ¶åŸå§‹æ•°æ®æ¡†
    processed_df = df.copy()
    invalid_df = df.copy()  # åˆ›å»ºä¸€ä¸ªå‰¯æœ¬ç”¨äºå­˜å‚¨æ— æ•ˆæ•°æ®
    
    # éœ€è¦å¤„ç†çš„åµŒå¥—åˆ—å
    nested_columns = ['entry_results', 'entry_points_info', 'total_profit_details']
    
    # å¤„ç†æ¯ä¸ªåµŒå¥—åˆ—
    for col in nested_columns:
        if col in df.columns:
            try:
                # å±•å¹³åµŒå¥—æ•°æ®
                processed_df = flatten_nested_column(processed_df, col)
            except Exception as e:
                print(f"å¤„ç†åˆ— {col} æ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜channelåˆ—çš„åŸå§‹å€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    channel_col = None
    if 'channel' in processed_df.columns:
        channel_col = processed_df['channel'].copy()
    
    # è½¬æ¢æ‰€æœ‰åˆ—åä¸ºä¸­æ–‡
    processed_df.columns = [get_chinese_column_name(col) for col in processed_df.columns]
    
    # å¦‚æœä¹‹å‰å­˜åœ¨channelåˆ—ï¼Œæ¢å¤å®ƒ
    if channel_col is not None:
        processed_df['channel'] = channel_col
    
    # æ¸…ç†æ‰€æœ‰åŒ…å«"é£é™©æ”¶ç›Šæ¯”"ã€"å®é™…ä»·æ ¼"æˆ–"æŒä»“æ—¶é—´"çš„åˆ—
    for col in processed_df.columns:
        if 'é£é™©æ”¶ç›Šæ¯”' in col or 'å®é™…ä»·æ ¼' in col or 'æŒä»“æ—¶é—´' in col:
            processed_df[col] = processed_df[col].apply(clean_numeric_value)
    
    # åˆ†ç¦»æœ‰æ•ˆå’Œæ— æ•ˆæ•°æ®
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å…³é”®åˆ—æ¥åˆ¤æ–­æ•°æ®æ˜¯å¦æœ‰æ•ˆ
    key_columns = ['å…¥åœºç‚¹ä½1', 'æ­¢æŸç‚¹ä½1', 'æ­¢ç›ˆç‚¹ä½1', 'æ–¹å‘']
    has_valid_data = False
    
    for col in key_columns:
        if col in processed_df.columns:
            has_valid_data = True
            # å°†æ‰€æœ‰éç©ºå€¼çš„è¡Œæ ‡è®°ä¸ºæœ‰æ•ˆæ•°æ®
            valid_mask = processed_df[col].notna()
            processed_df = processed_df[valid_mask]
            invalid_df = invalid_df[~valid_mask]
            break
    
    if not has_valid_data:
        print("è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•å…³é”®åˆ—æ¥åˆ¤æ–­æ•°æ®æœ‰æ•ˆæ€§")
        return processed_df, None
    
    return processed_df, invalid_df

def direct_parser(s):
    """
    ç›´æ¥è§£æåµŒå¥—æ•°æ®ç»“æ„çš„å­—ç¬¦ä¸²è¡¨ç¤º
    """
    if not isinstance(s, str):
        return s
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå­—å…¸æ ¼å¼ï¼ˆå•ä¸ªå­—å…¸è€Œéåˆ—è¡¨ï¼‰
    if s.startswith('{') and s.endswith('}'):
        try:
            # è§£æå•ä¸ªå­—å…¸
            dict_obj = {}
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…é”®å€¼å¯¹
            key_value_pattern = r"'([^']+)':\s*([^,]+)"
            pairs = re.findall(key_value_pattern, s)
            
            for key, value in pairs:
                # å¤„ç†ä¸åŒç±»å‹çš„å€¼
                if value.strip() == 'True':
                    dict_obj[key] = True
                elif value.strip() == 'False':
                    dict_obj[key] = False
                elif value.strip() == 'None':
                    dict_obj[key] = None
                elif 'np.float64' in value:
                    # æå–np.float64ä¸­çš„æ•°å€¼
                    match = re.search(r'np\.float64\(([^)]+)\)', value)
                    if match:
                        dict_obj[key] = float(match.group(1))
                    else:
                        dict_obj[key] = value
                elif 'Timestamp' in value:
                    # æå–Timestampä¸­çš„æ—¥æœŸæ—¶é—´
                    match = re.search(r"Timestamp\('([^']+)'\)", value)
                    if match:
                        try:
                            dict_obj[key] = datetime.fromisoformat(match.group(1))
                        except:
                            dict_obj[key] = match.group(1)
                    else:
                        dict_obj[key] = value
                else:
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                    try:
                        dict_obj[key] = float(value)
                    except:
                        dict_obj[key] = value.strip("' ")
                        
            return dict_obj
        except Exception as e:
            print(f"è§£æå­—å…¸é”™è¯¯: {e}")
            return s
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ—è¡¨æ ¼å¼çš„å­—ç¬¦ä¸²
    if not (s.startswith('[') and s.endswith(']')):
        return s
    
    try:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¯†åˆ«å¹¶æå–å­—å…¸å¯¹è±¡
        dict_pattern = r'\{([^{}]*(?:\{[^{}]*\}[^{}]*)*)\}'
        dicts_found = re.findall(dict_pattern, s)
        
        result_list = []
        
        for dict_str in dicts_found:
            # è§£ææ¯ä¸ªå­—å…¸
            dict_obj = {}
            
            # ç®€åŒ–: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…é”®å€¼å¯¹
            key_value_pattern = r"'([^']+)':\s*([^,]+)"
            pairs = re.findall(key_value_pattern, '{' + dict_str + '}')
            
            for key, value in pairs:
                # å¤„ç†ä¸åŒç±»å‹çš„å€¼
                if value.strip() == 'True':
                    dict_obj[key] = True
                elif value.strip() == 'False':
                    dict_obj[key] = False
                elif value.strip() == 'None':
                    dict_obj[key] = None
                elif 'np.float64' in value:
                    # æå–np.float64ä¸­çš„æ•°å€¼
                    match = re.search(r'np\.float64\(([^)]+)\)', value)
                    if match:
                        dict_obj[key] = float(match.group(1))
                    else:
                        dict_obj[key] = value
                elif 'Timestamp' in value:
                    # æå–Timestampä¸­çš„æ—¥æœŸæ—¶é—´
                    match = re.search(r"Timestamp\('([^']+)'\)", value)
                    if match:
                        try:
                            dict_obj[key] = datetime.fromisoformat(match.group(1))
                        except:
                            dict_obj[key] = match.group(1)
                    else:
                        dict_obj[key] = value
                else:
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                    try:
                        dict_obj[key] = float(value)
                    except:
                        dict_obj[key] = value.strip("' ")
            
            result_list.append(dict_obj)
        
        return result_list
    except Exception as e:
        print(f"è§£æåˆ—è¡¨é”™è¯¯: {e}")
        return s

def flatten_nested_column(df, col_name):
    """
    å°†åµŒå¥—åˆ—å±•å¹³ä¸ºå¤šä¸ªå•ç‹¬çš„åˆ—
    """
    # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æ•°æ®
    if col_name not in df.columns or df[col_name].isnull().all():
        return df
    
    # ä½¿ç”¨ç›´æ¥è§£æå™¨è§£æåµŒå¥—æ•°æ®
    df[col_name] = df[col_name].apply(direct_parser)
    
    # æ‰“å°ä¸€äº›è§£æåçš„æ ·æœ¬ï¼Œç”¨äºè°ƒè¯•
    print(f"è§£æåçš„ {col_name} æ ·æœ¬:")
    first_valid_index = df[col_name].first_valid_index()
    if first_valid_index is not None:
        print(f"ç±»å‹: {type(df.loc[first_valid_index, col_name])}")
        print(df.loc[first_valid_index, col_name])
    
    # å¤„ç†å•ä¸ªå­—å…¸çš„æƒ…å†µ
    if df[col_name].apply(lambda x: isinstance(x, dict)).any():
        print(f"æ£€æµ‹åˆ° {col_name} åˆ—åŒ…å«å­—å…¸æ•°æ®ï¼Œæ­£åœ¨å¤„ç†...")
        # è·å–ä¸€ä¸ªæ ·æœ¬å­—å…¸ç”¨äºæå–é”®
        sample_dict = df.loc[df[col_name].apply(lambda x: isinstance(x, dict))].iloc[0][col_name]
        
        # ä¸ºå­—å…¸ä¸­çš„æ¯ä¸ªé”®åˆ›å»ºæ–°åˆ—
        for key in sample_dict.keys():
            new_col_name = f"{col_name}_{key}"
            df[new_col_name] = df[col_name].apply(lambda x: x.get(key) if isinstance(x, dict) else None)
            # æ‰“å°åˆ›å»ºçš„åˆ—ä¿¡æ¯
            print(f"åˆ›å»ºåˆ— {new_col_name}, æ ·æœ¬å€¼: {df[new_col_name].iloc[0]}")
        
        # åˆ é™¤åŸå§‹å­—å…¸åˆ—
        df = df.drop(columns=[col_name])
        return df
    
    # å¤„ç†åˆ—è¡¨æ•°æ®
    valid_list_entries = df[col_name].apply(lambda x: isinstance(x, list) and len(x) > 0)
    if not valid_list_entries.any():
        print(f"è­¦å‘Š: {col_name} åˆ—æ²¡æœ‰æœ‰æ•ˆçš„åˆ—è¡¨æ•°æ®")
        return df
    
    # è·å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„åˆ—è¡¨å…ƒç´ ä½œä¸ºæ¨¡æ¿
    template_index = valid_list_entries[valid_list_entries].index[0]
    first_elem = df.loc[template_index, col_name]
    
    # å¯¹äºåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ ä½ç½®åˆ›å»ºæ–°åˆ—
    for i in range(len(first_elem)):
        prefix = f"{col_name}_{i+1}"
        
        # åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æå–ç¬¬iä¸ªå…ƒç´ ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        def extract_element(row, idx):
            if isinstance(row, list) and len(row) > idx:
                return row[idx]
            return None
        
        # ä¸ºç¬¬iä¸ªå…ƒç´ åˆ›å»ºæ–°åˆ—
        df[prefix] = df[col_name].apply(lambda x: extract_element(x, i))
        
        # å¦‚æœå…ƒç´ æ˜¯å­—å…¸ï¼Œåˆ™ä¸ºå­—å…¸ä¸­çš„æ¯ä¸ªé”®åˆ›å»ºæ–°åˆ—
        sample_elem = extract_element(first_elem, i)
        if isinstance(sample_elem, dict):
            for key in sample_elem.keys():
                new_col_name = f"{prefix}_{key}"
                df[new_col_name] = df[prefix].apply(lambda x: x.get(key) if isinstance(x, dict) else None)
                # æ‰“å°åˆ›å»ºçš„åˆ—ä¿¡æ¯
                print(f"åˆ›å»ºåˆ— {new_col_name}, æ ·æœ¬å€¼: {df[new_col_name].iloc[0]}")
            
            # åˆ é™¤ä¸­é—´å­—å…¸åˆ—
            df = df.drop(columns=[prefix])
    
    # åˆ é™¤åŸå§‹åµŒå¥—åˆ—
    df = df.drop(columns=[col_name])
    
    return df

# ä»¥ä¸‹ä¸ºæ¨¡å—1: å›æµ‹ç»“æœå¤„ç†å‡½æ•°
def process_backtest_results_main():
    # è·å–æ¡Œé¢è·¯å¾„
    desktop_path = Path(os.path.join(os.path.expanduser("~"), "Desktop"))
    
    # å›ºå®šæ–‡ä»¶åä¸º"å›æµ‹ç»“æœ.xlsx"
    file_name = "å›æµ‹ç»“æœ.xlsx"
    output_file_name = f"processed_{file_name}"
    invalid_file_name = f"unprocessed_{file_name}"
    
    file_path = desktop_path / file_name
    output_path = desktop_path / output_file_name
    invalid_path = desktop_path / invalid_file_name
    
    if not file_path.exists():
        print(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨!")
        return
    
    # å¤„ç†æ–‡ä»¶
    try:
        # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œå°è¯•åˆ é™¤å®ƒä»¬
        for path in [output_path, invalid_path]:
            if path.exists():
                try:
                    os.remove(path)
                    print(f"å·²åˆ é™¤ç°æœ‰çš„è¾“å‡ºæ–‡ä»¶: {path}")
                except Exception as e:
                    print(f"æ— æ³•åˆ é™¤ç°æœ‰çš„è¾“å‡ºæ–‡ä»¶: {e}")
                    print("è¯·ç¡®ä¿æ–‡ä»¶æœªè¢«å…¶ä»–ç¨‹åºæ‰“å¼€ï¼Œå¹¶ä¸”æ‚¨æœ‰è¶³å¤Ÿçš„æƒé™ã€‚")
                    return
        
        # è¯»å–åŸå§‹æ–‡ä»¶ç”¨äºç»Ÿè®¡åˆ—æ•°
        if file_path.suffix == '.csv':
            original_df = pd.read_csv(file_path)
        else:
            original_df = pd.read_excel(file_path)
            
        # å¤„ç†æ–‡ä»¶
        result_df, invalid_df = process_backtest_results(str(file_path))
        
        # å°è¯•ä¿å­˜å¤„ç†åçš„æœ‰æ•ˆæ•°æ®
        temp_output_path = desktop_path / f"temp_{output_file_name}"
        if file_path.suffix == '.csv':
            result_df.to_csv(temp_output_path, index=False)
        else:
            result_df.to_excel(temp_output_path, index=False)
        
        # å¦‚æœæœ‰æ— æ•ˆæ•°æ®ï¼Œä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶
        if invalid_df is not None and not invalid_df.empty:
            temp_invalid_path = desktop_path / f"temp_{invalid_file_name}"
            if file_path.suffix == '.csv':
                invalid_df.to_csv(temp_invalid_path, index=False)
            else:
                invalid_df.to_excel(temp_invalid_path, index=False)
        
        # å¦‚æœä¸´æ—¶æ–‡ä»¶ä¿å­˜æˆåŠŸï¼Œé‡å‘½åä¸ºæœ€ç»ˆæ–‡ä»¶å
        try:
            if temp_output_path.exists():
                if output_path.exists():
                    os.remove(output_path)
                os.rename(temp_output_path, output_path)
                print(f"æœ‰æ•ˆæ•°æ®å·²ä¿å­˜è‡³ï¼š{output_path}")
                print(f"åŸå§‹åˆ—æ•°: {len(original_df.columns)}, å¤„ç†ååˆ—æ•°: {len(result_df.columns)}")
            
            if invalid_df is not None and not invalid_df.empty and temp_invalid_path.exists():
                if invalid_path.exists():
                    os.remove(invalid_path)
                os.rename(temp_invalid_path, invalid_path)
                print(f"æœªå¤„ç†çš„æ•°æ®å·²ä¿å­˜è‡³ï¼š{invalid_path}")
                print(f"æœªå¤„ç†æ•°æ®è¡Œæ•°: {len(invalid_df)}")
        except Exception as e:
            print(f"é‡å‘½åæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            if temp_output_path.exists():
                print(f"å¤„ç†åçš„æ–‡ä»¶å·²ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶: {temp_output_path}")
            if invalid_df is not None and not invalid_df.empty and temp_invalid_path.exists():
                print(f"æœªå¤„ç†çš„æ•°æ®å·²ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶: {temp_invalid_path}")
            
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

# æ¨¡å—2: Discordæ¶ˆæ¯å¤„ç†å‡½æ•°
def process_discord_messages_main():
    # è¯»å–JSONæ–‡ä»¶
    desktop = os.path.join(os.path.expanduser('~'), 'Desktop')
    target_file = os.path.join(desktop, 'ğ‘¾ğ‘¾ğ‘®-ğ‘¬ğ‘³ğ‘°ğ’ [1224357517321048248].json')
    
    if not os.path.exists(target_file):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {target_file}")
        return
    
    try:
        # è¯»å–JSONæ–‡ä»¶
        with open(target_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ç­›é€‰æ¶ˆæ¯
        filtered_messages = []
        for message in data.get('messages', []):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æƒ…æˆ–è€…ç‰¹å®šå†…å®¹
            has_emojis = bool(message.get('inlineEmojis'))
            has_strategy = 'Elizäº¤æ˜“ç­–ç•¥' in message.get('content', '')
            
            if has_emojis or has_strategy:
                filtered_message = {
                    'id': message['id'],
                    'timestamp': message['timestamp'],
                    'content': message['content'],
                    'author': message['author']['name'],
                    'has_emojis': has_emojis,
                    'has_strategy': has_strategy,
                    'inlineEmojis': message.get('inlineEmojis', [])
                }
                filtered_messages.append(filtered_message)
        
        # å°†ç»“æœä¿å­˜åˆ°æ¡Œé¢
        output_file = os.path.join(desktop, 'filtered_messages.json')
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(filtered_messages, f, ensure_ascii=False, indent=2)
        
        # ç»Ÿè®¡ä¿¡æ¯
        emoji_count = sum(1 for msg in filtered_messages if msg['has_emojis'])
        strategy_count = sum(1 for msg in filtered_messages if msg['has_strategy'])
        
        print(f"ç­›é€‰ç»“æœï¼š")
        print(f"- åŒ…å«è¡¨æƒ…çš„æ¶ˆæ¯ï¼š{emoji_count} æ¡")
        print(f"- åŒ…å«äº¤æ˜“ç­–ç•¥çš„æ¶ˆæ¯ï¼š{strategy_count} æ¡")
        print(f"- æ€»å…±ç­›é€‰å‡ºï¼š{len(filtered_messages)} æ¡æ¶ˆæ¯")
        print(f"ç»“æœå·²ä¿å­˜åˆ°ï¼š{output_file}")
    
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

# æ¨¡å—3: åŠ å¯†è´§å¸å¸‚åœºæ•°æ®å‡†å¤‡
def prepare_market_data_main():
    crypto_folder_path = input("è¯·è¾“å…¥åŠ å¯†è´§å¸æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤ä¸ºæ¡Œé¢ä¸Šçš„cryptoæ–‡ä»¶å¤¹): ")
    if not crypto_folder_path:
        desktop_path = os.path.expanduser('~') + '/Desktop'
        crypto_folder_path = os.path.join(desktop_path, 'crypto')
    
    output_file = input("è¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶å (é»˜è®¤ä¸ºmarket_data.csv): ")
    if not output_file:
        output_file = 'market_data.csv'
    
    if not os.path.exists(crypto_folder_path):
        print(f"é”™è¯¯: æ–‡ä»¶å¤¹ {crypto_folder_path} ä¸å­˜åœ¨")
        return
    
    try:
        result = prepare_market_data(crypto_folder_path, output_file)
        if result is not None:
            print(f"å¤„ç†å®Œæˆï¼Œå…±åˆå¹¶ {len(result['symbol'].unique())} ç§åŠ å¯†è´§å¸çš„æ•°æ®")
    except Exception as e:
        print(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

# æ¨¡å—4: Excelåˆ†æç»“æ„ä¿®å¤
def fix_analysis_structure_main():
    # è®¾ç½®æ¡Œé¢ä¸Šçš„result3æ–‡ä»¶
    desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
    target_file = os.path.join(desktop_path, "result3.xlsx")
    output_file = os.path.join(desktop_path, "result3_fixed_analysis.xlsx")
    
    if not os.path.exists(target_file):
        print(f"æ–‡ä»¶ {target_file} ä¸å­˜åœ¨!")
        return
    
    try:
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(target_file)
        
        # å¤„ç†æ•°æ®
        processed_df = process_excel_structure(df)
        
        # ä¿å­˜å¤„ç†åçš„æ–‡ä»¶
        processed_df.to_excel(output_file, index=False)
        print(f"æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def process_excel_structure(df):
    """
    å¤„ç†Excelæ–‡ä»¶çš„ç»“æ„é—®é¢˜
    """
    # å¤åˆ¶æ•°æ®æ¡†
    processed_df = df.copy()
    
    # å¤„ç†åˆ—å
    processed_df.columns = [col.strip() for col in processed_df.columns]
    
    # å¤„ç†ç©ºå€¼
    processed_df = processed_df.fillna('')
    
    # å¤„ç†æ•°æ®ç±»å‹
    for col in processed_df.columns:
        if 'date' in col.lower() or 'time' in col.lower():
            try:
                processed_df[col] = pd.to_datetime(processed_df[col])
            except:
                pass
    
    return processed_df

# æ¨¡å—5: åŠ å¯†è´§å¸æ•°æ®å¤„ç†ä¸æ—¶é—´ç¼ºå£æ£€æŸ¥
def process_crypto_data_main():
    # å®šä¹‰æ•°æ®æºè·¯å¾„ï¼ˆæ¡Œé¢ä¸Šçš„crypto_dataæ–‡ä»¶å¤¹ï¼‰
    desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop')
    data_folder = os.path.join(desktop_path, 'crypto_data')
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦è‡ªå®šä¹‰ç›®å½•
    custom_dir = input(f"é»˜è®¤å¤„ç†ç›®å½•ä¸º: {data_folder}\næ˜¯å¦è¦è‡ªå®šä¹‰ç›®å½•? (y/n, é»˜è®¤n): ").lower()
    if custom_dir == 'y':
        data_folder = input("è¯·è¾“å…¥ç›®å½•è·¯å¾„: ")
        if not os.path.exists(data_folder):
            print(f"ç›®å½• {data_folder} ä¸å­˜åœ¨!")
            return
    
    output_folder = os.path.join(data_folder, 'processed_data')
    
    # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    # è¦å¤„ç†çš„åŠ å¯†è´§å¸åˆ—è¡¨
    crypto_files = []
    custom_crypto = input("è¯·è¾“å…¥è¦å¤„ç†çš„åŠ å¯†è´§å¸æ–‡ä»¶åï¼Œç”¨é€—å·åˆ†éš” (é»˜è®¤ä¸ºbtcusdt_history,ethusdt_history,solusdt_history): ")
    if custom_crypto:
        crypto_files = [name.strip() for name in custom_crypto.split(',')]
    else:
        crypto_files = [
            'btcusdt_history',
            'ethusdt_history',
            'solusdt_history'
        ]
    
    # å¤„ç†æ‰€æœ‰åŠ å¯†è´§å¸æ•°æ®
    all_results = []
    for crypto_file in crypto_files:
        result = process_crypto_data(crypto_file)
        if result:
            all_results.append(result)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("\n===== æ•°æ®å®Œæ•´æ€§æŠ¥å‘Š =====")
    all_gaps = []
    
    for result in all_results:
        filename = result["filename"]
        start_date = result["start_date"]
        end_date = result["end_date"]
        gaps = result["gaps"]
        
        print(f"\n{filename}:")
        print(f"  æ•°æ®èŒƒå›´: {start_date} è‡³ {end_date}")
        print(f"  ç¼ºå¤±æ•°é‡: {len(gaps)} å¤„")
        
        if gaps:
            total_missing_minutes = sum(minutes for _, _, minutes in gaps)
            total_duration = (end_date - start_date).total_seconds() / 60
            missing_percentage = (total_missing_minutes / total_duration) * 100
            
            print(f"  æ€»ç¼ºå¤±æ—¶é—´: {total_missing_minutes} åˆ†é’Ÿ (çº¦ {missing_percentage:.2f}% çš„æ•°æ®)")
            print("\n  è¯¦ç»†ç¼ºå¤±æ—¶æ®µ:")
            for i, (start, end, minutes) in enumerate(gaps):
                print(f"    {i+1}. {start} è‡³ {end} (çº¦ {minutes} åˆ†é’Ÿ)")
                # æ”¶é›†æ‰€æœ‰ç¼ºå¤±æ•°æ®ç”¨äºè¡¨æ ¼è¾“å‡º
                all_gaps.append({
                    "å¸ç§": filename,
                    "å¼€å§‹æ—¶é—´": start,
                    "ç»“æŸæ—¶é—´": end,
                    "ç¼ºå¤±åˆ†é’Ÿæ•°": minutes
                })
    
    # å°†ç¼ºå¤±æ•°æ®ä¿å­˜åˆ°CSVæ–‡ä»¶
    if all_gaps:
        # åˆ›å»ºDataFrame
        gaps_df = pd.DataFrame(all_gaps)
        
        # ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶å¤¹ä¸­çš„CSVæ–‡ä»¶
        gaps_csv_path = os.path.join(output_folder, f"crypto_data_gaps.csv")
        gaps_df.to_csv(gaps_csv_path, index=False, encoding='utf-8-sig')
        
        print(f"\nå·²å°†ç¼ºå¤±æ•°æ®ä¿å­˜åˆ°è¡¨æ ¼: {gaps_csv_path}")
    else:
        print("\næ‰€æœ‰æ•°æ®å®Œæ•´ï¼Œæ— ç¼ºå¤±ã€‚")
    
    print("\næ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ°:", output_folder)

# æ¨¡å—6: åˆå¹¶è¡¨æ–‡ä»¶å¤„ç†ç¨‹åº
def process_merged_tables_main():
    # è®¾ç½®æ–‡ä»¶å¤¹è·¯å¾„
    desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
    data_dir = os.path.join(desktop_path, "historydata")
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦è‡ªå®šä¹‰ç›®å½•
    custom_dir = input(f"é»˜è®¤å¤„ç†ç›®å½•ä¸º: {data_dir}\næ˜¯å¦è¦è‡ªå®šä¹‰ç›®å½•? (y/n, é»˜è®¤n): ").lower()
    if custom_dir == 'y':
        data_dir = input("è¯·è¾“å…¥ç›®å½•è·¯å¾„: ")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not os.path.exists(data_dir):
        print(f"é”™è¯¯: ç›®å½• {data_dir} ä¸å­˜åœ¨!")
        os.makedirs(data_dir)
        print(f"å·²åˆ›å»ºç›®å½•: {data_dir}")
        print("è¯·å°†æ•°æ®æ–‡ä»¶æ”¾å…¥è¯¥ç›®å½•ï¼Œç„¶åé‡æ–°è¿è¡Œç¨‹åºã€‚")
        return
    
    # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
    files = list_files(data_dir)
    
    if not files:
        print("æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶!")
        return
    
    # è®©ç”¨æˆ·é€‰æ‹©æ–‡ä»¶
    while True:
        try:
            choice = int(input("\nè¯·é€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶ç¼–å· (0é€€å‡º): "))
            if choice == 0:
                return
            if 1 <= choice <= len(files):
                break
            print(f"è¯·è¾“å…¥1åˆ°{len(files)}ä¹‹é—´çš„æ•°å­—!")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—!")
    
    selected_file = files[choice-1]
    print(f"\nå·²é€‰æ‹©: {selected_file}")
    
    # è¯»å–æ•°æ®
    df = read_file(selected_file)
    
    # å¤‡ä»½åŸå§‹æ•°æ®ï¼Œç”¨äºåç»­æå–æ–¹å‘ä¸ºNaNçš„è¡Œ
    original_df = df.copy() if df is not None else None
    
    # å¤„ç†æ•°æ®
    processed_df = process_data(df)
    
    if processed_df is not None:
        # ç”Ÿæˆé»˜è®¤çš„è¾“å‡ºæ–‡ä»¶åï¼ˆåŸºäºåŸå§‹æ–‡ä»¶åï¼‰
        original_filename = os.path.basename(selected_file)
        file_name, file_ext = os.path.splitext(original_filename)
        if not file_ext or file_ext.lower() not in ['.csv', '.xlsx', '.xls']:
            file_ext = '.csv'  # é»˜è®¤CSVæ ¼å¼
        
        default_output_name = f"{file_name}_processed{file_ext}"
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦ä¿å­˜å¤„ç†åçš„æ•°æ®ï¼ˆé»˜è®¤æ˜¯ï¼‰
        save_choice = input(f"\næ˜¯å¦è¦ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°æ–°æ–‡ä»¶? (y/n, é»˜è®¤y): ").lower()
        if save_choice == '' or save_choice == 'y':
            output_name = input(f"è¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: {default_output_name}): ")
            if not output_name:
                output_name = default_output_name
            
            # ç¡®ä¿æ–‡ä»¶åæœ‰æ­£ç¡®çš„æ‰©å±•å
            if not any(output_name.lower().endswith(ext) for ext in ['.csv', '.xlsx', '.xls']):
                output_name += file_ext
            
            output_path = os.path.join(data_dir, output_name)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(output_path):
                overwrite = input(f"æ–‡ä»¶ {output_name} å·²å­˜åœ¨ã€‚æ˜¯å¦è¦†ç›–? (y/n): ").lower()
                if overwrite != 'y':
                    new_name = input("è¯·è¾“å…¥æ–°çš„æ–‡ä»¶å: ")
                    if not new_name:
                        print("æœªä¿å­˜æ–‡ä»¶")
                        return
                    
                    # ç¡®ä¿æ–‡ä»¶åæœ‰æ­£ç¡®çš„æ‰©å±•å
                    if not any(new_name.lower().endswith(ext) for ext in ['.csv', '.xlsx', '.xls']):
                        new_name += file_ext
                    
                    output_path = os.path.join(data_dir, new_name)
            
            save_processed_data(processed_df, output_path)
            print(f"\nå¤„ç†å®Œæˆ! åŸå§‹æ–‡ä»¶ä¿æŒä¸å˜ï¼Œå¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°æ–°æ–‡ä»¶: {os.path.basename(output_path)}")
        else:
            print("\næ•°æ®æœªä¿å­˜")
        
        # å¤„ç†æ–¹å‘åˆ—ä¸ºNaNçš„è¡Œ
        if original_df is not None:
            direction_column = None
            possible_direction_columns = ['æ–¹å‘', 'äº¤æ˜“æ–¹å‘', 'ä¹°å–æ–¹å‘', 'å¤šç©º', 'direction', 'Direction']
            
            for col in possible_direction_columns:
                if col in original_df.columns:
                    direction_column = col
                    break
            
            if direction_column:
                # æŸ¥æ‰¾æ–¹å‘åˆ—ä¸ºNaNçš„è¡Œ
                nan_direction_rows = original_df[original_df[direction_column].isna()]
                
                # å¦‚æœå­˜åœ¨æ–¹å‘åˆ—ä¸ºNaNçš„è¡Œï¼Œè¯¢é—®ç”¨æˆ·æ˜¯å¦è¦å•ç‹¬ä¿å­˜
                if not nan_direction_rows.empty:
                    print(f"\nå‘ç° {len(nan_direction_rows)} è¡Œçš„äº¤æ˜“æ–¹å‘åˆ— '{direction_column}' ä¸ºç©ºå€¼")
                    save_nan_choice = input("æ˜¯å¦è¦å°†è¿™äº›è¡Œå•ç‹¬ä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶? (y/n, é»˜è®¤y): ").lower()
                    
                    if save_nan_choice == '' or save_nan_choice == 'y':
                        nan_output_name = f"{file_name}_direction_empty{file_ext}"
                        nan_output_name = input(f"è¯·è¾“å…¥ä¿å­˜æ–¹å‘ä¸ºç©ºçš„è¡Œçš„æ–‡ä»¶å (é»˜è®¤: {nan_output_name}): ") or nan_output_name
                        
                        # ç¡®ä¿æ–‡ä»¶åæœ‰æ­£ç¡®çš„æ‰©å±•å
                        if not any(nan_output_name.lower().endswith(ext) for ext in ['.csv', '.xlsx', '.xls']):
                            nan_output_name += file_ext
                        
                        nan_output_path = os.path.join(data_dir, nan_output_name)
                        
                        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                        if os.path.exists(nan_output_path):
                            overwrite = input(f"æ–‡ä»¶ {nan_output_name} å·²å­˜åœ¨ã€‚æ˜¯å¦è¦†ç›–? (y/n): ").lower()
                            if overwrite != 'y':
                                new_name = input("è¯·è¾“å…¥æ–°çš„æ–‡ä»¶å: ")
                                if not new_name:
                                    print("æœªä¿å­˜æ–¹å‘ä¸ºç©ºçš„è¡Œ")
                                    return
                                
                                # ç¡®ä¿æ–‡ä»¶åæœ‰æ­£ç¡®çš„æ‰©å±•å
                                if not any(new_name.lower().endswith(ext) for ext in ['.csv', '.xlsx', '.xls']):
                                    new_name += file_ext
                                
                                nan_output_path = os.path.join(data_dir, new_name)
                        
                        # ä¿å­˜æ–¹å‘ä¸ºNaNçš„è¡Œ
                        save_processed_data(nan_direction_rows, nan_output_path)
                        print(f"æ–¹å‘ä¸ºç©ºçš„è¡Œå·²ä¿å­˜åˆ°: {os.path.basename(nan_output_path)}")
                else:
                    print("\næœªå‘ç°äº¤æ˜“æ–¹å‘åˆ—ä¸ºç©ºçš„è¡Œ")
            else:
                print("\næœªæ‰¾åˆ°äº¤æ˜“æ–¹å‘åˆ—ï¼Œæ— æ³•æå–æ–¹å‘ä¸ºç©ºçš„è¡Œ")

# æ¨¡å—7: ExcelåµŒå¥—æ•°æ®ä¿®å¤
def fix_nested_json_main():
    # è®¾ç½®æ¡Œé¢ä¸Šçš„result3æ–‡ä»¶
    desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
    target_file = os.path.join(desktop_path, "result3.xlsx")
    output_file = os.path.join(desktop_path, "result3_fixed_nested.xlsx")
    
    if not os.path.exists(target_file):
        print(f"æ–‡ä»¶ {target_file} ä¸å­˜åœ¨!")
        return
    
    try:
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(target_file)
        
        # å¤„ç†åµŒå¥—æ•°æ®
        processed_df = process_excel_nested_data(df)
        
        # ä¿å­˜å¤„ç†åçš„æ–‡ä»¶
        processed_df.to_excel(output_file, index=False)
        print(f"æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def process_excel_nested_data(df):
    """
    å¤„ç†Excelæ–‡ä»¶ä¸­çš„åµŒå¥—æ•°æ®
    """
    # å¤åˆ¶æ•°æ®æ¡†
    processed_df = df.copy()
    
    # å¤„ç†å¯èƒ½åŒ…å«åµŒå¥—æ•°æ®çš„åˆ—
    for col in processed_df.columns:
        if isinstance(processed_df[col].iloc[0], str) and ('{' in processed_df[col].iloc[0] or '[' in processed_df[col].iloc[0]):
            try:
                # å°è¯•è§£æåµŒå¥—æ•°æ®
                processed_df[col] = processed_df[col].apply(lambda x: eval(x) if isinstance(x, str) and (x.startswith('{') or x.startswith('[')) else x)
                
                # å¦‚æœæˆåŠŸè§£æï¼Œå±•å¼€åµŒå¥—æ•°æ®
                if isinstance(processed_df[col].iloc[0], dict):
                    new_cols = pd.json_normalize(processed_df[col])
                    processed_df = pd.concat([processed_df.drop(columns=[col]), new_cols], axis=1)
            except:
                pass
    
    return processed_df

# ä¸»èœå•å‡½æ•°
def main():
    while True:
        print("\n======================================================")
        print("                  æ•°æ®å¤„ç†å·¥å…·é›† v1.0                  ")
        print("======================================================")
        print("è¯·é€‰æ‹©è¦è¿è¡Œçš„æ¨¡å—:")
        print("1. å›æµ‹ç»“æœå¤„ç†å·¥å…· - å¤„ç†å›æµ‹ç»“æœæ•°æ®ç»“æ„")
        print("2. Discordæ¶ˆæ¯ç­›é€‰å·¥å…· - ç­›é€‰åŒ…å«è¡¨æƒ…æˆ–ç‰¹å®šå†…å®¹çš„æ¶ˆæ¯")
        print("3. åŠ å¯†è´§å¸å¸‚åœºæ•°æ®å‡†å¤‡å·¥å…· - æ•´åˆå¤šä¸ªåŠ å¯†è´§å¸å†å²æ•°æ®")
        print("4. Excelåˆ†æç»“æ„ä¿®å¤å·¥å…· - ä¿®å¤åˆ†æç»“æ„")
        print("5. åŠ å¯†è´§å¸æ•°æ®å¤„ç†ä¸æ—¶é—´ç¼ºå£æ£€æŸ¥å·¥å…· - æŒ‰æœˆæ‹†åˆ†æ•°æ®å¹¶æ£€æŸ¥ç¼ºå¤±")
        print("6. åˆå¹¶è¡¨æ–‡ä»¶å¤„ç†ç¨‹åº - å¤„ç†äº¤æ˜“æ•°æ®æ–‡ä»¶")
        print("7. ExcelåµŒå¥—æ•°æ®ä¿®å¤å·¥å…· - ä¿®å¤åµŒå¥—çš„Excelæ•°æ®")
        print("0. é€€å‡ºç¨‹åº")
        print("------------------------------------------------------")
        
        choice = input("è¯·è¾“å…¥æ¨¡å—ç¼–å· (0-7): ").strip()
        
        if choice == '0':
            print("\næ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼")
            break
        elif choice == '1':
            print("\næ­£åœ¨å¯åŠ¨å›æµ‹ç»“æœå¤„ç†å·¥å…·...\n")
            process_backtest_results_main()
        elif choice == '2':
            print("\næ­£åœ¨å¯åŠ¨Discordæ¶ˆæ¯ç­›é€‰å·¥å…·...\n")
            process_discord_messages_main()
        elif choice == '3':
            print("\næ­£åœ¨å¯åŠ¨åŠ å¯†è´§å¸å¸‚åœºæ•°æ®å‡†å¤‡å·¥å…·...\n")
            prepare_market_data_main()
        elif choice == '4':
            print("\næ­£åœ¨å¯åŠ¨Excelåˆ†æç»“æ„ä¿®å¤å·¥å…·...\n")
            fix_analysis_structure_main()
        elif choice == '5':
            print("\næ­£åœ¨å¯åŠ¨åŠ å¯†è´§å¸æ•°æ®å¤„ç†ä¸æ—¶é—´ç¼ºå£æ£€æŸ¥å·¥å…·...\n")
            process_crypto_data_main()
        elif choice == '6':
            print("\næ­£åœ¨å¯åŠ¨åˆå¹¶è¡¨æ–‡ä»¶å¤„ç†ç¨‹åº...\n")
            process_merged_tables_main()
        elif choice == '7':
            print("\næ­£åœ¨å¯åŠ¨ExcelåµŒå¥—æ•°æ®ä¿®å¤å·¥å…·...\n")
            fix_nested_json_main()
        else:
            print("\næ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·è¾“å…¥0-7ä¹‹é—´çš„æ•°å­—")
        
        input("\næŒ‰Enteré”®è¿”å›ä¸»èœå•...")

if __name__ == "__main__":
    main() 