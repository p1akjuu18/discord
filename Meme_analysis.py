#!/usr/bin/env python
# -*- coding: utf-8 -*-
import asyncio
import json
import sys
import logging
import aiohttp
from datetime import datetime, timedelta
import os
from pathlib import Path
import pandas as pd
import time
import threading
from typing import Optional, Dict, List, Any
import re
import twitter_api
import hmac
import hashlib
import base64
import requests
import traceback
from Feishu_Message_Send import FeishuBot
import lark_oapi as lark
from lark_oapi.api.im.v1 import CreateMessageRequest, CreateMessageRequestBody
import coingecko_api 
import argparse
# æ·»åŠ  watchdog å¯¼å…¥
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileModifiedEvent

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

__all__ = ['MemeAnalyzer', 'BacktestProcessor', 'process_message', 'CoinGeckoAnalyzer', 'MemeFileWatcher']  # æ–°å¢ MemeFileWatcher

# æ·»åŠ  TelegramBot ç±» - ç§»åˆ°å‰é¢å®šä¹‰
class TelegramBot:
    """å¤„ç† Telegram æ¶ˆæ¯å‘é€åŠŸèƒ½"""
    
    def __init__(self, token=None):
        """
        åˆå§‹åŒ– Telegram æœºå™¨äºº
        
        å‚æ•°:
            token: Telegram Bot API ä»¤ç‰Œ
        """
        # ä»é…ç½®æ–‡ä»¶åŠ è½½ï¼Œå¦‚æœæœªæä¾›
        if token is None:
            with open('config.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
                token = config.get('telegram_token', '')
                
        self.token = token
        self.base_url = f"https://api.telegram.org/bot{token}"
        
        logger.info(f"Telegramæœºå™¨äººåˆå§‹åŒ–ï¼ŒToken: {token[:6]}...{token[-4:] if token else ''}")
    
    def send_message(self, chat_id, text, parse_mode='Markdown'):
        """
        å‘é€æ¶ˆæ¯åˆ° Telegram
        
        å‚æ•°:
            chat_id: ç›®æ ‡èŠå¤©IDæˆ–èŠå¤©IDåˆ—è¡¨
            text: æ¶ˆæ¯å†…å®¹
            parse_mode: è§£ææ¨¡å¼ï¼Œå¯é€‰ 'Markdown' æˆ– 'HTML'
            
        è¿”å›:
            æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
        """
        try:
            # å°†å•ä¸ªèŠå¤©IDè½¬æ¢ä¸ºåˆ—è¡¨
            if isinstance(chat_id, str):
                chat_ids = [chat_id]
            else:
                chat_ids = chat_id
            
            success = False
            
            # å‘é€åˆ°æ¯ä¸ªèŠå¤©ID
            for single_chat_id in chat_ids:
                url = f"{self.base_url}/sendMessage"
                payload = {
                    'chat_id': single_chat_id,
                    'text': text,
                    'parse_mode': parse_mode
                }
                
                response = requests.post(url, json=payload)
                
                if response.status_code == 200:
                    logger.info(f"æ¶ˆæ¯å·²æˆåŠŸå‘é€åˆ° Telegram èŠå¤© {single_chat_id}")
                    success = True
                else:
                    logger.error(f"å‘é€ Telegram æ¶ˆæ¯å¤±è´¥: {response.status_code} - {response.text}")
            
            return success
                
        except Exception as e:
            logger.error(f"å‘é€ Telegram æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return False

# é¦–å…ˆå®šä¹‰ AnalysisIntegrator ç±»
class AnalysisIntegrator:
    """æ•´åˆå¤šä¸ªæ•°æ®æºçš„åˆ†æç»“æœ"""
    
    def __init__(self, app_id=None, app_secret=None):
        # åŠ è½½é…ç½®
        with open('config.json', 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        # é£ä¹¦é…ç½®
        self.app_id = app_id
        self.app_secret = app_secret
        
        # åªæœ‰å½“æä¾›äº†app_idå’Œapp_secretæ—¶æ‰åˆå§‹åŒ–é£ä¹¦æœºå™¨äºº
        if app_id and app_secret:
            self.feishu_bot = FeishuBot(app_id=app_id, app_secret=app_secret)
        else:
            self.feishu_bot = None
        
        # é£ä¹¦èŠå¤©ID
        self.feishu_chat_id = self.config.get('feishu_chat_id', 'oc_a2d2c5616c900bda2ab8e13a77361287')
        
        # æ·»åŠ  Telegram æ”¯æŒ
        telegram_token = self.config.get('telegram_token', '')
        if telegram_token:
            self.telegram_bot = TelegramBot(token=telegram_token)
        else:
            self.telegram_bot = None
        
        self.telegram_chat_id = self.config.get('telegram_chat_id', '')
        
        self.data_dir = Path('data')
        self.analysis_path = self.data_dir / 'integrated_analysis_results.xlsx'
        self.pending_tokens = {}
        self.processed_tokens = set()
        self.sending_tokens = set()
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        self._ensure_data_directory()
        
        # åˆ›å»ºæˆ–åŠ è½½ç»“æœæ–‡ä»¶
        self._init_results_file()
    
    def _ensure_data_directory(self):
        """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
        try:
            self.data_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"æ•°æ®ç›®å½•å·²ç¡®è®¤: {self.data_dir}")
        except Exception as e:
            logger.error(f"åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {str(e)}")
            raise
    
    def _init_results_file(self):
        """åˆå§‹åŒ–ç»“æœæ–‡ä»¶"""
        if not self.analysis_path.exists():
            # åˆ›å»ºç©ºçš„DataFrameå¹¶è®¾ç½®åˆ—
            columns = [
                'ä»£å¸åœ°å€', 'åˆ†ææ—¶é—´',
                # Twitterå’ŒDeepseekåˆ†æç»“æœ
                'æœç´¢å…³é”®è¯', 'å™äº‹ä¿¡æ¯', 'å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦', 'å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›', 
                'å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼', 'åŸå§‹æ¨æ–‡æ•°é‡',
                # CoinGeckoæ•°æ®
                'symbol', 'name', 'network', 'token_id',
                'fdv_usd', 'volume_usd_24h', 'price_change_m5', 'price_change_h1',
                'm5_buys', 'm5_sells', 'm15_buys', 'm15_sells', 'pool_created_at',
                # æ ‡è®°å­—æ®µ
                'twitter_analyzed', 'coingecko_analyzed', 'sent_to_feishu'
            ]
            
            df = pd.DataFrame(columns=columns)
            df.to_excel(self.analysis_path, index=False)
            logger.info(f"åˆ›å»ºäº†æ–°çš„æ•´åˆåˆ†æç»“æœæ–‡ä»¶: {self.analysis_path}")
    
    def register_token(self, token_address):
        """
        æ³¨å†Œä¸€ä¸ªæ–°çš„ä»£å¸åˆ†æä»»åŠ¡
        
        å‚æ•°:
            token_address: ä»£å¸åœ°å€
        
        è¿”å›:
            ä»»åŠ¡ID
        """
        if token_address in self.pending_tokens:
            logger.info(f"ä»£å¸ {token_address} å·²åœ¨å¤„ç†é˜Ÿåˆ—ä¸­")
            return
            
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
        if token_address in self.processed_tokens:
            logger.info(f"ä»£å¸ {token_address} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡")
            return
            
        # å°†ä»£å¸æ·»åŠ åˆ°å¾…å¤„ç†åˆ—è¡¨
        self.pending_tokens[token_address] = {
            'ä»£å¸åœ°å€': token_address,
            'åˆ†ææ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'twitter_analyzed': False,
            'coingecko_analyzed': False,
            'sent_to_feishu': False
        }
        
        logger.info(f"æ³¨å†Œäº†æ–°çš„ä»£å¸åˆ†æä»»åŠ¡: {token_address}")
        
        # å°†æ–°ä»»åŠ¡ä¿å­˜åˆ°æ–‡ä»¶
        try:
            df = pd.read_excel(self.analysis_path)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if not ((df['ä»£å¸åœ°å€'] == token_address) & (df['sent_to_feishu'] == False)).any():
                # æ·»åŠ æ–°è¡Œ
                new_row = pd.DataFrame([self.pending_tokens[token_address]])
                df = pd.concat([df, new_row], ignore_index=True)
                df.to_excel(self.analysis_path, index=False)
                logger.info(f"å·²å°†ä»£å¸ {token_address} æ·»åŠ åˆ°åˆ†æç»“æœæ–‡ä»¶")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–°ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
    
    def update_twitter_analysis(self, token_address, analysis_result):
        """
        æ›´æ–°Twitterå’ŒDeepseekåˆ†æç»“æœ
        
        å‚æ•°:
            token_address: ä»£å¸åœ°å€
            analysis_result: åˆ†æç»“æœå­—å…¸
        """
        if token_address not in self.pending_tokens:
            self.register_token(token_address)
        
        # æ›´æ–°å†…å­˜ä¸­çš„åˆ†æç»“æœ
        token_data = self.pending_tokens[token_address]
        token_data.update({
            'æœç´¢å…³é”®è¯': analysis_result.get('æœç´¢å…³é”®è¯', ''),
            'å™äº‹ä¿¡æ¯': analysis_result.get('å™äº‹ä¿¡æ¯', ''),
            'å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦': analysis_result.get('å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦', ''),
            'å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›': analysis_result.get('å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›', ''),
            'å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼': analysis_result.get('å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼', ''),
            'åŸå§‹æ¨æ–‡æ•°é‡': analysis_result.get('åŸå§‹æ¨æ–‡æ•°é‡', 0),
            'twitter_analyzed': True
        })
        
        # æ›´æ–°æ–‡ä»¶
        self._update_analysis_file(token_address, token_data)
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†æéƒ½å®Œæˆ
        self._check_and_send(token_address)
        
    def update_coingecko_analysis(self, token_address, coin_data):
        """
        æ›´æ–°CoinGeckoåˆ†æç»“æœ
        
        å‚æ•°:
            token_address: ä»£å¸åœ°å€
            coin_data: CoinGeckoåˆ†æç»“æœå­—å…¸
        """
        if token_address not in self.pending_tokens:
            self.register_token(token_address)
        
        # æ›´æ–°å†…å­˜ä¸­çš„åˆ†æç»“æœ
        token_data = self.pending_tokens[token_address]
        token_data.update({
            'symbol': coin_data.get('symbol', ''),
            'name': coin_data.get('name', ''),
            'network': coin_data.get('network', ''),
            'token_id': coin_data.get('token_id', ''),
            'fdv_usd': coin_data.get('fdv_usd', ''),
            'volume_usd_24h': coin_data.get('volume_usd_24h', ''),
            'price_change_m5': coin_data.get('price_change_m5', ''),
            'price_change_h1': coin_data.get('price_change_h1', ''),
            'm5_buys': coin_data.get('m5_buys', 0),
            'm5_sells': coin_data.get('m5_sells', 0),
            'm15_buys': coin_data.get('m15_buys', 0),
            'm15_sells': coin_data.get('m15_sells', 0),
            'pool_created_at': coin_data.get('pool_created_at', ''),
            'coingecko_analyzed': True
        })
        
        # æ›´æ–°æ–‡ä»¶
        self._update_analysis_file(token_address, token_data)
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†æéƒ½å®Œæˆ
        self._check_and_send(token_address)
    
    def _update_analysis_file(self, token_address, token_data):
        """ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æ›´æ–°åˆ†æç»“æœ"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = self.analysis_path.with_suffix('.tmp')
            
            # è¯»å–ç°æœ‰æ•°æ®
            if self.analysis_path.exists():
                df = pd.read_excel(self.analysis_path)
            else:
                df = pd.DataFrame(columns=self._get_columns())
            
            # æ›´æ–°æ•°æ®
            mask = df['ä»£å¸åœ°å€'] == token_address
            if mask.any():
                for col, value in token_data.items():
                    if col in df.columns:
                        df.loc[mask, col] = value
            else:
                new_row = pd.DataFrame([token_data])
                df = pd.concat([df, new_row], ignore_index=True)
            
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            df.to_excel(temp_file, index=False)
            
            # æ›¿æ¢åŸæ–‡ä»¶
            if self.analysis_path.exists():
                self.analysis_path.unlink()
            temp_file.rename(self.analysis_path)
            
            logger.info(f"å·²æ›´æ–°ä»£å¸ {token_address} çš„åˆ†æç»“æœ")
            
        except Exception as e:
            logger.error(f"æ›´æ–°åˆ†ææ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            if temp_file.exists():
                temp_file.unlink()
            raise
    
    def _check_and_send(self, token_address):
        """æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†æéƒ½å®Œæˆï¼Œå¦‚æœæ˜¯åˆ™å‘é€åˆ°é£ä¹¦"""
        token_data = self.pending_tokens.get(token_address)
        
        if not token_data:
            return
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨å‘é€ä¸­
        if token_address in self.sending_tokens:
            return
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆ
        need_both = self._need_both_analyses()
        
        # å…³é”®ä¿®æ”¹: æ£€æŸ¥æ˜¯å¦å·²ç»å‘é€è¿‡æˆ–è€…æ¡ä»¶ä¸æ»¡è¶³
        if token_data.get('sent_to_feishu'):
            return  # å¦‚æœå·²ç»å‘é€è¿‡ï¼Œç›´æ¥è¿”å›ï¼Œä¸é‡å¤å‘é€
        
        # å¦‚æœéœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆï¼Œä½†å…¶ä¸­ä¸€ä¸ªæœªå®Œæˆï¼Œåˆ™è¿”å›
        if need_both and not (token_data.get('twitter_analyzed') and token_data.get('coingecko_analyzed')):
            return
        
        # å¦‚æœä»£ç æ‰§è¡Œåˆ°è¿™é‡Œï¼Œæ„å‘³ç€å¯ä»¥å‘é€æ¶ˆæ¯äº†
        # æ·»åŠ å‘é€ä¸­æ ‡å¿—
        self.sending_tokens.add(token_address)
        
        try:
            # å‘é€æ¶ˆæ¯
            success = self._send_integrated_analysis(token_address)
            
            if success:
                # æ›´æ–°çŠ¶æ€
                token_data['sent_to_feishu'] = True
                self._update_analysis_file(token_address, token_data)
                
                # æ·»åŠ åˆ°å·²å¤„ç†é›†åˆ
                self.processed_tokens.add(token_address)
                
                # æ¸…ç†å†…å­˜ä¸­çš„æ•°æ®
                if token_address in self.pending_tokens:
                    del self.pending_tokens[token_address]
        finally:
            # ç§»é™¤å‘é€ä¸­æ ‡å¿—
            self.sending_tokens.remove(token_address)
    
    def _need_both_analyses(self):
        """ç¡®å®šæ˜¯å¦éœ€è¦åŒæ—¶å®ŒæˆTwitterå’ŒCoinGeckoåˆ†æ"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®é…ç½®æˆ–å‘½ä»¤è¡Œå‚æ•°å†³å®šæ˜¯å¦éœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆ
        # é»˜è®¤éœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆ
        return True
    
    def _send_integrated_analysis(self, token_address):
        """å‘é€æ•´åˆåçš„åˆ†æç»“æœåˆ°é£ä¹¦å’ŒTelegram"""
        try:
            token_data = self.pending_tokens.get(token_address)
            
            if not token_data:
                logger.error(f"æ‰¾ä¸åˆ°ä»£å¸ {token_address} çš„åˆ†ææ•°æ®")
                return False
            
            # æ„å»ºæ¶ˆæ¯
            message = f"""ğŸ”é‡‘ç‹—é¢„è­¦

ğŸ“Œ ä»£å¸åœ°å€: {token_address}"""

            # æ·»åŠ  CoinGecko æ•°æ®
            message += f"""
ğŸª™ åç§°: {token_data.get('name', 'N/A')} ({token_data.get('symbol', 'N/A')})
ğŸŒ ç½‘ç»œ: {token_data.get('network', 'N/A')}

ğŸ’° å¸‚åœºæ•°æ®:
â€¢ å¸‚å€¼: {coingecko_api.format_currency(token_data.get('fdv_usd', 'N/A'))}
â€¢ 24å°æ—¶äº¤æ˜“é‡: {coingecko_api.format_currency(token_data.get('volume_usd_24h', 'N/A'))}
â€¢ åˆ›å»ºæ—¶é—´: {token_data.get('pool_created_at', 'N/A')}

ğŸ“ˆ ä»·æ ¼å˜åŠ¨:
â€¢ 5åˆ†é’Ÿ: {coingecko_api.format_percentage(token_data.get('price_change_m5', 'N/A'))}
â€¢ 1å°æ—¶: {coingecko_api.format_percentage(token_data.get('price_change_h1', 'N/A'))}

ğŸ”„ æœ€è¿‘äº¤æ˜“æ¬¡æ•°:
â€¢ 5åˆ†é’Ÿå†…: ä¹°å…¥ {token_data.get('m5_buys', 0)} æ¬¡, å–å‡º {token_data.get('m5_sells', 0)} æ¬¡
â€¢ 15åˆ†é’Ÿå†…: ä¹°å…¥ {token_data.get('m15_buys', 0)} æ¬¡, å–å‡º {token_data.get('m15_sells', 0)} æ¬¡"""

            # æ·»åŠ  Twitter åˆ†ææ•°æ®
            message += f"""

ğŸ“ å™äº‹ä¿¡æ¯:
{token_data.get('å™äº‹ä¿¡æ¯', 'N/A')}

ğŸŒ¡ï¸ å¯æŒç»­æ€§åˆ†æ:
â€¢ ç¤¾åŒºçƒ­åº¦: {token_data.get('å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦', 'N/A')}
â€¢ ä¼ æ’­æ½œåŠ›: {token_data.get('å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›', 'N/A')}
â€¢ çŸ­æœŸæŠ•æœºä»·å€¼: {token_data.get('å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼', 'N/A')}"""

            # å‘é€åˆ°é£ä¹¦
            feishu_success = True
            if self.feishu_bot and self.feishu_chat_id:
                feishu_success = self.feishu_bot.send_message(
                    receive_id=self.feishu_chat_id,
                    content=message,
                    use_webhook=False
                )
                
                if feishu_success:
                    logger.info(f"å·²æˆåŠŸå‘é€ä»£å¸ {token_address} çš„ç»¼åˆåˆ†æç»“æœåˆ°é£ä¹¦")
                else:
                    logger.error(f"å‘é€ä»£å¸ {token_address} çš„ç»¼åˆåˆ†æç»“æœåˆ°é£ä¹¦å¤±è´¥")
            
            # å‘é€åˆ°Telegram
            telegram_success = True
            if self.telegram_bot and self.telegram_chat_id:
                telegram_success = self.telegram_bot.send_message(
                    chat_id=self.telegram_chat_id,
                    text=message
                )
                
                if telegram_success:
                    logger.info(f"å·²æˆåŠŸå‘é€ä»£å¸ {token_address} çš„ç»¼åˆåˆ†æç»“æœåˆ°Telegram")
                else:
                    logger.error(f"å‘é€ä»£å¸ {token_address} çš„ç»¼åˆåˆ†æç»“æœåˆ°Telegramå¤±è´¥")
            
            # å¦‚æœè‡³å°‘ä¸€ä¸ªå¹³å°å‘é€æˆåŠŸï¼Œåˆ™è¿”å›æˆåŠŸ
            return feishu_success or telegram_success
            
        except Exception as e:
            logger.error(f"å‘é€ç»¼åˆåˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return False

    def _save_with_retry(self, func, max_retries=3, delay=1):
        """å¸¦é‡è¯•æœºåˆ¶çš„ä¿å­˜æ“ä½œ"""
        for attempt in range(max_retries):
            try:
                return func()
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                logger.warning(f"ä¿å­˜æ“ä½œå¤±è´¥ï¼Œ{delay}ç§’åé‡è¯•: {str(e)}")
                time.sleep(delay)

# ç„¶ååˆ›å»ºå®ä¾‹
# ä»é…ç½®æ–‡ä»¶è·å–app_idå’Œapp_secret
import json

def load_config(config_file='config.json'):
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
        return {}

# åŠ è½½é…ç½®
config = load_config()
app_id = config.get('feishu_app_id', '')
app_secret = config.get('feishu_app_secret', '')

# ä¿®æ”¹è¿™ä¸€è¡Œï¼Œä¼ å…¥app_idå’Œapp_secret
integrator = AnalysisIntegrator(app_id=app_id, app_secret=app_secret)

# ç„¶åæ˜¯å…¶ä»–ç±»å®šä¹‰
class MemeAnalyzer:
    def __init__(self, config_file='config.json', api_key=None):
        self.config = self.load_config(config_file)
        self.setup_directories()
        
        # APIé…ç½®
        self.base_url = self.config.get("base_url", "https://api.siliconflow.cn")
        self.api_key = api_key or self.config.get("api_keys", {}).get("deepseek")
        
        # éªŒè¯ API key æ ¼å¼
        if not self.api_key:
            logger.error("Deepseek API keyæœªè®¾ç½®")
            raise ValueError("Deepseek API key is required")
        elif not self.api_key.startswith("sk-"):
            logger.error("Deepseek API key æ ¼å¼é”™è¯¯ï¼Œåº”è¯¥ä»¥ sk- å¼€å¤´")
            raise ValueError("Invalid Deepseek API key format")
            
        logger.info(f"Deepseek API key æ ¼å¼éªŒè¯é€šè¿‡")
        
        self.min_occurrence_threshold = self.config.get("min_occurrence_threshold", 2)
        self.term_history = {}
        self.history_cleanup_threshold = timedelta(hours=self.config.get("history_cleanup_threshold", 24))
        
        # åˆå§‹åŒ–é£ä¹¦æœºå™¨äºº - ä¼ å…¥æ­£ç¡®çš„å‡­æ®
        self.app_id = "cli_a736cea2ff78100d"
        self.app_secret = "C9FsC6CnJz3CLf0PEz0NQewkuH6uvCdS"
        self.feishu_bot = FeishuBot(app_id=self.app_id, app_secret=self.app_secret)
        
        self.feishu_chat_id = self.config.get("feishu_chat_id", "oc_a2d2c5616c900bda2ab8e13a77361287")
        self.integrator = integrator

    def load_config(self, config_file):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8-sig') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise

    def setup_directories(self):
        """è®¾ç½®å¿…è¦çš„ç›®å½•"""
        self.data_dir = Path('data')
        self.data_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ‰€æœ‰æ¨ç‰¹æ•°æ®å°†ä¿å­˜åœ¨ä¸€ä¸ªExcelæ–‡ä»¶
        self.twitter_all_data_path = self.data_dir / 'twitter_all_data.xlsx'
        self.meme_path = self.data_dir / 'meme.xlsx'
        
        # è®¾ç½®å…³é”®è¯è¿‡æ»¤é…ç½®
        self.keyword_filters = {
            # é»‘åå•å…³é”®è¯ - åŒ…å«è¿™äº›è¯çš„æ¨æ–‡å°†è¢«è¿‡æ»¤æ‰
            'blacklist': [
                'airdrop', 'alert', 'bot', 'scam', 'fake',
                'é‡‘ç‹—ä¿¡å·', 'èªæ˜é’±', 'æŠ¥è­¦', 'ğŸ”¥', 'ğŸš€', 'ğŸš¨'
            ],
    
            # æ˜¯å¦åŒºåˆ†å¤§å°å†™
            'case_sensitive': False
        }

    async def analyze_tweets(self, term: str, tweets: List[dict]) -> dict:
        """ä½¿ç”¨ Deepseek API åˆ†ææ¨æ–‡ï¼Œå¹¶ä¿å­˜åˆ°Excel"""
        try:
            # æå–æ¨æ–‡å†…å®¹å¹¶è¿›è¡Œé«˜çº§æ¸…ç†
            tweet_data = []  # ç”¨äºä¿å­˜åˆ°Excelçš„æ•°æ®
            tweet_texts = []
            
            logger.info(f"æ”¶é›† {len(tweets)} æ¡å…³äº {term} çš„æ¨æ–‡")
            
            # æ·»åŠ å…³é”®è¯è¿‡æ»¤ç»Ÿè®¡
            total_tweets = len(tweets)
            filtered_by_blacklist = 0
            filtered_by_whitelist = 0
            
            for tweet in tweets:
                text = tweet.get('text', '').strip()
                
                # æå–æ¨æ–‡çš„è¯¦ç»†ä¿¡æ¯
                tweet_id = tweet.get('tweet_id', '')
                user = tweet.get('user', {})
                username = user.get('screen_name', '')
                followers = user.get('followers_count', 0)
                verified = user.get('verified', False)
                created_at = tweet.get('created_at', '')
                favorite_count = tweet.get('favorite_count', 0)
                retweet_count = tweet.get('retweet_count', 0)
                media_urls = tweet.get('medias', [])
                media_type = tweet.get('media_type', '')
                
                if text:
                    # å…³é”®è¯è¿‡æ»¤å¤„ç†
                    # 1. è½¬æ¢æ–‡æœ¬å¤§å°å†™ï¼ˆå¦‚æœä¸åŒºåˆ†å¤§å°å†™ï¼‰
                    filter_text = text if self.keyword_filters['case_sensitive'] else text.lower()
                    
                    # 2. é»‘åå•è¿‡æ»¤
                    blacklist_keywords = self.keyword_filters['blacklist']
                    if not self.keyword_filters['case_sensitive']:
                        blacklist_keywords = [k.lower() for k in blacklist_keywords]
                        
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«é»‘åå•å…³é”®è¯
                    contains_blacklist = any(keyword in filter_text for keyword in blacklist_keywords)
                    if contains_blacklist:
                        filtered_by_blacklist += 1
                        logger.debug(f"æ¨æ–‡è¢«é»‘åå•è¿‡æ»¤: {text[:50]}...")
                        continue
                    
                    # 3. ç™½åå•è¿‡æ»¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if self.keyword_filters['whitelist_mode']:
                        whitelist_keywords = self.keyword_filters['whitelist']
                        if not self.keyword_filters['case_sensitive']:
                            whitelist_keywords = [k.lower() for k in whitelist_keywords]
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç™½åå•å…³é”®è¯
                        contains_whitelist = any(keyword in filter_text for keyword in whitelist_keywords)
                        if not contains_whitelist:
                            filtered_by_whitelist += 1
                            logger.debug(f"æ¨æ–‡æœªé€šè¿‡ç™½åå•: {text[:50]}...")
                            continue
                    
                    # åŸºç¡€æ¸…ç†
                    # æ¸…ç†åˆçº¦åœ°å€
                    text_cleaned = re.sub(r'[A-Za-z0-9]{32,}', '', text)
                    # æ¸…ç†URL
                    text_cleaned = re.sub(r'https?://\S+', '', text_cleaned)
                    
                    # é«˜çº§æ¸…ç† - æ–°å¢å¤„ç†æ­¥éª¤
                    # ç§»é™¤ç”¨æˆ·åæåŠ
                    text_cleaned = re.sub(r'@\w+', '', text_cleaned)
                    # ç§»é™¤hashtagsä½†ä¿ç•™æ–‡æœ¬
                    text_cleaned = re.sub(r'#(\w+)', r'\1', text_cleaned)
                    # ç§»é™¤è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
                    text_cleaned = re.sub(r'[^\w\s,.!?ï¼Œã€‚ï¼ï¼Ÿ]', '', text_cleaned)
                    # ç§»é™¤å¤šä½™çš„æ ‡ç‚¹ç¬¦å·
                    text_cleaned = re.sub(r'([.,!?ï¼Œã€‚ï¼ï¼Ÿ])\1+', r'\1', text_cleaned)
                    
                    # æ¸…ç†å¤šä½™ç©ºç™½
                    text_cleaned = ' '.join(text_cleaned.split())
                    
                    # ä¿å­˜æ¨æ–‡è¯¦ç»†æ•°æ®
                    tweet_data.append({
                        'token_address': term,
                        'tweet_id': tweet_id,
                        'username': username,
                        'followers': followers,
                        'verified': verified,
                        'created_at': created_at,
                        'text_original': text,
                        'text_cleaned': text_cleaned,
                        'likes': favorite_count,
                        'retweets': retweet_count,
                        'media_type': media_type,
                        'media_urls': ';'.join(media_urls) if media_urls else ''
                    })
                    
                    if text_cleaned.strip():  # ç¡®ä¿æ¸…ç†åè¿˜æœ‰å†…å®¹
                        # æ·»åŠ æ¨æ–‡é•¿åº¦æ£€æŸ¥ï¼Œè¿‡æ»¤è¿‡çŸ­çš„æ¨æ–‡
                        if len(text_cleaned.split()) >= 3:  # è‡³å°‘åŒ…å«3ä¸ªè¯
                            tweet_texts.append(text_cleaned)
            
            # è®°å½•è¿‡æ»¤ç»Ÿè®¡
            logger.info(f"æ¨æ–‡è¿‡æ»¤ç»Ÿè®¡: æ€»æ•°={total_tweets}, ä¿ç•™={len(tweet_data)}, "
                       f"è¢«é»‘åå•è¿‡æ»¤={filtered_by_blacklist}, "
                       f"æœªé€šè¿‡ç™½åå•={filtered_by_whitelist if self.keyword_filters['whitelist_mode'] else 'N/A'}")
            
            # ä¿å­˜åˆ°Excelæ–‡ä»¶
            self._save_tweets_to_excel(term, tweet_data)
            
            # ä»¥ä¸‹æ˜¯DeepSeek APIè°ƒç”¨éƒ¨åˆ†ï¼Œç°åœ¨è¢«æ³¨é‡Šæ‰
            '''
            # å†…å®¹èšåˆä¸å»é‡ - æ›´æ™ºèƒ½çš„å»é‡æ–¹å¼
            unique_texts = []
            seen_contents = set()
            
            for text in tweet_texts:
                # åˆ›å»ºå†…å®¹æŒ‡çº¹ (å¿½ç•¥å¤§å°å†™å’Œé¢å¤–ç©ºæ ¼)
                content_fingerprint = ' '.join(text.lower().split())
                
                # å¦‚æœå†…å®¹åŸºæœ¬ç›¸åŒåˆ™è·³è¿‡
                if content_fingerprint in seen_contents:
                    continue
                    
                # æ£€æŸ¥å†…å®¹ç›¸ä¼¼åº¦
                skip = False
                for existing in seen_contents:
                    # å¦‚æœä¸€ä¸ªæ–‡æœ¬æ˜¯å¦ä¸€ä¸ªçš„å­é›†ï¼Œæˆ–ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œåˆ™è·³è¿‡
                    if content_fingerprint in existing or existing in content_fingerprint:
                        skip = True
                        break
                
                if not skip:
                    seen_contents.add(content_fingerprint)
                    unique_texts.append(text)
            
            # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆä½¿ç”¨å†…å®¹æ›´ä¸°å¯Œçš„æ¨æ–‡
            unique_texts.sort(key=len, reverse=True)
            
            # é™åˆ¶æ¨æ–‡æ•°é‡ï¼Œé¿å…è¶…å‡ºAPIé™åˆ¶
            max_tweets = 15
            processed_tweets = unique_texts[:max_tweets]
            
            if not processed_tweets:
                logger.warning(f"æ¸…ç†åæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¨æ–‡å†…å®¹ç”¨äºåˆ†æ")
                return self._get_default_analysis(term, len(tweets))
            
            # åˆ›å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡æç¤º
            tweet_context = f"ä»¥ä¸‹æ˜¯å…³äºåŠ å¯†è´§å¸ {term} çš„ {len(processed_tweets)} æ¡çƒ­é—¨æ¨æ–‡:\n\n"
            
            for i, tweet in enumerate(processed_tweets, 1):
                tweet_context += f"æ¨æ–‡{i}: {tweet}\n\n"
            
            # ä¿®æ”¹è®¤è¯å¤´æ ¼å¼
            headers = {
                "Authorization": f"Bearer {self.api_key}",  # ç¡®ä¿æ˜¯ Bearer è®¤è¯
                "Content-Type": "application/json",
                "Accept": "application/json"  # æ·»åŠ  Accept å¤´
            }
            
            # æ£€æŸ¥å¹¶è®°å½• API keyï¼ˆéšè—éƒ¨åˆ†å†…å®¹ï¼‰
            masked_key = f"{self.api_key[:6]}...{self.api_key[-4:]}" if self.api_key else "None"
            logger.info(f"ä½¿ç”¨çš„ API key: {masked_key}")
            
            data = {
                "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                "messages": [
                    {
                        "role": "user",
                        "content": f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸åˆ†æå¸ˆï¼Œæˆ‘å¸Œæœ›ä½ èƒ½å¸®æˆ‘è¯„ä¼°è¿™ä¸ª Meme å¸çš„æ½œåŠ›ï¼Œå¹¶ç»™å‡ºè¯¦ç»†çš„åˆ†æå’Œå»ºè®®ã€‚

{tweet_context}

è¯·ä»ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢åˆ†åˆ«è¿›è¡Œåˆ†æï¼Œåˆ†2ç‚¹ï¼Œå¹¶ç”¨ä¸­æ–‡å›ç­”ï¼Œæˆ‘éœ€è¦çš„ç»“æœä¸è¶…è¿‡100å­—ï¼Œä½ éœ€è¦åˆ†ä»¥ä¸‹2ç‚¹æ˜ç¡®çš„è¿”å›ï¼š

1. å™äº‹ä¿¡æ¯ï¼šç”¨2-3å¥è¯æ€»ç»“è¿™ä¸ªmemeå¸çš„æ ¸å¿ƒå’Œå®ƒçš„æ ¸å¿ƒå–ç‚¹ã€‚

2. å¯æŒç»­æ€§ï¼šä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼š
   - ç¤¾åŒºçƒ­åº¦
   - ä¼ æ’­æ½œåŠ›
   - çŸ­æœŸæŠ•æœºä»·å€¼"""
                    }
                ],
                "stream": False,
                "temperature": 0.7,
                "max_tokens": 512,
                "top_p": 0.7,
                "top_k": 50,
                "frequency_penalty": 0.5
            }
            
            logger.info(f"å‘é€Deepseek APIè¯·æ±‚ï¼Œåˆ†æ {len(processed_tweets)} æ¡æ¨æ–‡")
            
            # å‘èµ·è¯·æ±‚
            max_retries = 3
            retry_count = 0
            backoff_time = 1  # åˆå§‹ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            
            while retry_count < max_retries:
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            f"{self.base_url}/v1/chat/completions",
                            headers=headers,
                            json=data,
                            timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´
                        ) as response:
                            if response.status == 200:
                                result = await response.json()
                                analysis_content = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                                
                                if not analysis_content:
                                    logger.warning("APIè¿”å›å†…å®¹ä¸ºç©º")
                                    return self._get_default_analysis(term, len(tweets))
                                    
                                logger.info("æˆåŠŸæ¥æ”¶åˆ°APIå“åº”")
                                
                                # è§£æå›å¤å†…å®¹
                                analysis_results = self._parse_analysis(analysis_content)
                                analysis_results['æœç´¢å…³é”®è¯'] = term
                                analysis_results['åŸå§‹æ¨æ–‡æ•°é‡'] = len(tweets)
                                return analysis_results
                            else:
                                error_text = await response.text()
                                logger.error(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, é”™è¯¯: {error_text}")
                                
                                if response.status == 429:  # é€Ÿç‡é™åˆ¶
                                    retry_count += 1
                                    wait_time = backoff_time * (2 ** (retry_count - 1))  # æŒ‡æ•°é€€é¿
                                    logger.warning(f"APIé€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• ({retry_count}/{max_retries})")
                                    await asyncio.sleep(wait_time)
                                    continue
                                
                                return self._get_default_analysis(term, len(tweets))
                except Exception as e:
                    logger.error(f"è¯·æ±‚APIæ—¶å‡ºé”™: {str(e)}")
                    retry_count += 1
                    
                    if retry_count < max_retries:
                        wait_time = backoff_time * (2 ** (retry_count - 1))
                        logger.warning(f"ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• ({retry_count}/{max_retries})")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒè¯·æ±‚")
                        return self._get_default_analysis(term, len(tweets))
            '''
            
            # è¿”å›ä¸€ä¸ªç®€å•çš„ç»Ÿè®¡ç»“æœï¼Œä¸ä½¿ç”¨DeepSeek
            return {
                'æœç´¢å…³é”®è¯': term,
                'åŸå§‹æ¨æ–‡æ•°é‡': total_tweets,
                'è¿‡æ»¤åæ¨æ–‡æ•°é‡': len(tweet_data),
                'è¿‡æ»¤æ‰çš„æ¨æ–‡æ•°é‡': total_tweets - len(tweet_data),
                'æ”¶é›†æ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†æ¨æ–‡æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return self._get_default_analysis(term, len(tweets))

    def _save_tweets_to_excel(self, term: str, tweet_data: List[dict]):
        """ä¿å­˜æ¨æ–‡æ•°æ®åˆ°åŒä¸€ä¸ªExcelæ–‡ä»¶ä¸­"""
        try:
            logger.info(f"å¼€å§‹ä¿å­˜ {len(tweet_data)} æ¡æ¨æ–‡æ•°æ®åˆ°ç»Ÿä¸€æ–‡ä»¶...")
            
            # å‡†å¤‡æ•°æ®ï¼Œæ·»åŠ æ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            for tweet in tweet_data:
                tweet['ä¿å­˜æ—¶é—´'] = timestamp
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯»å–ç°æœ‰æ•°æ®
            if self.twitter_all_data_path.exists():
                try:
                    with pd.ExcelFile(self.twitter_all_data_path) as xls:
                        # æ£€æŸ¥æ–‡ä»¶ä¸­æ˜¯å¦å·²æœ‰è¯¦ç»†æ¨æ–‡è¡¨
                        if 'tweets_details' in xls.sheet_names:
                            existing_df = pd.read_excel(xls, sheet_name='tweets_details')
                            # åˆå¹¶æ•°æ®
                            new_df = pd.concat([existing_df, pd.DataFrame(tweet_data)], ignore_index=True)
                        else:
                            new_df = pd.DataFrame(tweet_data)
                        
                        # è¯»å–å…¶ä»–è¡¨æ ¼æ•°æ®
                        other_sheets = {}
                        for sheet in xls.sheet_names:
                            if sheet != 'tweets_details':
                                other_sheets[sheet] = pd.read_excel(xls, sheet_name=sheet)
                except Exception as e:
                    logger.error(f"è¯»å–ç°æœ‰Excelæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                    # å¦‚æœè¯»å–å¤±è´¥ï¼Œåˆ›å»ºæ–°DataFrame
                    new_df = pd.DataFrame(tweet_data)
                    other_sheets = {}
            else:
                # åˆ›å»ºæ–°DataFrame
                new_df = pd.DataFrame(tweet_data)
                other_sheets = {}
            
            # åˆ›å»ºExcelWriterï¼Œå‡†å¤‡å†™å…¥å¤šä¸ªè¡¨æ ¼
            with pd.ExcelWriter(self.twitter_all_data_path, engine='openpyxl') as writer:
                # å†™å…¥è¯¦ç»†æ¨æ–‡æ•°æ®
                new_df.to_excel(writer, sheet_name='tweets_details', index=False)
                
                # å†™å…¥å…¶ä»–è¡¨æ ¼
                for sheet_name, df in other_sheets.items():
                    df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # è°ƒç”¨æ–¹æ³•ä¿å­˜ç»Ÿè®¡ç»“æœåˆ°ç»Ÿä¸€æ–‡ä»¶
                self._append_to_main_results(term, tweet_data, writer)
            
            logger.info(f"å·²å°† {len(tweet_data)} æ¡æ¨æ–‡æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶: {self.twitter_all_data_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨æ–‡æ•°æ®åˆ°Excelæ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())

    def _append_to_main_results(self, term: str, tweet_data: List[dict], writer=None):
        """å°†ç»Ÿè®¡æ•°æ®æ·»åŠ åˆ°åŒä¸€ä¸ªExcelæ–‡ä»¶çš„ä¸åŒè¡¨æ ¼ä¸­"""
        try:
            # æ•´åˆæ‰€æœ‰æ¨æ–‡ç›¸å…³ä¿¡æ¯
            if not tweet_data:
                return
                
            # è®¡ç®—ä¸€äº›ç»Ÿè®¡æ•°æ®
            total_tweets = len(tweet_data)
            verified_tweets = sum(1 for t in tweet_data if t.get('verified', False))
            total_followers = sum(t.get('followers', 0) for t in tweet_data)
            total_likes = sum(t.get('likes', 0) for t in tweet_data)
            total_retweets = sum(t.get('retweets', 0) for t in tweet_data)
            
            # æå–æœ€é«˜å½±å“åŠ›çš„æ¨æ–‡(æ ¹æ®ç‚¹èµ+è½¬å‘æ•°)
            sorted_tweets = sorted(tweet_data, key=lambda t: (t.get('likes', 0) + t.get('retweets', 0)), reverse=True)
            top_tweet = sorted_tweets[0] if sorted_tweets else {}
            
            # å‡†å¤‡è¦æ·»åŠ åˆ°ç»Ÿè®¡è¡¨æ ¼çš„æ•°æ®
            main_data = {
                'ä»£å¸åœ°å€': term,
                'åˆ†ææ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'æœç´¢å…³é”®è¯': term,
                'åŸå§‹æ¨æ–‡æ•°é‡': total_tweets,
                'å·²éªŒè¯è´¦å·æ¨æ–‡': verified_tweets,
                'æ€»ç²‰ä¸æ•°': total_followers,
                'æ€»ç‚¹èµæ•°': total_likes,
                'æ€»è½¬å‘æ•°': total_retweets,
                'æœ€çƒ­é—¨æ¨æ–‡': top_tweet.get('text_cleaned', '')[:100] if top_tweet else '',
                'æœ€çƒ­é—¨æ¨æ–‡_ç”¨æˆ·': top_tweet.get('username', '') if top_tweet else '',
                'æœ€çƒ­é—¨æ¨æ–‡_ç‚¹èµ': top_tweet.get('likes', 0) if top_tweet else 0,
                'æœ€çƒ­é—¨æ¨æ–‡_è½¬å‘': top_tweet.get('retweets', 0) if top_tweet else 0,
                'twitter_analyzed': True,
                'deepseek_analyzed': False
            }
            
            # å®šä¹‰åˆ—å
            columns = [
                'ä»£å¸åœ°å€', 'åˆ†ææ—¶é—´', 'æœç´¢å…³é”®è¯', 'åŸå§‹æ¨æ–‡æ•°é‡',
                'å·²éªŒè¯è´¦å·æ¨æ–‡', 'æ€»ç²‰ä¸æ•°', 'æ€»ç‚¹èµæ•°', 'æ€»è½¬å‘æ•°',
                'æœ€çƒ­é—¨æ¨æ–‡', 'æœ€çƒ­é—¨æ¨æ–‡_ç”¨æˆ·', 'æœ€çƒ­é—¨æ¨æ–‡_ç‚¹èµ', 'æœ€çƒ­é—¨æ¨æ–‡_è½¬å‘',
                'twitter_analyzed', 'deepseek_analyzed'
            ]
            
            # å¦‚æœæœ‰æä¾›writerï¼Œè¯´æ˜æ˜¯åœ¨_save_tweets_to_excelä¸­è°ƒç”¨çš„
            if writer:
                # è¯»å–ç°æœ‰ç»Ÿè®¡è¡¨æ ¼æˆ–åˆ›å»ºæ–°è¡¨æ ¼
                try:
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç»Ÿè®¡è¡¨æ ¼
                    with pd.ExcelFile(self.twitter_all_data_path) as xls:
                        if 'statistics' in xls.sheet_names:
                            main_df = pd.read_excel(xls, sheet_name='statistics')
                        else:
                            main_df = pd.DataFrame(columns=columns)
                except:
                    main_df = pd.DataFrame(columns=columns)
                    
                # æ›´æ–°æˆ–æ·»åŠ è®°å½•
                if 'ä»£å¸åœ°å€' in main_df.columns:
                    existing_mask = main_df['ä»£å¸åœ°å€'] == term
                    if existing_mask.any():
                        # æ›´æ–°ç°æœ‰è®°å½•
                        for key, value in main_data.items():
                            if key in main_df.columns:
                                main_df.loc[existing_mask, key] = value
                    else:
                        # æ·»åŠ æ–°è®°å½•
                        main_df = pd.concat([main_df, pd.DataFrame([main_data])], ignore_index=True)
                else:
                    # å¦‚æœåˆ—ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°DataFrame
                    main_df = pd.DataFrame([main_data])
                    
                # å†™å…¥ç»Ÿè®¡è¡¨æ ¼
                main_df.to_excel(writer, sheet_name='statistics', index=False)
            else:
                # å¦‚æœæ˜¯å•ç‹¬è°ƒç”¨çš„ï¼Œéœ€è¦å•ç‹¬å†™å…¥æ–‡ä»¶
                try:
                    with pd.ExcelFile(self.twitter_all_data_path) as xls:
                        # è¯»å–æ‰€æœ‰è¡¨æ ¼
                        sheets = {sheet: pd.read_excel(xls, sheet_name=sheet) for sheet in xls.sheet_names}
                        
                        # æ›´æ–°æˆ–åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
                        if 'statistics' in sheets:
                            main_df = sheets['statistics']
                            # æ›´æ–°æˆ–æ·»åŠ è®°å½•
                            existing_mask = main_df['ä»£å¸åœ°å€'] == term
                            if existing_mask.any():
                                for key, value in main_data.items():
                                    if key in main_df.columns:
                                        main_df.loc[existing_mask, key] = value
                            else:
                                main_df = pd.concat([main_df, pd.DataFrame([main_data])], ignore_index=True)
                        else:
                            main_df = pd.DataFrame([main_data], columns=columns)
                        
                        sheets['statistics'] = main_df
                        
                        # é‡æ–°å†™å…¥æ‰€æœ‰è¡¨æ ¼
                        with pd.ExcelWriter(self.twitter_all_data_path, engine='openpyxl') as writer:
                            for sheet_name, df in sheets.items():
                                df.to_excel(writer, sheet_name=sheet_name, index=False)
                except Exception as e:
                    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
                    main_df = pd.DataFrame([main_data], columns=columns)
                    with pd.ExcelWriter(self.twitter_all_data_path, engine='openpyxl') as writer:
                        main_df.to_excel(writer, sheet_name='statistics', index=False)
            
            logger.info(f"å·²æ›´æ–°ç»Ÿè®¡æ•°æ®è¡¨æ ¼")
            
        except Exception as e:
            logger.error(f"æ›´æ–°ç»Ÿè®¡æ•°æ®è¡¨æ ¼æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())

    def _get_default_analysis(self, term: str, tweet_count: int) -> dict:
        """è¿”å›é»˜è®¤çš„åˆ†æç»“æœ"""
        return {
            'æœç´¢å…³é”®è¯': term,
            'å™äº‹ä¿¡æ¯': f'APIè®¤è¯å¤±è´¥ï¼Œæ— æ³•åˆ†æã€‚å…±æœ‰{tweet_count}æ¡æ¨æ–‡',
            'å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦': 'æœªçŸ¥',
            'å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›': 'æœªçŸ¥',
            'å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼': 'æœªçŸ¥',
            'åŸå§‹æ¨æ–‡æ•°é‡': tweet_count,
            'åˆ†ææ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

    async def process_history_file(self):
        """å¤„ç†å†å²æ•°æ®æ–‡ä»¶"""
        try:
            logger.info("å¼€å§‹å¤„ç†meme.xlsxå†å²æ•°æ®")
            
            if not self.meme_path.exists():
                logger.error("meme.xlsxæ–‡ä»¶ä¸å­˜åœ¨")
                return
                
            df = pd.read_excel(self.meme_path)
            logger.info(f"åŠ è½½äº† {len(df)} æ¡å†å²è®°å½•")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            if 'å†…å®¹' not in df.columns:
                logger.error("meme.xlsxæ–‡ä»¶ç¼ºå°‘'å†…å®¹'åˆ—")
                return
                
            # åˆ›å»º CoinGecko åˆ†æå™¨å®ä¾‹
            coingecko_analyzer = CoinGeckoAnalyzer()
            
            # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
            for _, row in df.iterrows():
                content = row['å†…å®¹']
                logger.info(f"å¤„ç†å…³é”®è¯: {content}")
                
                # æ³¨å†Œåˆ°æ•´åˆå™¨
                self.integrator.register_token(content)
                
                # æœç´¢Twitter
                tweets = await twitter_api.search_tweets(content)
                logger.info(f"æ‰¾åˆ° {len(tweets)} æ¡ç›¸å…³æ¨æ–‡")
                
                if tweets:
                    # åˆ†ææ¨æ–‡
                    analysis = await self.analyze_tweets(content, tweets)
                    if analysis:
                        logger.info(f"å·²ä¿å­˜å…³é”®è¯ '{content}' çš„åˆ†æç»“æœ")
                        
                        # åœ¨è¿™é‡Œè°ƒç”¨ CoinGecko åˆ†æ
                        logger.info(f"å¼€å§‹å¯¹ '{content}' è¿›è¡Œ CoinGecko åˆ†æ...")
                        token_data = await coingecko_analyzer.analyze_token(content)
                        if token_data:
                            logger.info(f"å®Œæˆå¯¹ '{content}' çš„ CoinGecko åˆ†æ")
                        else:
                            logger.warning(f"CoinGecko æ— æ³•åˆ†æä»£å¸ '{content}'")
                    else:
                        logger.warning(f"å…³é”®è¯ '{content}' çš„åˆ†æç»“æœä¸ºç©º")
                else:
                    logger.warning(f"å…³é”®è¯ '{content}' æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ¨æ–‡")
            
            logger.info("å†å²æ•°æ®å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"å¤„ç†å†å²æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            logger.exception(e)

    def update_keyword_filters(self, new_filters):
        """æ›´æ–°å…³é”®è¯è¿‡æ»¤é…ç½®"""
        if not isinstance(new_filters, dict):
            raise ValueError("è¿‡æ»¤å™¨å¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
            
        # æ›´æ–°é…ç½®
        for key, value in new_filters.items():
            if key in self.keyword_filters:
                self.keyword_filters[key] = value
                
        logger.info(f"å…³é”®è¯è¿‡æ»¤é…ç½®å·²æ›´æ–°: {self.keyword_filters}")
        return self.keyword_filters

class BacktestProcessor:
    def __init__(self):
        self.data_dir = Path('data')
        self.data_dir.mkdir(exist_ok=True)
        self.meme_path = self.data_dir / 'meme.xlsx'
        self.twitter_results_path = self.data_dir / 'twitter_results.xlsx'
        
    async def save_meme_data(self, meme_data):
        """ä¿å­˜memeæ•°æ®åˆ°Excel"""
        try:
            if self.meme_path.exists():
                df_meme = pd.read_excel(self.meme_path)
                df_meme = pd.concat([df_meme, pd.DataFrame(meme_data)], ignore_index=True)
            else:
                df_meme = pd.DataFrame(meme_data)
            
            df_meme.to_excel(self.meme_path, index=False)
            logger.info(f"æˆåŠŸä¿å­˜ {len(meme_data)} æ¡memeæ•°æ®åˆ°Excel")
        except Exception as e:
            logger.error(f"ä¿å­˜memeæ•°æ®æ—¶å‡ºé”™: {e}")

# åˆ›å»ºå…¨å±€å®ä¾‹
processor = BacktestProcessor()

async def process_message(message_data: Dict[str, Any]) -> None:
    """å¤„ç†æ¥è‡ªDiscordçš„æ¶ˆæ¯"""
    try:
        # æå–å¤„ç†å¥½çš„æ•°æ®
        meme_data = message_data.get('meme_data', [])
        search_terms = message_data.get('search_terms', [])
        
        # ä¿å­˜memeæ•°æ®
        if meme_data:
            await processor.save_meme_data(meme_data)
            
    except Exception as e:
        logger.error(f"å¤„ç†æ¶ˆæ¯æ•°æ®æ—¶å‡ºé”™: {str(e)}")

class CoinGeckoAnalyzer:
    """è´Ÿè´£å¤„ç†CoinGeckoç›¸å…³çš„ä»£å¸åˆ†æåŠŸèƒ½"""
    
    def __init__(self, api_key=None):
        """
        åˆå§‹åŒ–CoinGeckoåˆ†æå™¨
        
        å‚æ•°:
            api_key: CoinGecko API å¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ¨¡å—é»˜è®¤å€¼
        """
        self.api_key = api_key or coingecko_api.API_KEY
        self.data_dir = Path('data')
        self.data_dir.mkdir(exist_ok=True)
        self.meme_path = self.data_dir / 'meme.xlsx'
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        self.client = coingecko_api.CoinGeckoAPI(self.api_key)
        
        # æ·»åŠ æ•´åˆå™¨å¼•ç”¨
        self.integrator = integrator
        
        logger.info("CoinGeckoåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def analyze_token(self, token_address):
        """
        åˆ†æå•ä¸ªä»£å¸çš„äº¤æ˜“æ•°æ®
        
        å‚æ•°:
            token_address: ä»£å¸åœ°å€
            
        è¿”å›:
            åˆ†æç»“æœå­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹åˆ†æä»£å¸: {token_address}")
            
            # æ¸…ç†å’ŒéªŒè¯tokenåœ°å€
            if isinstance(token_address, str):
                token_address = token_address.strip()
                # å®½æ¾çš„éªŒè¯ï¼Œå…è®¸ä¸åŒç½‘ç»œçš„åœ°å€æ ¼å¼
                if len(token_address) < 20:
                    logger.warning(f"è·³è¿‡æ— æ•ˆçš„åœ°å€æ ¼å¼: {token_address}")
                    return None
            
            # å°è¯•å¤šç½‘ç»œè¯·æ±‚
            token_info, network = await coingecko_api.try_multiple_networks(self.client, token_address)
            
            if not token_info or not network:
                logger.warning(f"æ‰€æœ‰ç½‘ç»œå‡æœªæ‰¾åˆ°ä»£å¸: {token_address}")
                return None
            
            # æå–åŸºæœ¬ä¿¡æ¯å’Œäº¤æ˜“æ•°æ®
            token_data = {
                'token_address': token_address,
                'network': network
            }
            
            # æå–è¯¦ç»†å±æ€§
            attributes = token_info['data']['attributes']
            
            # åŸºæœ¬ä¿¡æ¯
            token_data['token_id'] = token_info['data'].get('id', '')
            token_data['address'] = attributes.get('address', '')
            token_data['symbol'] = attributes.get('symbol', '')
            token_data['name'] = attributes.get('name', '')
            
            # å¸‚å€¼å’Œäº¤æ˜“é‡
            token_data['fdv_usd'] = attributes.get('fdv_usd', '')
            token_data['fdv_usd_formatted'] = coingecko_api.format_currency(attributes.get('fdv_usd', ''))
            
            # 24å°æ—¶äº¤æ˜“é‡
            if 'volume_usd' in attributes and 'h24' in attributes['volume_usd']:
                volume_24h = attributes['volume_usd']['h24']
                token_data['volume_usd_24h'] = volume_24h
                token_data['volume_usd_24h_formatted'] = coingecko_api.format_currency(volume_24h)
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨includedæ•°æ®ï¼ˆæ± ä¿¡æ¯ï¼‰
            if 'included' in token_info and token_info['included'] and len(token_info['included']) > 0:
                # æå–ç¬¬ä¸€ä¸ªæ± çš„æ•°æ®
                pool = token_info['included'][0]
                if 'attributes' in pool:
                    pool_attrs = pool['attributes']
                    
                    # æ·»åŠ æ± åˆ›å»ºæ—¶é—´
                    if 'pool_created_at' in pool_attrs:
                        utc_time = pool_attrs['pool_created_at']
                        token_data['pool_created_at'] = coingecko_api.convert_utc_to_utc8(utc_time)
                    
                    # ä»·æ ¼å˜åŠ¨
                    if 'price_change_percentage' in pool_attrs:
                        price_changes = pool_attrs['price_change_percentage']
                        # 5åˆ†é’Ÿä»·æ ¼å˜åŒ–
                        if 'm5' in price_changes:
                            m5_change = price_changes['m5']
                            token_data['price_change_m5'] = m5_change
                            token_data['price_change_m5_formatted'] = coingecko_api.format_percentage(m5_change)
                        # 1å°æ—¶ä»·æ ¼å˜åŒ–
                        if 'h1' in price_changes:
                            h1_change = price_changes['h1']
                            token_data['price_change_h1'] = h1_change
                            token_data['price_change_h1_formatted'] = coingecko_api.format_percentage(h1_change)
                    
                    # äº¤æ˜“æ•°é‡
                    if 'transactions' in pool_attrs:
                        txs = pool_attrs['transactions']
                        # 5åˆ†é’Ÿäº¤æ˜“
                        if 'm5' in txs:
                            token_data['m5_buys'] = txs['m5'].get('buys', 0)
                            token_data['m5_sells'] = txs['m5'].get('sells', 0)
                        # 15åˆ†é’Ÿäº¤æ˜“
                        if 'm15' in txs:
                            token_data['m15_buys'] = txs['m15'].get('buys', 0)
                            token_data['m15_sells'] = txs['m15'].get('sells', 0)
            
            logger.info(f"æˆåŠŸåˆ†æä»£å¸ {token_address} çš„äº¤æ˜“æ•°æ®")
            
            # åœ¨æˆåŠŸåˆ†æä»£å¸åï¼Œæ›´æ–°æ•´åˆå™¨çš„æ•°æ®
            if token_data:
                self.integrator.update_coingecko_analysis(token_address, token_data)
            
            return token_data
            
        except Exception as e:
            logger.error(f"åˆ†æä»£å¸ {token_address} æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return None
    
    async def send_token_analysis_to_feishu(self, token_data):
        """å°†ä»£å¸åˆ†æç»“æœå‘é€åˆ°é£ä¹¦"""
        if not token_data:
            logger.warning("æ— æœ‰æ•ˆä»£å¸æ•°æ®ï¼Œè·³è¿‡å‘é€åˆ°é£ä¹¦")
            return False
        
        try:
            # åˆ›å»ºé€‚åˆé£ä¹¦æ˜¾ç¤ºçš„æ¶ˆæ¯æ ¼å¼
            message = f"""ğŸª™ ä»£å¸äº¤æ˜“æ•°æ®åˆ†æ

ğŸ“Š åŸºæœ¬ä¿¡æ¯:
â€¢ åç§°: {token_data.get('name', 'N/A')} ({token_data.get('symbol', 'N/A')})
â€¢ ç½‘ç»œ: {token_data.get('network', 'N/A')}
â€¢ åœ°å€: {token_data.get('address', 'N/A')}

ğŸ’° å¸‚åœºæ•°æ®:
â€¢ å…¨é¢å¸‚å€¼: {token_data.get('fdv_usd_formatted', 'N/A')}
â€¢ 24å°æ—¶äº¤æ˜“é‡: {token_data.get('volume_usd_24h_formatted', 'N/A')}

ğŸ“ˆ ä»·æ ¼å˜åŠ¨:
â€¢ 5åˆ†é’Ÿ: {token_data.get('price_change_m5_formatted', 'N/A')}
â€¢ 1å°æ—¶: {token_data.get('price_change_h1_formatted', 'N/A')}

ğŸ”„ æœ€è¿‘äº¤æ˜“:
â€¢ 5åˆ†é’Ÿå†…: ä¹°å…¥ {token_data.get('m5_buys', 0)} æ¬¡, å–å‡º {token_data.get('m5_sells', 0)} æ¬¡
â€¢ 15åˆ†é’Ÿå†…: ä¹°å…¥ {token_data.get('m15_buys', 0)} æ¬¡, å–å‡º {token_data.get('m15_sells', 0)} æ¬¡

â±ï¸ æ± åˆ›å»ºæ—¶é—´: {token_data.get('pool_created_at', 'N/A')}"""

            success = self.feishu_bot.send_message(
                receive_id=self.feishu_chat_id,
                content=message,
                use_webhook=False
            )
            
            if success:
                logger.info("ä»£å¸åˆ†æç»“æœå·²æˆåŠŸå‘é€åˆ°é£ä¹¦")
            else:
                logger.error("å‘é€ä»£å¸åˆ†æç»“æœåˆ°é£ä¹¦å¤±è´¥")
            
            return success
            
        except Exception as e:
            logger.error(f"å‘é€ä»£å¸åˆ†æç»“æœåˆ°é£ä¹¦æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return False
    
    async def process_meme_file(self, start_index=0, batch_size=10, save_interval=60):
        """
        æ‰¹é‡å¤„ç†meme.xlsxæ–‡ä»¶ä¸­çš„ä»£å¸åœ°å€
        
        å‚æ•°:
            start_index: å¼€å§‹å¤„ç†çš„ç´¢å¼•
            batch_size: æ¯æ‰¹å¤„ç†çš„æ•°é‡
            save_interval: ä¿å­˜ç»“æœçš„æ—¶é—´é—´éš”(ç§’)
        """
        try:
            logger.info("å¼€å§‹æ‰¹é‡å¤„ç†meme.xlsxä¸­çš„ä»£å¸åœ°å€")
            
            if not self.meme_path.exists():
                logger.error("meme.xlsxæ–‡ä»¶ä¸å­˜åœ¨")
                return
            
            # è·å–å½“å‰æ—¶é—´æˆ³
            current_time = time.strftime('%Y%m%d_%H%M%S')
            output_excel_path = self.data_dir / f'token_trading_data_{current_time}.xlsx'
            
            # è¯»å–Excelæ–‡ä»¶
            logger.info(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {self.meme_path}")
            df = pd.read_excel(self.meme_path)
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            if 'å†…å®¹' not in df.columns:
                logger.error("meme.xlsxæ–‡ä»¶ç¼ºå°‘'å†…å®¹'åˆ—")
                return
            
            logger.info(f"æ€»è¡Œæ•°: {len(df)}")
            logger.info(f"å°†ä»ç¬¬ {start_index} ä¸ªä»£å¸å¼€å§‹å¤„ç†...")
            
            # æ·»åŠ éœ€è¦çš„åˆ—
            columns = [
                'token_id', 'address', 'symbol', 'name', 'network',
                'fdv_usd', 'fdv_usd_formatted', 'volume_usd_24h', 'volume_usd_24h_formatted',
                'price_change_m5', 'price_change_m5_formatted', 'price_change_h1', 'price_change_h1_formatted',
                'm5_buys', 'm5_sells', 'm15_buys', 'm15_sells', 'pool_created_at'
            ]
            
            for col in columns:
                if col not in df.columns:
                    df[col] = ''
            
            # ç”¨äºè®°å½•ä¸Šæ¬¡ä¿å­˜çš„æ—¶é—´
            last_save_time = time.time()
            modified = False
            
            # ç»Ÿè®¡è®¡æ•°å™¨
            processed_count = 0
            success_count = 0
            error_count = 0
            
            # éå†æ¯ä¸ªä»£å¸
            for index, row in df.iloc[start_index:].iterrows():
                try:
                    token_address = row['å†…å®¹']
                    if pd.isna(token_address):
                        logger.warning(f"è·³è¿‡ç©ºåœ°å€ï¼Œç´¢å¼• {index}")
                        continue
                    
                    processed_count += 1
                    logger.info(f"æ­£åœ¨è·å–ç´¢å¼• {index} ({processed_count}/{len(df)}) çš„äº¤æ˜“æ•°æ®...")
                    
                    # æ³¨å†Œåˆ°æ•´åˆå™¨
                    self.integrator.register_token(token_address)
                    
                    # åˆ†æä»£å¸
                    token_data = await self.analyze_token(token_address)
                    
                    if token_data:
                        # å°†æ•°æ®æ›´æ–°åˆ°DataFrame
                        for key, value in token_data.items():
                            if key in df.columns:
                                df.at[index, key] = value
                        
                        # ä¸å†ç›´æ¥å‘é€åˆ°é£ä¹¦
                        # await self.send_token_analysis_to_feishu(token_data)
                        
                        modified = True
                        success_count += 1
                        
                    else:
                        error_count += 1
                        logger.error(f"æ— æ³•è·å– {token_address} çš„äº¤æ˜“æ•°æ®")
                    
                    # å®šæœŸä¿å­˜ç»“æœ
                    current_time = time.time()
                    if modified and (processed_count % batch_size == 0 or current_time - last_save_time >= save_interval):
                        try:
                            logger.info(f"å‡†å¤‡ä¿å­˜è¿›åº¦ï¼Œå·²å¤„ç† {processed_count} æ¡æ•°æ®...")
                            temp_file = str(output_excel_path).replace('.xlsx', '_temp.xlsx')
                            df.to_excel(temp_file, index=False)
                            if output_excel_path.exists():
                                output_excel_path.unlink()
                            os.rename(temp_file, output_excel_path)
                            logger.info(f"å·²ä¿å­˜å½“å‰è¿›åº¦åˆ°: {output_excel_path}")
                            last_save_time = current_time
                            modified = False
                        except Exception as save_error:
                            logger.error(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(save_error)}")
                    
                    time.sleep(1)  # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è§¦å‘APIé™åˆ¶
                    
                except Exception as e:
                    error_count += 1
                    logger.error(f"å¤„ç†ä»£å¸æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            # æœ€åä¿å­˜ä¸€æ¬¡
            if modified:
                try:
                    temp_file = str(output_excel_path).replace('.xlsx', '_temp.xlsx')
                    df.to_excel(temp_file, index=False)
                    if output_excel_path.exists():
                        output_excel_path.unlink()
                    os.rename(temp_file, output_excel_path)
                    logger.info("æœ€ç»ˆäº¤æ˜“æ•°æ®å·²ä¿å­˜")
                except Exception as final_save_error:
                    logger.error(f"æœ€ç»ˆä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(final_save_error)}")
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            logger.info("\nå¤„ç†ç»Ÿè®¡ä¿¡æ¯:")
            logger.info(f"æ€»è®°å½•æ•°: {len(df)}")
            logger.info(f"å¤„ç†è®°å½•æ•°: {processed_count}")
            logger.info(f"æˆåŠŸå¤„ç†æ•°: {success_count}")
            logger.info(f"å¤±è´¥è®°å½•æ•°: {error_count}")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            logger.error(traceback.format_exc())

class MemeAnalysisMonitor:
    def __init__(self):
        # ä»é…ç½®æ–‡ä»¶åŠ è½½é£ä¹¦é…ç½®
        with open('config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
            
        # è·å–é£ä¹¦é…ç½®
        self.app_id = config.get('feishu_app_id', 'cli_a736cea2ff78100d')
        self.app_secret = config.get('feishu_app_secret', 'C9FsC6CnJz3CLf0PEz0NQewkuH6uvCdS')
        
        # åˆå§‹åŒ–é£ä¹¦æœºå™¨äººï¼Œä¼ å…¥å¿…è¦çš„å‚æ•°
        self.feishu_bot = FeishuBot(app_id=self.app_id, app_secret=self.app_secret)
        self.feishu_chat_id = config.get('feishu_chat_id', 'oc_a2d2c5616c900bda2ab8e13a77361287')
        self.data_dir = Path('data')
        self.meme_file = self.data_dir / 'meme.xlsx'  # ä¿®æ”¹ä¸ºç›‘æ§ meme.xlsx
        self.last_modified_time = None
        self.last_processed_index = -1  # è®°å½•æœ€åå¤„ç†çš„è¡Œç´¢å¼•
        
        # åˆå§‹åŒ–æ—¶è¯»å–å½“å‰æ–‡ä»¶çš„è¡Œæ•°
        self._init_last_processed_index()
        
    def _init_last_processed_index(self):
        """åˆå§‹åŒ–æ—¶è¯»å–å½“å‰æ–‡ä»¶çš„è¡Œæ•°"""
        try:
            if self.meme_file.exists():
                df = pd.read_excel(self.meme_file)
                self.last_processed_index = len(df) - 1  # è®¾ç½®ä¸ºæœ€åä¸€è¡Œçš„ç´¢å¼•
                logger.info(f"åˆå§‹åŒ–å®Œæˆï¼Œå½“å‰ meme.xlsx æ–‡ä»¶å…±æœ‰ {self.last_processed_index + 1} è¡Œ")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æœ€åå¤„ç†ç´¢å¼•æ—¶å‡ºé”™: {str(e)}")
            self.last_processed_index = -1

    def monitor_analysis_file(self, interval: int = 5):
        """
        ç›‘æ§ meme.xlsx æ–‡ä»¶çš„æ›´æ–°
        :param interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        logging.info(f"å¼€å§‹ç›‘æ§æ–‡ä»¶: {self.meme_file}")
        
        while True:
            try:
                if not self.meme_file.exists():
                    logging.warning("meme.xlsx æ–‡ä»¶ä¸å­˜åœ¨")
                    time.sleep(interval)
                    continue

                current_mtime = os.path.getmtime(self.meme_file)
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ›´æ–°
                if self.last_modified_time is None or current_mtime > self.last_modified_time:
                    logging.info("æ£€æµ‹åˆ° meme.xlsx æ–‡ä»¶æ›´æ–°ï¼Œå¤„ç†æ–°æ•°æ®...")
                    self._process_new_data()
                    self.last_modified_time = current_mtime
                
                time.sleep(interval)
                
            except Exception as e:
                logging.error(f"ç›‘æ§æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                time.sleep(interval)

    def _process_new_data(self):
        """å¤„ç†æ–°çš„ meme æ•°æ®"""
        try:
            df = pd.read_excel(self.meme_file)
            current_rows = len(df)
            
            # å¦‚æœæœ‰æ–°è¡Œ
            if current_rows > self.last_processed_index + 1:
                # åªå¤„ç†æ–°å¢çš„è¡Œ
                new_rows = df.iloc[self.last_processed_index + 1:]
                logging.info(f"å‘ç° {len(new_rows)} æ¡æ–°æ•°æ®")
                
                # å¤„ç†æ¯ä¸€è¡Œæ–°æ•°æ®
                for _, row in new_rows.iterrows():
                    token_address = row['å†…å®¹']
                    if pd.isna(token_address):
                        continue
                        
                    # æ³¨å†Œåˆ°æ•´åˆå™¨è¿›è¡Œå¤„ç†
                    integrator.register_token(token_address)
                
                # æ›´æ–°æœ€åå¤„ç†çš„ç´¢å¼•
                self.last_processed_index = current_rows - 1
                logging.info(f"æ›´æ–°æœ€åå¤„ç†ç´¢å¼•ä¸º: {self.last_processed_index}")
                
        except Exception as e:
            logging.error(f"å¤„ç†æ–°æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

# æ³¨é‡Šæ‰MemeFileWatcherç±»æˆ–ä¿ç•™ä½†ä¸ä½¿ç”¨
'''
class MemeFileWatcher(FileSystemEventHandler):
    """ç›‘æ§ meme.xlsx æ–‡ä»¶çš„å˜åŒ–å¹¶å¤„ç†æ–°å¢æ•°æ®"""
    
    def __init__(self, meme_file_path, analyzer=None, coingecko_analyzer=None):
        super().__init__()
        self.meme_file_path = meme_file_path
        self.analyzer = analyzer
        self.coingecko_analyzer = coingecko_analyzer
        self.last_processed_row = 0
        self.last_modified_time = self._get_file_mtime()
        
        # åŠ è½½é…ç½®
        with open('config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # åˆå§‹åŒ–é£ä¹¦æœºå™¨äºº
        self.app_id = config.get('feishu_app_id', 'cli_a736cea2ff78100d')
        self.app_secret = config.get('feishu_app_secret', 'C9FsC6CnJz3CLf0PEz0NQewkuH6uvCdS')
        self.feishu_bot = FeishuBot(app_id=self.app_id, app_secret=self.app_secret)
        self.feishu_chat_id = config.get('feishu_chat_id', 'oc_a2d2c5616c900bda2ab8e13a77361287')
        
        # åˆå§‹åŒ–Telegramæœºå™¨äºº
        telegram_token = config.get('telegram_token', '')
        if telegram_token:
            self.telegram_bot = TelegramBot(token=telegram_token)
        else:
            self.telegram_bot = None
        self.telegram_chat_id = config.get('telegram_chat_id', '')
        
        # æ·»åŠ æ—¶é—´çª—å£å’Œè®¡æ•°é€»è¾‘
        self.token_occurrences = {}  # è®°å½•ä»£å¸å‡ºç°æ¬¡æ•°å’Œæ—¶é—´
        self.time_window = 600  # 10åˆ†é’Ÿ = 600ç§’
        self.occurrence_threshold = 3  # å‡ºç°3æ¬¡æ‰æŠ¥è­¦
        
        # æ·»åŠ æŠ¥è­¦å†å²è®°å½•
        self.alert_history = {}  # è®°å½•ä»£å¸çš„æŠ¥è­¦æ—¶é—´
        self.alert_cooldown = 3600  # 1å°æ—¶ = 3600ç§’
        
        # æ·»åŠ æ¶ˆæ¯æ¨é€çŠ¶æ€ç®¡ç†
        self.message_status = {}  # è®°å½•æ¶ˆæ¯æ¨é€çŠ¶æ€
        self.message_lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”
        
        # åˆå§‹åŒ–æ—¶æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œè®°å½•å½“å‰è¡Œæ•°
        self._check_initial_state()
        
        logger.info(f"æ–‡ä»¶ç›‘æ§å™¨å·²åˆå§‹åŒ–ï¼Œç›‘æ§æ–‡ä»¶: {meme_file_path}")
        logger.info(f"å½“å‰è®°å½•çš„è¡Œæ•°: {self.last_processed_row}")

    def _check_initial_state(self):
        """æ£€æŸ¥æ–‡ä»¶åˆå§‹çŠ¶æ€ï¼Œè®°å½•å½“å‰è¡Œæ•°"""
        if os.path.exists(self.meme_file_path):
            try:
                df = pd.read_excel(self.meme_file_path)
                self.last_processed_row = len(df)
                logger.info(f"åˆå§‹æ–‡ä»¶åŒ…å« {self.last_processed_row} è¡Œæ•°æ®")
            except Exception as e:
                logger.error(f"è¯»å–åˆå§‹æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                self.last_processed_row = 0
        else:
            logger.warning(f"ç›‘æ§çš„æ–‡ä»¶ {self.meme_file_path} ä¸å­˜åœ¨")
            self.last_processed_row = 0

    def _get_file_mtime(self):
        """è·å–æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´"""
        if os.path.exists(self.meme_file_path):
            return os.path.getmtime(self.meme_file_path)
        return 0

    def _check_token_occurrence(self, token_address):
        """æ£€æŸ¥ä»£å¸åœ¨æ—¶é—´çª—å£å†…çš„å‡ºç°æ¬¡æ•°"""
        current_time = time.time()
        
        # å¦‚æœä»£å¸ä¸åœ¨è®°å½•ä¸­ï¼Œåˆå§‹åŒ–è®°å½•
        if token_address not in self.token_occurrences:
            self.token_occurrences[token_address] = {
                'count': 1,
                'first_seen': current_time,
                'last_seen': current_time
            }
            return False
        
        # è·å–ä»£å¸è®°å½•
        record = self.token_occurrences[token_address]
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…
        if current_time - record['first_seen'] <= self.time_window:
            # åœ¨æ—¶é—´çª—å£å†…ï¼Œå¢åŠ è®¡æ•°
            record['count'] += 1
            record['last_seen'] = current_time
            
            # å¦‚æœè¾¾åˆ°é˜ˆå€¼ï¼Œè¿”å›True
            if record['count'] >= self.occurrence_threshold:
                logger.info(f"ä»£å¸ {token_address} åœ¨10åˆ†é’Ÿå†…å‡ºç° {record['count']} æ¬¡ï¼Œè§¦å‘æŠ¥è­¦")
                return True
        else:
            # è¶…å‡ºæ—¶é—´çª—å£ï¼Œé‡ç½®è®¡æ•°
            record['count'] = 1
            record['first_seen'] = current_time
            record['last_seen'] = current_time
        
        return False

    def _cleanup_old_records(self):
        """æ¸…ç†è¿‡æœŸçš„è®°å½•"""
        current_time = time.time()
        expired_tokens = [
            token for token, record in self.token_occurrences.items()
            if current_time - record['last_seen'] > self.time_window
        ]
        for token in expired_tokens:
            del self.token_occurrences[token]

    def _check_alert_history(self, token_address):
        """æ£€æŸ¥ä»£å¸æ˜¯å¦åœ¨å†·å´æœŸå†…"""
        current_time = time.time()
        
        if token_address in self.alert_history:
            last_alert_time = self.alert_history[token_address]
            if current_time - last_alert_time <= self.alert_cooldown:
                logger.info(f"ä»£å¸ {token_address} åœ¨1å°æ—¶å†…å·²ç»æŠ¥è­¦è¿‡ï¼Œè·³è¿‡")
                return False
            else:
                # è¶…è¿‡å†·å´æœŸï¼Œæ›´æ–°æŠ¥è­¦æ—¶é—´
                self.alert_history[token_address] = current_time
                return True
        else:
            # é¦–æ¬¡æŠ¥è­¦ï¼Œè®°å½•æ—¶é—´
            self.alert_history[token_address] = current_time
            return True

    def _cleanup_alert_history(self):
        """æ¸…ç†è¿‡æœŸçš„æŠ¥è­¦è®°å½•"""
        current_time = time.time()
        expired_tokens = [
            token for token, alert_time in self.alert_history.items()
            if current_time - alert_time > self.alert_cooldown
        ]
        for token in expired_tokens:
            del self.alert_history[token]

    def _check_message_status(self, token_address):
        """æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²ç»æ¨é€è¿‡"""
        with self.message_lock:
            if token_address in self.message_status:
                status = self.message_status[token_address]
                # å¦‚æœæ¶ˆæ¯å·²ç»æ¨é€æˆåŠŸï¼Œè¿”å› False
                if status.get('sent', False):
                    return False
                # å¦‚æœæ¶ˆæ¯æ­£åœ¨å¤„ç†ä¸­ï¼Œè¿”å› False
                if status.get('processing', False):
                    return False
            return True

    def _update_message_status(self, token_address, status):
        """æ›´æ–°æ¶ˆæ¯çŠ¶æ€"""
        with self.message_lock:
            self.message_status[token_address] = status

    def _cleanup_message_status(self):
        """æ¸…ç†è¿‡æœŸçš„æ¶ˆæ¯çŠ¶æ€"""
        current_time = time.time()
        with self.message_lock:
            expired_tokens = [
                token for token, status in self.message_status.items()
                if current_time - status.get('timestamp', 0) > self.alert_cooldown
            ]
            for token in expired_tokens:
                del self.message_status[token]

    def on_modified(self, event):
        """å½“æ–‡ä»¶è¢«ä¿®æ”¹æ—¶å¤„ç†æ–°æ•°æ®"""
        if not isinstance(event, FileModifiedEvent):
            return
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ–‡ä»¶
        if event.src_path != str(self.meme_file_path):
            return
            
        # æ£€æŸ¥ä¿®æ”¹æ—¶é—´ï¼Œé¿å…é‡å¤å¤„ç†
        current_mtime = self._get_file_mtime()
        if current_mtime == self.last_modified_time:
            return
            
        self.last_modified_time = current_mtime
        
        # ç­‰å¾…æ–‡ä»¶å®Œå…¨å†™å…¥
        time.sleep(1)
        
        # å¤„ç†æ–°æ•°æ®
        self._process_new_data()

    def _process_new_data(self):
        """å¤„ç†æ–°å¢æ•°æ®è¡Œ"""
        try:
            # æ¸…ç†è¿‡æœŸè®°å½•
            self._cleanup_old_records()
            self._cleanup_alert_history()
            self._cleanup_message_status()  # æ¸…ç†è¿‡æœŸçš„æ¶ˆæ¯çŠ¶æ€
            
            # è¯»å–å½“å‰æ–‡ä»¶
            df = pd.read_excel(self.meme_file_path)
            current_rows = len(df)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°è¡Œ
            if current_rows <= self.last_processed_row:
                logger.info("æ²¡æœ‰æ£€æµ‹åˆ°æ–°çš„æ•°æ®è¡Œ")
                return
                
            # å¤„ç†æ–°å¢çš„è¡Œ
            new_rows = df.iloc[self.last_processed_row:current_rows]
            logger.info(f"æ£€æµ‹åˆ° {len(new_rows)} è¡Œæ–°æ•°æ®")
            
            # ä¸ºæ¯ä¸€ä¸ªæ–°è¡Œå¼‚æ­¥å¤„ç†æ•°æ®
            for idx, row in new_rows.iterrows():
                token_address = row['å†…å®¹']
                if pd.isna(token_address) or not token_address:
                    logger.warning(f"è·³è¿‡ç©ºåœ°å€ï¼Œç´¢å¼• {idx}")
                    continue
                    
                logger.info(f"å¤„ç†æ–°å¢ä»£å¸: {token_address}")
                
                # æ£€æŸ¥ä»£å¸å‡ºç°æ¬¡æ•°å’ŒæŠ¥è­¦å†å²
                if self._check_token_occurrence(token_address) and self._check_alert_history(token_address):
                    # å¼‚æ­¥å¤„ç†æ–°çš„ä»£å¸æ•°æ®
                    asyncio.run(self._analyze_token(token_address))
                else:
                    logger.info(f"ä»£å¸ {token_address} æœªè¾¾åˆ°æŠ¥è­¦é˜ˆå€¼æˆ–åœ¨å†·å´æœŸå†…ï¼Œè·³è¿‡å¤„ç†")
            
            # æ›´æ–°å¤„ç†è¿‡çš„è¡Œæ•°
            self.last_processed_row = current_rows
            logger.info(f"å·²å¤„ç†åˆ°ç¬¬ {self.last_processed_row} è¡Œ")
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶æ›´æ–°æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())

    async def _analyze_token(self, token_address):
        """åˆ†æå•ä¸ªä»£å¸æ•°æ®å¹¶å‘é€åˆ°é£ä¹¦"""
        try:
            # æ£€æŸ¥æ¶ˆæ¯çŠ¶æ€
            if not self._check_message_status(token_address):
                logger.info(f"ä»£å¸ {token_address} çš„æ¶ˆæ¯å·²ç»å¤„ç†è¿‡æˆ–æ­£åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡")
                return

            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            self._update_message_status(token_address, {
                'processing': True,
                'timestamp': time.time()
            })

            twitter_analysis = None
            coingecko_data = None
            
            # 1. Twitter åˆ†æ
            if self.analyzer:
                logger.info(f"å¼€å§‹å¯¹ {token_address} è¿›è¡Œ Twitter åˆ†æ")
                max_retries = 5
                retry_delay = 30
                
                for attempt in range(max_retries):
                    try:
                        tweets = await twitter_api.search_tweets(token_address)
                        if tweets:
                            logger.info(f"æ‰¾åˆ° {len(tweets)} æ¡ç›¸å…³æ¨æ–‡")
                            twitter_analysis = await self.analyzer.analyze_tweets(token_address, tweets)
                            logger.info(f"å·²å®Œæˆ Twitter æ•°æ®ä¿å­˜: {twitter_analysis}")
                            break
                        else:
                            logger.warning(f"æœªæ‰¾åˆ°å…³äº {token_address} çš„æ¨æ–‡")
                            break
                    except Exception as e:
                        if "Rate limit exceeded" in str(e):
                            if attempt < max_retries - 1:
                                current_delay = retry_delay * (2 ** attempt)
                                logger.warning(f"Twitter API é€Ÿç‡é™åˆ¶ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {current_delay} ç§’...")
                                await asyncio.sleep(current_delay)
                                continue
                        logger.error(f"Twitter åˆ†æå‡ºé”™: {str(e)}")
                        break
            
            # 2. CoinGecko åˆ†æ
            if self.coingecko_analyzer:
                logger.info(f"å¼€å§‹å¯¹ {token_address} è¿›è¡Œ CoinGecko åˆ†æ")
                coingecko_data = await self.coingecko_analyzer.analyze_token(token_address)
                if coingecko_data:
                    logger.info(f"å·²å®Œæˆ {token_address} çš„ CoinGecko åˆ†æ")
                else:
                    logger.warning(f"CoinGecko æ— æ³•åˆ†æä»£å¸ {token_address}")
            
            # 3. å‘é€åˆ†æç»“æœåˆ°é£ä¹¦ - è¿™éƒ¨åˆ†éœ€è¦ä¿®æ”¹
            if twitter_analysis or coingecko_data:
                # ä¸å†ç«‹å³å‘é€ï¼Œè€Œæ˜¯åªä¿å­˜åˆ†æç»“æœ
                message = self._build_analysis_message(token_address, twitter_analysis, coingecko_data)
                if message:
                    # æ›´æ–°æ¶ˆæ¯çŠ¶æ€ï¼Œä½†ä¸å‘é€
                    self._update_message_status(token_address, {
                        'processed': True,  # æ ‡è®°ä¸ºå·²å¤„ç†
                        'message': message,  # ä¿å­˜æ¶ˆæ¯å†…å®¹
                        'timestamp': time.time()
                    })
                    
                    logger.info(f"å·²å®Œæˆ {token_address} çš„åˆ†æï¼Œç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆåç»Ÿä¸€å‘é€")
                    
        except Exception as e:
            logger.error(f"åˆ†æä»£å¸ {token_address} æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())

    def _build_analysis_message(self, token_address, twitter_analysis, coingecko_data):
        """æ„å»ºåˆ†æç»“æœæ¶ˆæ¯"""
        try:
            message = f"""ğŸ”é‡‘ç‹—é¢„è­¦

ğŸ“Œ ä»£å¸åœ°å€: {token_address}"""

            if coingecko_data:
                message += f"""
ğŸª™ åç§°: {coingecko_data.get('name', 'N/A')} ({coingecko_data.get('symbol', 'N/A')})
ğŸŒ ç½‘ç»œ: {coingecko_data.get('network', 'N/A')}

ğŸ’° å¸‚åœºæ•°æ®:
â€¢ å¸‚å€¼: {coingecko_api.format_currency(coingecko_data.get('fdv_usd', 'N/A'))}
â€¢ 24å°æ—¶äº¤æ˜“é‡: {coingecko_api.format_currency(coingecko_data.get('volume_usd_24h', 'N/A'))}
â€¢ åˆ›å»ºæ—¶é—´: {coingecko_data.get('pool_created_at', 'N/A')}

ğŸ“ˆ ä»·æ ¼å˜åŠ¨:
â€¢ 5åˆ†é’Ÿ: {coingecko_api.format_percentage(coingecko_data.get('price_change_m5', 'N/A'))}
â€¢ 1å°æ—¶: {coingecko_api.format_percentage(coingecko_data.get('price_change_h1', 'N/A'))}

ğŸ”„ æœ€è¿‘äº¤æ˜“æ¬¡æ•°:
â€¢ 5åˆ†é’Ÿå†…: ä¹°å…¥ {coingecko_data.get('m5_buys', 0)} æ¬¡, å–å‡º {coingecko_data.get('m5_sells', 0)} æ¬¡
â€¢ 15åˆ†é’Ÿå†…: ä¹°å…¥ {coingecko_data.get('m15_buys', 0)} æ¬¡, å–å‡º {coingecko_data.get('m15_sells', 0)} æ¬¡"""

            if twitter_analysis:
                message += f"""

ğŸ“ å™äº‹ä¿¡æ¯:
{twitter_analysis.get('å™äº‹ä¿¡æ¯', 'N/A')}

ğŸŒ¡ï¸ å¯æŒç»­æ€§åˆ†æ:
â€¢ ç¤¾åŒºçƒ­åº¦: {twitter_analysis.get('å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦', 'N/A')}
â€¢ ä¼ æ’­æ½œåŠ›: {twitter_analysis.get('å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›', 'N/A')}
â€¢ çŸ­æœŸæŠ•æœºä»·å€¼: {twitter_analysis.get('å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼', 'N/A')}"""

            return message
            
        except Exception as e:
            logger.error(f"æ„å»ºåˆ†ææ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
            return None

    def start_watching(self):
        """å¼€å§‹ç›‘æ§æ–‡ä»¶"""
        observer = Observer()
        # ç›‘æ§æ–‡ä»¶æ‰€åœ¨ç›®å½•
        directory = os.path.dirname(self.meme_file_path)
        observer.schedule(self, directory, recursive=False)
        observer.start()
        logger.info(f"å¼€å§‹ç›‘æ§ç›®å½•: {directory}")
        return observer
'''

# ä¿®æ”¹ä¸»å‡½æ•°
async def main():
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        parser = argparse.ArgumentParser(description='Meme å¸åˆ†æå·¥å…·')
        parser.add_argument('--coingecko', action='store_true', help='åªè¿è¡Œ CoinGecko åˆ†æ')
        parser.add_argument('--twitter', action='store_true', help='åªè¿è¡Œ Twitter åˆ†æ')
        parser.add_argument('--start', type=int, default=0, help='CoinGecko åˆ†æçš„èµ·å§‹ç´¢å¼•')
        parser.add_argument('--batch', type=int, default=10, help='CoinGecko æ‰¹å¤„ç†å¤§å°')
        # æ³¨é‡Šæ‰ç›‘æ§æ¨¡å¼é€‰é¡¹
        # parser.add_argument('--watch', action='store_true', help='å¯ç”¨æ–‡ä»¶ç›‘æ§æ¨¡å¼')
        args = parser.parse_args()
        
        # é»˜è®¤è¿è¡ŒTwitteråˆ†æ
        run_twitter = True  # é»˜è®¤å¯ç”¨Twitteråˆ†æ
        run_coingecko = not args.twitter or args.coingecko
        
        # é…ç½®æ•´åˆå™¨æ˜¯å¦éœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆ
        integrator._need_both_analyses = lambda: False  # ä¿®æ”¹ä¸ºä¸éœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆ
        
        # åˆå§‹åŒ–åˆ†æå™¨å®ä¾‹
        twitter_analyzer = None
        coingecko_analyzer = None
        
        if run_twitter:
            logger.info("åˆå§‹åŒ– Twitter Meme åˆ†æå™¨...")
            twitter_analyzer = MemeAnalyzer()
            
        if run_coingecko:
            logger.info("åˆå§‹åŒ– CoinGecko ä»£å¸åˆ†æå™¨...")
            coingecko_analyzer = CoinGeckoAnalyzer()
        
        # ç›´æ¥å¤„ç†meme.xlsxæ–‡ä»¶ï¼Œè€Œä¸æ˜¯ç›‘æ§å®ƒ
        logger.info("å¼€å§‹ç›´æ¥å¤„ç† meme.xlsx æ–‡ä»¶...")
        meme_file_path = Path('data') / 'meme.xlsx'
        
        if not meme_file_path.exists():
            logger.error(f"æ–‡ä»¶ {meme_file_path} ä¸å­˜åœ¨!")
            return
            
        # è¯»å–meme.xlsx
        df = pd.read_excel(meme_file_path)
        logger.info(f"æˆåŠŸè¯»å– meme.xlsxï¼Œå…±æœ‰ {len(df)} æ¡è®°å½•")
            
        # å¤„ç†æ‰€æœ‰ä»£å¸åœ°å€
        for index, row in df.iterrows():
            try:
                token_address = row['å†…å®¹']
                if pd.isna(token_address) or not token_address.strip():
                    logger.warning(f"è·³è¿‡ç©ºåœ°å€ï¼Œç´¢å¼• {index}")
                    continue
                
                logger.info(f"å¤„ç†ä»£å¸ [{index+1}/{len(df)}]: {token_address}")
                
                # æ³¨å†Œåˆ°æ•´åˆå™¨
                integrator.register_token(token_address)
                
                # Twitteråˆ†æ
                if run_twitter and twitter_analyzer:
                    try:
                        logger.info(f"å¼€å§‹å¯¹ {token_address} è¿›è¡Œ Twitter æœç´¢...")
                        tweets = await twitter_api.search_tweets(token_address)
                        
                        if tweets:
                            logger.info(f"æ‰¾åˆ° {len(tweets)} æ¡ç›¸å…³æ¨æ–‡ï¼Œä¿å­˜æ•°æ®...")
                            analysis = await twitter_analyzer.analyze_tweets(token_address, tweets)
                            logger.info(f"å·²å®Œæˆ Twitter æ•°æ®ä¿å­˜: {analysis}")
                        else:
                            logger.warning(f"æœªæ‰¾åˆ°å…³äº {token_address} çš„æ¨æ–‡")
                    except Exception as e:
                        logger.error(f"Twitter åˆ†æå¤±è´¥: {str(e)}")
                
                # æ³¨é‡Šæ‰ CoinGecko åˆ†æéƒ¨åˆ† 
                '''
                # CoinGeckoåˆ†æ
                if run_coingecko and coingecko_analyzer:
                    try:
                        logger.info(f"å¼€å§‹å¯¹ {token_address} è¿›è¡Œ CoinGecko åˆ†æ...")
                        token_data = await coingecko_analyzer.analyze_token(token_address)
                        if token_data:
                            logger.info(f"å®Œæˆ CoinGecko åˆ†æ")
                        else:
                            logger.warning(f"CoinGecko æ— æ³•åˆ†æä»£å¸ {token_address}")
                    except Exception as e:
                        logger.error(f"CoinGecko åˆ†æå¤±è´¥: {str(e)}")
                '''
                
                # æ·»åŠ é—´éš”ï¼Œé¿å…APIé™åˆ¶
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"å¤„ç†ä»£å¸æ—¶å‡ºé”™: {str(e)}")
                continue
        
        logger.info("æ‰€æœ‰ä»£å¸å¤„ç†å®Œæˆ")
            
    except Exception as e:
        logger.error(f"è¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.exception(e)

# ä¿®æ”¹ __main__ éƒ¨åˆ†
if __name__ == '__main__':
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    # ç›´æ¥è¿è¡Œä¸»å‡½æ•°ï¼Œä¸ä½¿ç”¨ç›‘æ§æ¨¡å¼
    try:
        logger.info("å¯åŠ¨ç¨‹åº - ç›´æ¥å¤„ç†æ¨¡å¼")
        asyncio.run(main())
    except Exception as e:
        logger.error(f"è¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.exception(e)