#!/usr/bin/env python
# -*- coding: utf-8 -*-
import asyncio
import json
import sys
import logging
import aiohttp
from datetime import datetime, timedelta
import os
from pathlib import Path
import pandas as pd
import time
import threading
from typing import Optional, Dict, List, Any
import re
import twitter_api
import hmac
import hashlib
import base64
import requests
import traceback
from feishu_bot import FeishuBot
import lark_oapi as lark
from lark_oapi.api.im.v1 import CreateMessageRequest, CreateMessageRequestBody
import coingecko_api 
import argparse
# æ·»åŠ  watchdog å¯¼å…¥
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileModifiedEvent

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

__all__ = ['MemeAnalyzer', 'BacktestProcessor', 'process_message', 'CoinGeckoAnalyzer', 'MemeFileWatcher']  # æ–°å¢ MemeFileWatcher

# æ·»åŠ  TelegramBot ç±» - ç§»åˆ°å‰é¢å®šä¹‰
class TelegramBot:
    """å¤„ç† Telegram æ¶ˆæ¯å‘é€åŠŸèƒ½"""
    
    def __init__(self, token=None):
        """
        åˆå§‹åŒ– Telegram æœºå™¨äºº
        
        å‚æ•°:
            token: Telegram Bot API ä»¤ç‰Œ
        """
        # ä»é…ç½®æ–‡ä»¶åŠ è½½ï¼Œå¦‚æœæœªæä¾›
        if token is None:
            with open('config.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
                token = config.get('telegram_token', '')
                
        self.token = token
        self.base_url = f"https://api.telegram.org/bot{token}"
        
        logger.info(f"Telegramæœºå™¨äººåˆå§‹åŒ–ï¼ŒToken: {token[:6]}...{token[-4:] if token else ''}")
    
    def send_message(self, chat_id, text, parse_mode='Markdown'):
        """
        å‘é€æ¶ˆæ¯åˆ° Telegram
        
        å‚æ•°:
            chat_id: ç›®æ ‡èŠå¤©IDæˆ–èŠå¤©IDåˆ—è¡¨
            text: æ¶ˆæ¯å†…å®¹
            parse_mode: è§£ææ¨¡å¼ï¼Œå¯é€‰ 'Markdown' æˆ– 'HTML'
            
        è¿”å›:
            æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
        """
        try:
            # å°†å•ä¸ªèŠå¤©IDè½¬æ¢ä¸ºåˆ—è¡¨
            if isinstance(chat_id, str):
                chat_ids = [chat_id]
            else:
                chat_ids = chat_id
            
            success = False
            
            # å‘é€åˆ°æ¯ä¸ªèŠå¤©ID
            for single_chat_id in chat_ids:
                url = f"{self.base_url}/sendMessage"
                payload = {
                    'chat_id': single_chat_id,
                    'text': text,
                    'parse_mode': parse_mode
                }
                
                response = requests.post(url, json=payload)
                
                if response.status_code == 200:
                    logger.info(f"æ¶ˆæ¯å·²æˆåŠŸå‘é€åˆ° Telegram èŠå¤© {single_chat_id}")
                    success = True
                else:
                    logger.error(f"å‘é€ Telegram æ¶ˆæ¯å¤±è´¥: {response.status_code} - {response.text}")
            
            return success
                
        except Exception as e:
            logger.error(f"å‘é€ Telegram æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return False

# é¦–å…ˆå®šä¹‰ AnalysisIntegrator ç±»
class AnalysisIntegrator:
    """æ•´åˆå¤šä¸ªæ•°æ®æºçš„åˆ†æç»“æœ"""
    
    def __init__(self, app_id=None, app_secret=None):
        # åŠ è½½é…ç½®
        with open('config.json', 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        # é£ä¹¦é…ç½®
        self.app_id = app_id
        self.app_secret = app_secret
        
        # åªæœ‰å½“æä¾›äº†app_idå’Œapp_secretæ—¶æ‰åˆå§‹åŒ–é£ä¹¦æœºå™¨äºº
        if app_id and app_secret:
            self.feishu_bot = FeishuBot(app_id=app_id, app_secret=app_secret)
        else:
            self.feishu_bot = None
        
        # é£ä¹¦èŠå¤©ID
        self.feishu_chat_id = self.config.get('feishu_chat_id', 'oc_a2d2c5616c900bda2ab8e13a77361287')
        
        # æ·»åŠ  Telegram æ”¯æŒ
        telegram_token = self.config.get('telegram_token', '')
        if telegram_token:
            self.telegram_bot = TelegramBot(token=telegram_token)
        else:
            self.telegram_bot = None
        
        self.telegram_chat_id = self.config.get('telegram_chat_id', '')
        
        self.data_dir = Path('data')
        self.analysis_path = self.data_dir / 'integrated_analysis_results.xlsx'
        self.pending_tokens = {}
        self.processed_tokens = set()
        self.sending_tokens = set()
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        self._ensure_data_directory()
        
        # åˆ›å»ºæˆ–åŠ è½½ç»“æœæ–‡ä»¶
        self._init_results_file()
    
    def _ensure_data_directory(self):
        """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
        try:
            self.data_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"æ•°æ®ç›®å½•å·²ç¡®è®¤: {self.data_dir}")
        except Exception as e:
            logger.error(f"åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {str(e)}")
            raise
    
    def _init_results_file(self):
        """åˆå§‹åŒ–ç»“æœæ–‡ä»¶"""
        if not self.analysis_path.exists():
            # åˆ›å»ºç©ºçš„DataFrameå¹¶è®¾ç½®åˆ—
            columns = [
                'ä»£å¸åœ°å€', 'åˆ†ææ—¶é—´',
                # Twitterå’ŒDeepseekåˆ†æç»“æœ
                'æœç´¢å…³é”®è¯', 'å™äº‹ä¿¡æ¯', 'å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦', 'å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›', 
                'å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼', 'åŸå§‹æ¨æ–‡æ•°é‡',
                # CoinGeckoæ•°æ®
                'symbol', 'name', 'network', 'token_id',
                'fdv_usd', 'volume_usd_24h', 'price_change_m5', 'price_change_h1',
                'm5_buys', 'm5_sells', 'm15_buys', 'm15_sells', 'pool_created_at',
                # æ ‡è®°å­—æ®µ
                'twitter_analyzed', 'coingecko_analyzed', 'sent_to_feishu'
            ]
            
            df = pd.DataFrame(columns=columns)
            df.to_excel(self.analysis_path, index=False)
            logger.info(f"åˆ›å»ºäº†æ–°çš„æ•´åˆåˆ†æç»“æœæ–‡ä»¶: {self.analysis_path}")
    
    def register_token(self, token_address):
        """
        æ³¨å†Œä¸€ä¸ªæ–°çš„ä»£å¸åˆ†æä»»åŠ¡
        
        å‚æ•°:
            token_address: ä»£å¸åœ°å€
        
        è¿”å›:
            ä»»åŠ¡ID
        """
        if token_address in self.pending_tokens:
            logger.info(f"ä»£å¸ {token_address} å·²åœ¨å¤„ç†é˜Ÿåˆ—ä¸­")
            return
            
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
        if token_address in self.processed_tokens:
            logger.info(f"ä»£å¸ {token_address} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡")
            return
            
        # å°†ä»£å¸æ·»åŠ åˆ°å¾…å¤„ç†åˆ—è¡¨
        self.pending_tokens[token_address] = {
            'ä»£å¸åœ°å€': token_address,
            'åˆ†ææ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'twitter_analyzed': False,
            'coingecko_analyzed': False,
            'sent_to_feishu': False
        }
        
        logger.info(f"æ³¨å†Œäº†æ–°çš„ä»£å¸åˆ†æä»»åŠ¡: {token_address}")
        
        # å°†æ–°ä»»åŠ¡ä¿å­˜åˆ°æ–‡ä»¶
        try:
            df = pd.read_excel(self.analysis_path)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if not ((df['ä»£å¸åœ°å€'] == token_address) & (df['sent_to_feishu'] == False)).any():
                # æ·»åŠ æ–°è¡Œ
                new_row = pd.DataFrame([self.pending_tokens[token_address]])
                df = pd.concat([df, new_row], ignore_index=True)
                df.to_excel(self.analysis_path, index=False)
                logger.info(f"å·²å°†ä»£å¸ {token_address} æ·»åŠ åˆ°åˆ†æç»“æœæ–‡ä»¶")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–°ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
    
    def update_twitter_analysis(self, token_address, analysis_result):
        """
        æ›´æ–°Twitterå’ŒDeepseekåˆ†æç»“æœ
        
        å‚æ•°:
            token_address: ä»£å¸åœ°å€
            analysis_result: åˆ†æç»“æœå­—å…¸
        """
        if token_address not in self.pending_tokens:
            self.register_token(token_address)
        
        # æ›´æ–°å†…å­˜ä¸­çš„åˆ†æç»“æœ
        token_data = self.pending_tokens[token_address]
        token_data.update({
            'æœç´¢å…³é”®è¯': analysis_result.get('æœç´¢å…³é”®è¯', ''),
            'å™äº‹ä¿¡æ¯': analysis_result.get('å™äº‹ä¿¡æ¯', ''),
            'å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦': analysis_result.get('å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦', ''),
            'å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›': analysis_result.get('å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›', ''),
            'å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼': analysis_result.get('å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼', ''),
            'åŸå§‹æ¨æ–‡æ•°é‡': analysis_result.get('åŸå§‹æ¨æ–‡æ•°é‡', 0),
            'twitter_analyzed': True
        })
        
        # æ›´æ–°æ–‡ä»¶
        self._update_analysis_file(token_address, token_data)
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†æéƒ½å®Œæˆ
        self._check_and_send(token_address)
        
    def update_coingecko_analysis(self, token_address, coin_data):
        """
        æ›´æ–°CoinGeckoåˆ†æç»“æœ
        
        å‚æ•°:
            token_address: ä»£å¸åœ°å€
            coin_data: CoinGeckoåˆ†æç»“æœå­—å…¸
        """
        if token_address not in self.pending_tokens:
            self.register_token(token_address)
        
        # æ›´æ–°å†…å­˜ä¸­çš„åˆ†æç»“æœ
        token_data = self.pending_tokens[token_address]
        token_data.update({
            'symbol': coin_data.get('symbol', ''),
            'name': coin_data.get('name', ''),
            'network': coin_data.get('network', ''),
            'token_id': coin_data.get('token_id', ''),
            'fdv_usd': coin_data.get('fdv_usd', ''),
            'volume_usd_24h': coin_data.get('volume_usd_24h', ''),
            'price_change_m5': coin_data.get('price_change_m5', ''),
            'price_change_h1': coin_data.get('price_change_h1', ''),
            'm5_buys': coin_data.get('m5_buys', 0),
            'm5_sells': coin_data.get('m5_sells', 0),
            'm15_buys': coin_data.get('m15_buys', 0),
            'm15_sells': coin_data.get('m15_sells', 0),
            'pool_created_at': coin_data.get('pool_created_at', ''),
            'coingecko_analyzed': True
        })
        
        # æ›´æ–°æ–‡ä»¶
        self._update_analysis_file(token_address, token_data)
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†æéƒ½å®Œæˆ
        self._check_and_send(token_address)
    
    def _update_analysis_file(self, token_address, token_data):
        """ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æ›´æ–°åˆ†æç»“æœ"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = self.analysis_path.with_suffix('.tmp')
            
            # è¯»å–ç°æœ‰æ•°æ®
            if self.analysis_path.exists():
                df = pd.read_excel(self.analysis_path)
            else:
                df = pd.DataFrame(columns=self._get_columns())
            
            # æ›´æ–°æ•°æ®
            mask = df['ä»£å¸åœ°å€'] == token_address
            if mask.any():
                for col, value in token_data.items():
                    if col in df.columns:
                        df.loc[mask, col] = value
            else:
                new_row = pd.DataFrame([token_data])
                df = pd.concat([df, new_row], ignore_index=True)
            
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            df.to_excel(temp_file, index=False)
            
            # æ›¿æ¢åŸæ–‡ä»¶
            if self.analysis_path.exists():
                self.analysis_path.unlink()
            temp_file.rename(self.analysis_path)
            
            logger.info(f"å·²æ›´æ–°ä»£å¸ {token_address} çš„åˆ†æç»“æœ")
            
        except Exception as e:
            logger.error(f"æ›´æ–°åˆ†ææ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            if temp_file.exists():
                temp_file.unlink()
            raise
    
    def _check_and_send(self, token_address):
        """æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åˆ†æéƒ½å®Œæˆï¼Œå¦‚æœæ˜¯åˆ™å‘é€åˆ°é£ä¹¦"""
        token_data = self.pending_tokens.get(token_address)
        
        if not token_data:
            return
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨å‘é€ä¸­
        if token_address in self.sending_tokens:
            return
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆ
        need_both = self._need_both_analyses()
        
        # å…³é”®ä¿®æ”¹: æ£€æŸ¥æ˜¯å¦å·²ç»å‘é€è¿‡æˆ–è€…æ¡ä»¶ä¸æ»¡è¶³
        if token_data.get('sent_to_feishu'):
            return  # å¦‚æœå·²ç»å‘é€è¿‡ï¼Œç›´æ¥è¿”å›ï¼Œä¸é‡å¤å‘é€
        
        # å¦‚æœéœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆï¼Œä½†å…¶ä¸­ä¸€ä¸ªæœªå®Œæˆï¼Œåˆ™è¿”å›
        if need_both and not (token_data.get('twitter_analyzed') and token_data.get('coingecko_analyzed')):
            return
        
        # å¦‚æœä»£ç æ‰§è¡Œåˆ°è¿™é‡Œï¼Œæ„å‘³ç€å¯ä»¥å‘é€æ¶ˆæ¯äº†
        # æ·»åŠ å‘é€ä¸­æ ‡å¿—
        self.sending_tokens.add(token_address)
        
        try:
            # å‘é€æ¶ˆæ¯
            success = self._send_integrated_analysis(token_address)
            
            if success:
                # æ›´æ–°çŠ¶æ€
                token_data['sent_to_feishu'] = True
                self._update_analysis_file(token_address, token_data)
                
                # æ·»åŠ åˆ°å·²å¤„ç†é›†åˆ
                self.processed_tokens.add(token_address)
                
                # æ¸…ç†å†…å­˜ä¸­çš„æ•°æ®
                if token_address in self.pending_tokens:
                    del self.pending_tokens[token_address]
        finally:
            # ç§»é™¤å‘é€ä¸­æ ‡å¿—
            self.sending_tokens.remove(token_address)
    
    def _need_both_analyses(self):
        """ç¡®å®šæ˜¯å¦éœ€è¦åŒæ—¶å®ŒæˆTwitterå’ŒCoinGeckoåˆ†æ"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®é…ç½®æˆ–å‘½ä»¤è¡Œå‚æ•°å†³å®šæ˜¯å¦éœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆ
        # é»˜è®¤éœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆ
        return True
    
    def _send_integrated_analysis(self, token_address):
        """å‘é€æ•´åˆåçš„åˆ†æç»“æœåˆ°é£ä¹¦å’ŒTelegram"""
        try:
            token_data = self.pending_tokens.get(token_address)
            
            if not token_data:
                logger.error(f"æ‰¾ä¸åˆ°ä»£å¸ {token_address} çš„åˆ†ææ•°æ®")
                return False
            
            # æ„å»ºæ¶ˆæ¯
            message = f"""ğŸ”é‡‘ç‹—é¢„è­¦

ğŸ“Œ ä»£å¸åœ°å€: {token_address}"""

            # æ·»åŠ  CoinGecko æ•°æ®
            message += f"""
ğŸª™ åç§°: {token_data.get('name', 'N/A')} ({token_data.get('symbol', 'N/A')})
ğŸŒ ç½‘ç»œ: {token_data.get('network', 'N/A')}

ğŸ’° å¸‚åœºæ•°æ®:
â€¢ å¸‚å€¼: {coingecko_api.format_currency(token_data.get('fdv_usd', 'N/A'))}
â€¢ 24å°æ—¶äº¤æ˜“é‡: {coingecko_api.format_currency(token_data.get('volume_usd_24h', 'N/A'))}
â€¢ åˆ›å»ºæ—¶é—´: {token_data.get('pool_created_at', 'N/A')}

ğŸ“ˆ ä»·æ ¼å˜åŠ¨:
â€¢ 5åˆ†é’Ÿ: {coingecko_api.format_percentage(token_data.get('price_change_m5', 'N/A'))}
â€¢ 1å°æ—¶: {coingecko_api.format_percentage(token_data.get('price_change_h1', 'N/A'))}

ğŸ”„ æœ€è¿‘äº¤æ˜“æ¬¡æ•°:
â€¢ 5åˆ†é’Ÿå†…: ä¹°å…¥ {token_data.get('m5_buys', 0)} æ¬¡, å–å‡º {token_data.get('m5_sells', 0)} æ¬¡
â€¢ 15åˆ†é’Ÿå†…: ä¹°å…¥ {token_data.get('m15_buys', 0)} æ¬¡, å–å‡º {token_data.get('m15_sells', 0)} æ¬¡"""

            # æ·»åŠ  Twitter åˆ†ææ•°æ®
            message += f"""

ğŸ“ å™äº‹ä¿¡æ¯:
{token_data.get('å™äº‹ä¿¡æ¯', 'N/A')}

ğŸŒ¡ï¸ å¯æŒç»­æ€§åˆ†æ:
â€¢ ç¤¾åŒºçƒ­åº¦: {token_data.get('å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦', 'N/A')}
â€¢ ä¼ æ’­æ½œåŠ›: {token_data.get('å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›', 'N/A')}
â€¢ çŸ­æœŸæŠ•æœºä»·å€¼: {token_data.get('å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼', 'N/A')}"""

            # å‘é€åˆ°é£ä¹¦
            feishu_success = True
            if self.feishu_bot and self.feishu_chat_id:
                feishu_success = self.feishu_bot.send_message(
                    receive_id=self.feishu_chat_id,
                    content=message,
                    use_webhook=False
                )
                
                if feishu_success:
                    logger.info(f"å·²æˆåŠŸå‘é€ä»£å¸ {token_address} çš„ç»¼åˆåˆ†æç»“æœåˆ°é£ä¹¦")
                else:
                    logger.error(f"å‘é€ä»£å¸ {token_address} çš„ç»¼åˆåˆ†æç»“æœåˆ°é£ä¹¦å¤±è´¥")
            
            # å‘é€åˆ°Telegram
            telegram_success = True
            if self.telegram_bot and self.telegram_chat_id:
                telegram_success = self.telegram_bot.send_message(
                    chat_id=self.telegram_chat_id,
                    text=message
                )
                
                if telegram_success:
                    logger.info(f"å·²æˆåŠŸå‘é€ä»£å¸ {token_address} çš„ç»¼åˆåˆ†æç»“æœåˆ°Telegram")
                else:
                    logger.error(f"å‘é€ä»£å¸ {token_address} çš„ç»¼åˆåˆ†æç»“æœåˆ°Telegramå¤±è´¥")
            
            # å¦‚æœè‡³å°‘ä¸€ä¸ªå¹³å°å‘é€æˆåŠŸï¼Œåˆ™è¿”å›æˆåŠŸ
            return feishu_success or telegram_success
            
        except Exception as e:
            logger.error(f"å‘é€ç»¼åˆåˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return False

    def _save_with_retry(self, func, max_retries=3, delay=1):
        """å¸¦é‡è¯•æœºåˆ¶çš„ä¿å­˜æ“ä½œ"""
        for attempt in range(max_retries):
            try:
                return func()
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                logger.warning(f"ä¿å­˜æ“ä½œå¤±è´¥ï¼Œ{delay}ç§’åé‡è¯•: {str(e)}")
                time.sleep(delay)

# ç„¶ååˆ›å»ºå®ä¾‹
# ä»é…ç½®æ–‡ä»¶è·å–app_idå’Œapp_secret
import json

def load_config(config_file='config.json'):
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
        return {}

# åŠ è½½é…ç½®
config = load_config()
app_id = config.get('feishu_app_id', '')
app_secret = config.get('feishu_app_secret', '')

# ä¿®æ”¹è¿™ä¸€è¡Œï¼Œä¼ å…¥app_idå’Œapp_secret
integrator = AnalysisIntegrator(app_id=app_id, app_secret=app_secret)

# ç„¶åæ˜¯å…¶ä»–ç±»å®šä¹‰
class MemeAnalyzer:
    def __init__(self, config_file='config.json', api_key=None):
        self.config = self.load_config(config_file)
        self.setup_directories()
        
        # APIé…ç½®
        self.base_url = self.config.get("base_url", "https://api.siliconflow.cn")
        self.api_key = api_key or self.config.get("api_keys", {}).get("deepseek")
        
        # éªŒè¯ API key æ ¼å¼
        if not self.api_key:
            logger.error("Deepseek API keyæœªè®¾ç½®")
            raise ValueError("Deepseek API key is required")
        elif not self.api_key.startswith("sk-"):
            logger.error("Deepseek API key æ ¼å¼é”™è¯¯ï¼Œåº”è¯¥ä»¥ sk- å¼€å¤´")
            raise ValueError("Invalid Deepseek API key format")
            
        logger.info(f"Deepseek API key æ ¼å¼éªŒè¯é€šè¿‡")
        
        self.min_occurrence_threshold = self.config.get("min_occurrence_threshold", 2)
        self.term_history = {}
        self.history_cleanup_threshold = timedelta(hours=self.config.get("history_cleanup_threshold", 24))
        
        # åˆå§‹åŒ–é£ä¹¦æœºå™¨äºº - ä¼ å…¥æ­£ç¡®çš„å‡­æ®
        self.app_id = "cli_a736cea2ff78100d"
        self.app_secret = "C9FsC6CnJz3CLf0PEz0NQewkuH6uvCdS"
        self.feishu_bot = FeishuBot(app_id=self.app_id, app_secret=self.app_secret)
        
        self.feishu_chat_id = self.config.get("feishu_chat_id", "oc_a2d2c5616c900bda2ab8e13a77361287")
        self.integrator = integrator

    def load_config(self, config_file):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8-sig') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
            raise

    def setup_directories(self):
        """è®¾ç½®å¿…è¦çš„ç›®å½•"""
        self.data_dir = Path('data')
        self.data_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®Excelæ–‡ä»¶è·¯å¾„
        self.twitter_results_path = self.data_dir / 'twitter_results.xlsx'
        self.meme_path = self.data_dir / 'meme.xlsx'

    async def analyze_tweets(self, term: str, tweets: List[dict]) -> dict:
        """ä½¿ç”¨ Deepseek API åˆ†ææ¨æ–‡"""
        try:
            # æå–æ¨æ–‡å†…å®¹å¹¶æ¸…ç†
            tweet_texts = []
            for tweet in tweets:
                text = tweet.get('text', '').strip()
                if text:
                    # æ¸…ç†åˆçº¦åœ°å€
                    text = re.sub(r'[A-Za-z0-9]{32,}', '', text)
                    # æ¸…ç†URL
                    text = re.sub(r'https?://\S+', '', text)
                    # æ¸…ç†å¤šä½™ç©ºç™½
                    text = ' '.join(text.split())
                    if text.strip():  # ç¡®ä¿æ¸…ç†åè¿˜æœ‰å†…å®¹
                        tweet_texts.append(text)
            
            if not tweet_texts:
                logger.warning(f"æ¸…ç†åæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¨æ–‡å†…å®¹ç”¨äºåˆ†æ")
                return self._get_default_analysis(term, len(tweets))
            
            # å»é‡
            tweet_texts = list(set(tweet_texts))
            
            # ä¿®æ”¹è®¤è¯å¤´æ ¼å¼
            headers = {
                "Authorization": f"Bearer {self.api_key}",  # ç¡®ä¿æ˜¯ Bearer è®¤è¯
                "Content-Type": "application/json",
                "Accept": "application/json"  # æ·»åŠ  Accept å¤´
            }
            
            # æ£€æŸ¥å¹¶è®°å½• API keyï¼ˆéšè—éƒ¨åˆ†å†…å®¹ï¼‰
            masked_key = f"{self.api_key[:6]}...{self.api_key[-4:]}" if self.api_key else "None"
            logger.info(f"ä½¿ç”¨çš„ API key: {masked_key}")
            
            data = {
                "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
                "messages": [
                    {
                        "role": "user",
                        "content": f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸åˆ†æå¸ˆï¼Œæˆ‘å¸Œæœ›ä½ èƒ½å¸®æˆ‘è¯„ä¼°è¿™ä¸ª Meme å¸çš„æ½œåŠ›ï¼Œå¹¶ç»™å‡ºè¯¦ç»†çš„åˆ†æå’Œå»ºè®®ï¼Œè¯·åˆ†æä»¥ä¸‹å…³äºåŠ å¯†è´§å¸çš„æ¨æ–‡å†…å®¹ï¼š

{chr(10).join(tweet_texts)}

è¯·ä»ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢åˆ†åˆ«è¿›è¡Œåˆ†æï¼Œåˆ†2ç‚¹ï¼Œå¹¶ç”¨ä¸­æ–‡å›ç­”ï¼Œæˆ‘éœ€è¦çš„ç»“æœä¸è¶…è¿‡100å­—ï¼Œä½ éœ€è¦åˆ†ä»¥ä¸‹2ç‚¹æ˜ç¡®çš„è¿”å›ï¼š

1. å™äº‹ä¿¡æ¯ï¼šç”¨2-3å¥è¯æ€»ç»“è¿™ä¸ªmemeå¸çš„æ ¸å¿ƒå’Œå®ƒçš„æ ¸å¿ƒå–ç‚¹ã€‚

2. å¯æŒç»­æ€§ï¼šä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°ï¼š
   - ç¤¾åŒºçƒ­åº¦
   - ä¼ æ’­æ½œåŠ›
   - çŸ­æœŸæŠ•æœºä»·å€¼"""
                    }
                ],
                "stream": False,
                "temperature": 0.7,
                "max_tokens": 512,
                "top_p": 0.7,
                "top_k": 50,
                "frequency_penalty": 0.5
            }
            
            logger.info(f"å‘é€Deepseek APIè¯·æ±‚ï¼Œåˆ†æ {len(tweet_texts)} æ¡æ¨æ–‡")
            
            max_retries = 3
            retry_delay = 10
            
            for attempt in range(max_retries):
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            f"{self.base_url}/v1/chat/completions",
                            headers=headers,
                            json=data,
                            timeout=aiohttp.ClientTimeout(total=30)
                        ) as response:
                            response_text = await response.text()
                            logger.info(f"Deepseek APIå“åº”çŠ¶æ€ç : {response.status}")
                            logger.info(f"Deepseek APIå“åº”å¤´: {dict(response.headers)}")
                            logger.info(f"Deepseek APIè¯·æ±‚æ•°æ®: {json.dumps(data, ensure_ascii=False)}")
                            logger.info(f"Deepseek APIå“åº”å†…å®¹: {response_text}")
                            
                            if response.status == 200:
                                result = json.loads(response_text)
                                if 'choices' in result and len(result['choices']) > 0:
                                    analysis = result['choices'][0]['message']['content']
                                    
                                    # è§£æåˆ†æç»“æœ
                                    narrative = ""
                                    community_heat = ""
                                    spread_potential = ""
                                    investment_value = ""
                                    
                                    # ç§»é™¤æ‰€æœ‰ Markdown æ ‡è®°
                                    analysis = analysis.replace('**', '')
                                    
                                    # åˆ†å‰²ä¸»è¦éƒ¨åˆ†
                                    parts = analysis.split('\n\n')
                                    
                                    # è§£æå™äº‹ä¿¡æ¯å’Œå¯æŒç»­æ€§è¯„ä¼°
                                    for part in parts:
                                        if '1. å™äº‹ä¿¡æ¯' in part:
                                            narrative = part.replace('1. å™äº‹ä¿¡æ¯ï¼š', '').strip()
                                        elif '2. å¯æŒç»­æ€§' in part:
                                            lines = part.split('\n')
                                            for line in lines:
                                                line = line.strip()
                                                if 'ç¤¾åŒºçƒ­åº¦' in line:
                                                    community_heat = line.split('ï¼š')[1].strip() if 'ï¼š' in line else ''
                                                elif 'ä¼ æ’­æ½œåŠ›' in line:
                                                    spread_potential = line.split('ï¼š')[1].strip() if 'ï¼š' in line else ''
                                                elif 'çŸ­æœŸæŠ•æœºä»·å€¼' in line:
                                                    investment_value = line.split('ï¼š')[1].strip() if 'ï¼š' in line else ''
                                    
                                    result_dict = {
                                        'æœç´¢å…³é”®è¯': term,
                                        'å™äº‹ä¿¡æ¯': narrative,
                                        'å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦': community_heat,
                                        'å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›': spread_potential,
                                        'å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼': investment_value,
                                        'åŸå§‹æ¨æ–‡æ•°é‡': len(tweets),
                                        'åˆ†ææ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                                    }
                                    
                                    logger.info(f"æˆåŠŸå®Œæˆåˆ†æ: {result_dict}")
                                    
                                    # æ›´æ–°æ•´åˆå™¨çš„ Twitter åˆ†æç»“æœ
                                    self.integrator.update_twitter_analysis(term, result_dict)
                                    
                                    return result_dict
                                    
                            elif response.status == 400:
                                logger.error(f"Deepseek APIè¯·æ±‚å‚æ•°é”™è¯¯: {response_text}")
                                try:
                                    error_data = json.loads(response_text)
                                    logger.error(f"é”™è¯¯è¯¦æƒ…: {json.dumps(error_data, ensure_ascii=False, indent=2)}")
                                except:
                                    logger.error(f"æ— æ³•è§£æé”™è¯¯å“åº”: {response_text}")
                                return self._get_default_analysis(term, len(tweets))
                            elif response.status == 401:
                                logger.error("Deepseek APIè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥API key")
                                return self._get_default_analysis(term, len(tweets))
                            elif response.status == 429:
                                logger.warning("Deepseek APIé€Ÿç‡é™åˆ¶")
                                return self._get_default_analysis(term, len(tweets))
                            else:
                                logger.error(f"Deepseek APIè¯·æ±‚å¤±è´¥: {response.status}")
                                logger.error(f"é”™è¯¯å“åº”: {response_text}")
                                return self._get_default_analysis(term, len(tweets))
                                
                except Exception as e:
                    logger.error(f"è°ƒç”¨Deepseek APIæ—¶å‡ºé”™: {str(e)}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(retry_delay)
                        continue
                    
            return self._get_default_analysis(term, len(tweets))
            
        except Exception as e:
            logger.error(f"åˆ†ææ¨æ–‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            logger.exception(e)
            return self._get_default_analysis(term, len(tweets))

    def _get_default_analysis(self, term: str, tweet_count: int) -> dict:
        """è¿”å›é»˜è®¤çš„åˆ†æç»“æœ"""
        return {
            'æœç´¢å…³é”®è¯': term,
            'å™äº‹ä¿¡æ¯': f'APIè®¤è¯å¤±è´¥ï¼Œæ— æ³•åˆ†æã€‚å…±æœ‰{tweet_count}æ¡æ¨æ–‡',
            'å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦': 'æœªçŸ¥',
            'å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›': 'æœªçŸ¥',
            'å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼': 'æœªçŸ¥',
            'åŸå§‹æ¨æ–‡æ•°é‡': tweet_count,
            'åˆ†ææ—¶é—´': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

    async def process_history_file(self):
        """å¤„ç†å†å²æ•°æ®æ–‡ä»¶"""
        try:
            logger.info("å¼€å§‹å¤„ç†meme.xlsxå†å²æ•°æ®")
            
            if not self.meme_path.exists():
                logger.error("meme.xlsxæ–‡ä»¶ä¸å­˜åœ¨")
                return
                
            df = pd.read_excel(self.meme_path)
            logger.info(f"åŠ è½½äº† {len(df)} æ¡å†å²è®°å½•")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            if 'å†…å®¹' not in df.columns:
                logger.error("meme.xlsxæ–‡ä»¶ç¼ºå°‘'å†…å®¹'åˆ—")
                return
                
            # åˆ›å»º CoinGecko åˆ†æå™¨å®ä¾‹
            coingecko_analyzer = CoinGeckoAnalyzer()
            
            # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
            for _, row in df.iterrows():
                content = row['å†…å®¹']
                logger.info(f"å¤„ç†å…³é”®è¯: {content}")
                
                # æ³¨å†Œåˆ°æ•´åˆå™¨
                self.integrator.register_token(content)
                
                # æœç´¢Twitter
                tweets = await twitter_api.search_tweets(content)
                logger.info(f"æ‰¾åˆ° {len(tweets)} æ¡ç›¸å…³æ¨æ–‡")
                
                if tweets:
                    # åˆ†ææ¨æ–‡
                    analysis = await self.analyze_tweets(content, tweets)
                    if analysis:
                        logger.info(f"å·²ä¿å­˜å…³é”®è¯ '{content}' çš„åˆ†æç»“æœ")
                        
                        # åœ¨è¿™é‡Œè°ƒç”¨ CoinGecko åˆ†æ
                        logger.info(f"å¼€å§‹å¯¹ '{content}' è¿›è¡Œ CoinGecko åˆ†æ...")
                        token_data = await coingecko_analyzer.analyze_token(content)
                        if token_data:
                            logger.info(f"å®Œæˆå¯¹ '{content}' çš„ CoinGecko åˆ†æ")
                        else:
                            logger.warning(f"CoinGecko æ— æ³•åˆ†æä»£å¸ '{content}'")
                    else:
                        logger.warning(f"å…³é”®è¯ '{content}' çš„åˆ†æç»“æœä¸ºç©º")
                else:
                    logger.warning(f"å…³é”®è¯ '{content}' æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ¨æ–‡")
            
            logger.info("å†å²æ•°æ®å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"å¤„ç†å†å²æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            logger.exception(e)

class BacktestProcessor:
    def __init__(self):
        self.data_dir = Path('data')
        self.data_dir.mkdir(exist_ok=True)
        self.meme_path = self.data_dir / 'meme.xlsx'
        self.twitter_results_path = self.data_dir / 'twitter_results.xlsx'
        
    async def save_meme_data(self, meme_data):
        """ä¿å­˜memeæ•°æ®åˆ°Excel"""
        try:
            if self.meme_path.exists():
                df_meme = pd.read_excel(self.meme_path)
                df_meme = pd.concat([df_meme, pd.DataFrame(meme_data)], ignore_index=True)
            else:
                df_meme = pd.DataFrame(meme_data)
            
            df_meme.to_excel(self.meme_path, index=False)
            logger.info(f"æˆåŠŸä¿å­˜ {len(meme_data)} æ¡memeæ•°æ®åˆ°Excel")
        except Exception as e:
            logger.error(f"ä¿å­˜memeæ•°æ®æ—¶å‡ºé”™: {e}")

# åˆ›å»ºå…¨å±€å®ä¾‹
processor = BacktestProcessor()

async def process_message(message_data: Dict[str, Any]) -> None:
    """å¤„ç†æ¥è‡ªDiscordçš„æ¶ˆæ¯"""
    try:
        # æå–å¤„ç†å¥½çš„æ•°æ®
        meme_data = message_data.get('meme_data', [])
        search_terms = message_data.get('search_terms', [])
        
        # ä¿å­˜memeæ•°æ®
        if meme_data:
            await processor.save_meme_data(meme_data)
            
    except Exception as e:
        logger.error(f"å¤„ç†æ¶ˆæ¯æ•°æ®æ—¶å‡ºé”™: {str(e)}")

class CoinGeckoAnalyzer:
    """è´Ÿè´£å¤„ç†CoinGeckoç›¸å…³çš„ä»£å¸åˆ†æåŠŸèƒ½"""
    
    def __init__(self, api_key=None):
        """
        åˆå§‹åŒ–CoinGeckoåˆ†æå™¨
        
        å‚æ•°:
            api_key: CoinGecko API å¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æ¨¡å—é»˜è®¤å€¼
        """
        self.api_key = api_key or coingecko_api.API_KEY
        self.data_dir = Path('data')
        self.data_dir.mkdir(exist_ok=True)
        self.meme_path = self.data_dir / 'meme.xlsx'
        
        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        self.client = coingecko_api.CoinGeckoAPI(self.api_key)
        
        # æ·»åŠ æ•´åˆå™¨å¼•ç”¨
        self.integrator = integrator
        
        logger.info("CoinGeckoåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def analyze_token(self, token_address):
        """
        åˆ†æå•ä¸ªä»£å¸çš„äº¤æ˜“æ•°æ®
        
        å‚æ•°:
            token_address: ä»£å¸åœ°å€
            
        è¿”å›:
            åˆ†æç»“æœå­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹åˆ†æä»£å¸: {token_address}")
            
            # æ¸…ç†å’ŒéªŒè¯tokenåœ°å€
            if isinstance(token_address, str):
                token_address = token_address.strip()
                # å®½æ¾çš„éªŒè¯ï¼Œå…è®¸ä¸åŒç½‘ç»œçš„åœ°å€æ ¼å¼
                if len(token_address) < 20:
                    logger.warning(f"è·³è¿‡æ— æ•ˆçš„åœ°å€æ ¼å¼: {token_address}")
                    return None
            
            # å°è¯•å¤šç½‘ç»œè¯·æ±‚
            token_info, network = await coingecko_api.try_multiple_networks(self.client, token_address)
            
            if not token_info or not network:
                logger.warning(f"æ‰€æœ‰ç½‘ç»œå‡æœªæ‰¾åˆ°ä»£å¸: {token_address}")
                return None
            
            # æå–åŸºæœ¬ä¿¡æ¯å’Œäº¤æ˜“æ•°æ®
            token_data = {
                'token_address': token_address,
                'network': network
            }
            
            # æå–è¯¦ç»†å±æ€§
            attributes = token_info['data']['attributes']
            
            # åŸºæœ¬ä¿¡æ¯
            token_data['token_id'] = token_info['data'].get('id', '')
            token_data['address'] = attributes.get('address', '')
            token_data['symbol'] = attributes.get('symbol', '')
            token_data['name'] = attributes.get('name', '')
            
            # å¸‚å€¼å’Œäº¤æ˜“é‡
            token_data['fdv_usd'] = attributes.get('fdv_usd', '')
            token_data['fdv_usd_formatted'] = coingecko_api.format_currency(attributes.get('fdv_usd', ''))
            
            # 24å°æ—¶äº¤æ˜“é‡
            if 'volume_usd' in attributes and 'h24' in attributes['volume_usd']:
                volume_24h = attributes['volume_usd']['h24']
                token_data['volume_usd_24h'] = volume_24h
                token_data['volume_usd_24h_formatted'] = coingecko_api.format_currency(volume_24h)
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨includedæ•°æ®ï¼ˆæ± ä¿¡æ¯ï¼‰
            if 'included' in token_info and token_info['included'] and len(token_info['included']) > 0:
                # æå–ç¬¬ä¸€ä¸ªæ± çš„æ•°æ®
                pool = token_info['included'][0]
                if 'attributes' in pool:
                    pool_attrs = pool['attributes']
                    
                    # æ·»åŠ æ± åˆ›å»ºæ—¶é—´
                    if 'pool_created_at' in pool_attrs:
                        utc_time = pool_attrs['pool_created_at']
                        token_data['pool_created_at'] = coingecko_api.convert_utc_to_utc8(utc_time)
                    
                    # ä»·æ ¼å˜åŠ¨
                    if 'price_change_percentage' in pool_attrs:
                        price_changes = pool_attrs['price_change_percentage']
                        # 5åˆ†é’Ÿä»·æ ¼å˜åŒ–
                        if 'm5' in price_changes:
                            m5_change = price_changes['m5']
                            token_data['price_change_m5'] = m5_change
                            token_data['price_change_m5_formatted'] = coingecko_api.format_percentage(m5_change)
                        # 1å°æ—¶ä»·æ ¼å˜åŒ–
                        if 'h1' in price_changes:
                            h1_change = price_changes['h1']
                            token_data['price_change_h1'] = h1_change
                            token_data['price_change_h1_formatted'] = coingecko_api.format_percentage(h1_change)
                    
                    # äº¤æ˜“æ•°é‡
                    if 'transactions' in pool_attrs:
                        txs = pool_attrs['transactions']
                        # 5åˆ†é’Ÿäº¤æ˜“
                        if 'm5' in txs:
                            token_data['m5_buys'] = txs['m5'].get('buys', 0)
                            token_data['m5_sells'] = txs['m5'].get('sells', 0)
                        # 15åˆ†é’Ÿäº¤æ˜“
                        if 'm15' in txs:
                            token_data['m15_buys'] = txs['m15'].get('buys', 0)
                            token_data['m15_sells'] = txs['m15'].get('sells', 0)
            
            logger.info(f"æˆåŠŸåˆ†æä»£å¸ {token_address} çš„äº¤æ˜“æ•°æ®")
            
            # åœ¨æˆåŠŸåˆ†æä»£å¸åï¼Œæ›´æ–°æ•´åˆå™¨çš„æ•°æ®
            if token_data:
                self.integrator.update_coingecko_analysis(token_address, token_data)
            
            return token_data
            
        except Exception as e:
            logger.error(f"åˆ†æä»£å¸ {token_address} æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return None
    
    async def send_token_analysis_to_feishu(self, token_data):
        """å°†ä»£å¸åˆ†æç»“æœå‘é€åˆ°é£ä¹¦"""
        if not token_data:
            logger.warning("æ— æœ‰æ•ˆä»£å¸æ•°æ®ï¼Œè·³è¿‡å‘é€åˆ°é£ä¹¦")
            return False
        
        try:
            # åˆ›å»ºé€‚åˆé£ä¹¦æ˜¾ç¤ºçš„æ¶ˆæ¯æ ¼å¼
            message = f"""ğŸª™ ä»£å¸äº¤æ˜“æ•°æ®åˆ†æ

ğŸ“Š åŸºæœ¬ä¿¡æ¯:
â€¢ åç§°: {token_data.get('name', 'N/A')} ({token_data.get('symbol', 'N/A')})
â€¢ ç½‘ç»œ: {token_data.get('network', 'N/A')}
â€¢ åœ°å€: {token_data.get('address', 'N/A')}

ğŸ’° å¸‚åœºæ•°æ®:
â€¢ å…¨é¢å¸‚å€¼: {token_data.get('fdv_usd_formatted', 'N/A')}
â€¢ 24å°æ—¶äº¤æ˜“é‡: {token_data.get('volume_usd_24h_formatted', 'N/A')}

ğŸ“ˆ ä»·æ ¼å˜åŠ¨:
â€¢ 5åˆ†é’Ÿ: {token_data.get('price_change_m5_formatted', 'N/A')}
â€¢ 1å°æ—¶: {token_data.get('price_change_h1_formatted', 'N/A')}

ğŸ”„ æœ€è¿‘äº¤æ˜“:
â€¢ 5åˆ†é’Ÿå†…: ä¹°å…¥ {token_data.get('m5_buys', 0)} æ¬¡, å–å‡º {token_data.get('m5_sells', 0)} æ¬¡
â€¢ 15åˆ†é’Ÿå†…: ä¹°å…¥ {token_data.get('m15_buys', 0)} æ¬¡, å–å‡º {token_data.get('m15_sells', 0)} æ¬¡

â±ï¸ æ± åˆ›å»ºæ—¶é—´: {token_data.get('pool_created_at', 'N/A')}"""

            success = self.feishu_bot.send_message(
                receive_id=self.feishu_chat_id,
                content=message,
                use_webhook=False
            )
            
            if success:
                logger.info("ä»£å¸åˆ†æç»“æœå·²æˆåŠŸå‘é€åˆ°é£ä¹¦")
            else:
                logger.error("å‘é€ä»£å¸åˆ†æç»“æœåˆ°é£ä¹¦å¤±è´¥")
            
            return success
            
        except Exception as e:
            logger.error(f"å‘é€ä»£å¸åˆ†æç»“æœåˆ°é£ä¹¦æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return False
    
    async def process_meme_file(self, start_index=0, batch_size=10, save_interval=60):
        """
        æ‰¹é‡å¤„ç†meme.xlsxæ–‡ä»¶ä¸­çš„ä»£å¸åœ°å€
        
        å‚æ•°:
            start_index: å¼€å§‹å¤„ç†çš„ç´¢å¼•
            batch_size: æ¯æ‰¹å¤„ç†çš„æ•°é‡
            save_interval: ä¿å­˜ç»“æœçš„æ—¶é—´é—´éš”(ç§’)
        """
        try:
            logger.info("å¼€å§‹æ‰¹é‡å¤„ç†meme.xlsxä¸­çš„ä»£å¸åœ°å€")
            
            if not self.meme_path.exists():
                logger.error("meme.xlsxæ–‡ä»¶ä¸å­˜åœ¨")
                return
            
            # è·å–å½“å‰æ—¶é—´æˆ³
            current_time = time.strftime('%Y%m%d_%H%M%S')
            output_excel_path = self.data_dir / f'token_trading_data_{current_time}.xlsx'
            
            # è¯»å–Excelæ–‡ä»¶
            logger.info(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {self.meme_path}")
            df = pd.read_excel(self.meme_path)
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—
            if 'å†…å®¹' not in df.columns:
                logger.error("meme.xlsxæ–‡ä»¶ç¼ºå°‘'å†…å®¹'åˆ—")
                return
            
            logger.info(f"æ€»è¡Œæ•°: {len(df)}")
            logger.info(f"å°†ä»ç¬¬ {start_index} ä¸ªä»£å¸å¼€å§‹å¤„ç†...")
            
            # æ·»åŠ éœ€è¦çš„åˆ—
            columns = [
                'token_id', 'address', 'symbol', 'name', 'network',
                'fdv_usd', 'fdv_usd_formatted', 'volume_usd_24h', 'volume_usd_24h_formatted',
                'price_change_m5', 'price_change_m5_formatted', 'price_change_h1', 'price_change_h1_formatted',
                'm5_buys', 'm5_sells', 'm15_buys', 'm15_sells', 'pool_created_at'
            ]
            
            for col in columns:
                if col not in df.columns:
                    df[col] = ''
            
            # ç”¨äºè®°å½•ä¸Šæ¬¡ä¿å­˜çš„æ—¶é—´
            last_save_time = time.time()
            modified = False
            
            # ç»Ÿè®¡è®¡æ•°å™¨
            processed_count = 0
            success_count = 0
            error_count = 0
            
            # éå†æ¯ä¸ªä»£å¸
            for index, row in df.iloc[start_index:].iterrows():
                try:
                    token_address = row['å†…å®¹']
                    if pd.isna(token_address):
                        logger.warning(f"è·³è¿‡ç©ºåœ°å€ï¼Œç´¢å¼• {index}")
                        continue
                    
                    processed_count += 1
                    logger.info(f"æ­£åœ¨è·å–ç´¢å¼• {index} ({processed_count}/{len(df)}) çš„äº¤æ˜“æ•°æ®...")
                    
                    # æ³¨å†Œåˆ°æ•´åˆå™¨
                    self.integrator.register_token(token_address)
                    
                    # åˆ†æä»£å¸
                    token_data = await self.analyze_token(token_address)
                    
                    if token_data:
                        # å°†æ•°æ®æ›´æ–°åˆ°DataFrame
                        for key, value in token_data.items():
                            if key in df.columns:
                                df.at[index, key] = value
                        
                        # ä¸å†ç›´æ¥å‘é€åˆ°é£ä¹¦
                        # await self.send_token_analysis_to_feishu(token_data)
                        
                        modified = True
                        success_count += 1
                        
                    else:
                        error_count += 1
                        logger.error(f"æ— æ³•è·å– {token_address} çš„äº¤æ˜“æ•°æ®")
                    
                    # å®šæœŸä¿å­˜ç»“æœ
                    current_time = time.time()
                    if modified and (processed_count % batch_size == 0 or current_time - last_save_time >= save_interval):
                        try:
                            logger.info(f"å‡†å¤‡ä¿å­˜è¿›åº¦ï¼Œå·²å¤„ç† {processed_count} æ¡æ•°æ®...")
                            temp_file = str(output_excel_path).replace('.xlsx', '_temp.xlsx')
                            df.to_excel(temp_file, index=False)
                            if output_excel_path.exists():
                                output_excel_path.unlink()
                            os.rename(temp_file, output_excel_path)
                            logger.info(f"å·²ä¿å­˜å½“å‰è¿›åº¦åˆ°: {output_excel_path}")
                            last_save_time = current_time
                            modified = False
                        except Exception as save_error:
                            logger.error(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(save_error)}")
                    
                    time.sleep(1)  # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è§¦å‘APIé™åˆ¶
                    
                except Exception as e:
                    error_count += 1
                    logger.error(f"å¤„ç†ä»£å¸æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            # æœ€åä¿å­˜ä¸€æ¬¡
            if modified:
                try:
                    temp_file = str(output_excel_path).replace('.xlsx', '_temp.xlsx')
                    df.to_excel(temp_file, index=False)
                    if output_excel_path.exists():
                        output_excel_path.unlink()
                    os.rename(temp_file, output_excel_path)
                    logger.info("æœ€ç»ˆäº¤æ˜“æ•°æ®å·²ä¿å­˜")
                except Exception as final_save_error:
                    logger.error(f"æœ€ç»ˆä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(final_save_error)}")
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            logger.info("\nå¤„ç†ç»Ÿè®¡ä¿¡æ¯:")
            logger.info(f"æ€»è®°å½•æ•°: {len(df)}")
            logger.info(f"å¤„ç†è®°å½•æ•°: {processed_count}")
            logger.info(f"æˆåŠŸå¤„ç†æ•°: {success_count}")
            logger.info(f"å¤±è´¥è®°å½•æ•°: {error_count}")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            logger.error(traceback.format_exc())

class MemeAnalysisMonitor:
    def __init__(self):
        # ä»é…ç½®æ–‡ä»¶åŠ è½½é£ä¹¦é…ç½®
        with open('config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
            
        # è·å–é£ä¹¦é…ç½®
        self.app_id = config.get('feishu_app_id', 'cli_a736cea2ff78100d')
        self.app_secret = config.get('feishu_app_secret', 'C9FsC6CnJz3CLf0PEz0NQewkuH6uvCdS')
        
        # åˆå§‹åŒ–é£ä¹¦æœºå™¨äººï¼Œä¼ å…¥å¿…è¦çš„å‚æ•°
        self.feishu_bot = FeishuBot(app_id=self.app_id, app_secret=self.app_secret)
        self.feishu_chat_id = config.get('feishu_chat_id', 'oc_a2d2c5616c900bda2ab8e13a77361287')
        self.data_dir = Path('data')
        self.meme_file = self.data_dir / 'meme.xlsx'  # ä¿®æ”¹ä¸ºç›‘æ§ meme.xlsx
        self.last_modified_time = None
        self.last_processed_index = -1  # è®°å½•æœ€åå¤„ç†çš„è¡Œç´¢å¼•
        
        # åˆå§‹åŒ–æ—¶è¯»å–å½“å‰æ–‡ä»¶çš„è¡Œæ•°
        self._init_last_processed_index()
        
    def _init_last_processed_index(self):
        """åˆå§‹åŒ–æ—¶è¯»å–å½“å‰æ–‡ä»¶çš„è¡Œæ•°"""
        try:
            if self.meme_file.exists():
                df = pd.read_excel(self.meme_file)
                self.last_processed_index = len(df) - 1  # è®¾ç½®ä¸ºæœ€åä¸€è¡Œçš„ç´¢å¼•
                logger.info(f"åˆå§‹åŒ–å®Œæˆï¼Œå½“å‰ meme.xlsx æ–‡ä»¶å…±æœ‰ {self.last_processed_index + 1} è¡Œ")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–æœ€åå¤„ç†ç´¢å¼•æ—¶å‡ºé”™: {str(e)}")
            self.last_processed_index = -1

    def monitor_analysis_file(self, interval: int = 5):
        """
        ç›‘æ§ meme.xlsx æ–‡ä»¶çš„æ›´æ–°
        :param interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        logging.info(f"å¼€å§‹ç›‘æ§æ–‡ä»¶: {self.meme_file}")
        
        while True:
            try:
                if not self.meme_file.exists():
                    logging.warning("meme.xlsx æ–‡ä»¶ä¸å­˜åœ¨")
                    time.sleep(interval)
                    continue

                current_mtime = os.path.getmtime(self.meme_file)
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ›´æ–°
                if self.last_modified_time is None or current_mtime > self.last_modified_time:
                    logging.info("æ£€æµ‹åˆ° meme.xlsx æ–‡ä»¶æ›´æ–°ï¼Œå¤„ç†æ–°æ•°æ®...")
                    self._process_new_data()
                    self.last_modified_time = current_mtime
                
                time.sleep(interval)
                
            except Exception as e:
                logging.error(f"ç›‘æ§æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                time.sleep(interval)

    def _process_new_data(self):
        """å¤„ç†æ–°çš„ meme æ•°æ®"""
        try:
            df = pd.read_excel(self.meme_file)
            current_rows = len(df)
            
            # å¦‚æœæœ‰æ–°è¡Œ
            if current_rows > self.last_processed_index + 1:
                # åªå¤„ç†æ–°å¢çš„è¡Œ
                new_rows = df.iloc[self.last_processed_index + 1:]
                logging.info(f"å‘ç° {len(new_rows)} æ¡æ–°æ•°æ®")
                
                # å¤„ç†æ¯ä¸€è¡Œæ–°æ•°æ®
                for _, row in new_rows.iterrows():
                    token_address = row['å†…å®¹']
                    if pd.isna(token_address):
                        continue
                        
                    # æ³¨å†Œåˆ°æ•´åˆå™¨è¿›è¡Œå¤„ç†
                    integrator.register_token(token_address)
                
                # æ›´æ–°æœ€åå¤„ç†çš„ç´¢å¼•
                self.last_processed_index = current_rows - 1
                logging.info(f"æ›´æ–°æœ€åå¤„ç†ç´¢å¼•ä¸º: {self.last_processed_index}")
                
        except Exception as e:
            logging.error(f"å¤„ç†æ–°æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

# æ–°å¢æ–‡ä»¶ç›‘æ§ç±»
class MemeFileWatcher(FileSystemEventHandler):
    """ç›‘æ§ meme.xlsx æ–‡ä»¶çš„å˜åŒ–å¹¶å¤„ç†æ–°å¢æ•°æ®"""
    
    def __init__(self, meme_file_path, analyzer=None, coingecko_analyzer=None):
        super().__init__()
        self.meme_file_path = meme_file_path
        self.analyzer = analyzer
        self.coingecko_analyzer = coingecko_analyzer
        self.last_processed_row = 0
        self.last_modified_time = self._get_file_mtime()
        
        # åŠ è½½é…ç½®
        with open('config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # åˆå§‹åŒ–é£ä¹¦æœºå™¨äºº
        self.app_id = config.get('feishu_app_id', 'cli_a736cea2ff78100d')
        self.app_secret = config.get('feishu_app_secret', 'C9FsC6CnJz3CLf0PEz0NQewkuH6uvCdS')
        self.feishu_bot = FeishuBot(app_id=self.app_id, app_secret=self.app_secret)
        self.feishu_chat_id = config.get('feishu_chat_id', 'oc_a2d2c5616c900bda2ab8e13a77361287')
        
        # åˆå§‹åŒ–Telegramæœºå™¨äºº
        telegram_token = config.get('telegram_token', '')
        if telegram_token:
            self.telegram_bot = TelegramBot(token=telegram_token)
        else:
            self.telegram_bot = None
        self.telegram_chat_id = config.get('telegram_chat_id', '')
        
        # æ·»åŠ æ—¶é—´çª—å£å’Œè®¡æ•°é€»è¾‘
        self.token_occurrences = {}  # è®°å½•ä»£å¸å‡ºç°æ¬¡æ•°å’Œæ—¶é—´
        self.time_window = 600  # 10åˆ†é’Ÿ = 600ç§’
        self.occurrence_threshold = 3  # å‡ºç°3æ¬¡æ‰æŠ¥è­¦
        
        # æ·»åŠ æŠ¥è­¦å†å²è®°å½•
        self.alert_history = {}  # è®°å½•ä»£å¸çš„æŠ¥è­¦æ—¶é—´
        self.alert_cooldown = 3600  # 1å°æ—¶ = 3600ç§’
        
        # æ·»åŠ æ¶ˆæ¯æ¨é€çŠ¶æ€ç®¡ç†
        self.message_status = {}  # è®°å½•æ¶ˆæ¯æ¨é€çŠ¶æ€
        self.message_lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”
        
        # åˆå§‹åŒ–æ—¶æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œè®°å½•å½“å‰è¡Œæ•°
        self._check_initial_state()
        
        logger.info(f"æ–‡ä»¶ç›‘æ§å™¨å·²åˆå§‹åŒ–ï¼Œç›‘æ§æ–‡ä»¶: {meme_file_path}")
        logger.info(f"å½“å‰è®°å½•çš„è¡Œæ•°: {self.last_processed_row}")

    def _check_initial_state(self):
        """æ£€æŸ¥æ–‡ä»¶åˆå§‹çŠ¶æ€ï¼Œè®°å½•å½“å‰è¡Œæ•°"""
        if os.path.exists(self.meme_file_path):
            try:
                df = pd.read_excel(self.meme_file_path)
                self.last_processed_row = len(df)
                logger.info(f"åˆå§‹æ–‡ä»¶åŒ…å« {self.last_processed_row} è¡Œæ•°æ®")
            except Exception as e:
                logger.error(f"è¯»å–åˆå§‹æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                self.last_processed_row = 0
        else:
            logger.warning(f"ç›‘æ§çš„æ–‡ä»¶ {self.meme_file_path} ä¸å­˜åœ¨")
            self.last_processed_row = 0

    def _get_file_mtime(self):
        """è·å–æ–‡ä»¶æœ€åä¿®æ”¹æ—¶é—´"""
        if os.path.exists(self.meme_file_path):
            return os.path.getmtime(self.meme_file_path)
        return 0

    def _check_token_occurrence(self, token_address):
        """æ£€æŸ¥ä»£å¸åœ¨æ—¶é—´çª—å£å†…çš„å‡ºç°æ¬¡æ•°"""
        current_time = time.time()
        
        # å¦‚æœä»£å¸ä¸åœ¨è®°å½•ä¸­ï¼Œåˆå§‹åŒ–è®°å½•
        if token_address not in self.token_occurrences:
            self.token_occurrences[token_address] = {
                'count': 1,
                'first_seen': current_time,
                'last_seen': current_time
            }
            return False
        
        # è·å–ä»£å¸è®°å½•
        record = self.token_occurrences[token_address]
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…
        if current_time - record['first_seen'] <= self.time_window:
            # åœ¨æ—¶é—´çª—å£å†…ï¼Œå¢åŠ è®¡æ•°
            record['count'] += 1
            record['last_seen'] = current_time
            
            # å¦‚æœè¾¾åˆ°é˜ˆå€¼ï¼Œè¿”å›True
            if record['count'] >= self.occurrence_threshold:
                logger.info(f"ä»£å¸ {token_address} åœ¨10åˆ†é’Ÿå†…å‡ºç° {record['count']} æ¬¡ï¼Œè§¦å‘æŠ¥è­¦")
                return True
        else:
            # è¶…å‡ºæ—¶é—´çª—å£ï¼Œé‡ç½®è®¡æ•°
            record['count'] = 1
            record['first_seen'] = current_time
            record['last_seen'] = current_time
        
        return False

    def _cleanup_old_records(self):
        """æ¸…ç†è¿‡æœŸçš„è®°å½•"""
        current_time = time.time()
        expired_tokens = [
            token for token, record in self.token_occurrences.items()
            if current_time - record['last_seen'] > self.time_window
        ]
        for token in expired_tokens:
            del self.token_occurrences[token]

    def _check_alert_history(self, token_address):
        """æ£€æŸ¥ä»£å¸æ˜¯å¦åœ¨å†·å´æœŸå†…"""
        current_time = time.time()
        
        if token_address in self.alert_history:
            last_alert_time = self.alert_history[token_address]
            if current_time - last_alert_time <= self.alert_cooldown:
                logger.info(f"ä»£å¸ {token_address} åœ¨1å°æ—¶å†…å·²ç»æŠ¥è­¦è¿‡ï¼Œè·³è¿‡")
                return False
            else:
                # è¶…è¿‡å†·å´æœŸï¼Œæ›´æ–°æŠ¥è­¦æ—¶é—´
                self.alert_history[token_address] = current_time
                return True
        else:
            # é¦–æ¬¡æŠ¥è­¦ï¼Œè®°å½•æ—¶é—´
            self.alert_history[token_address] = current_time
            return True

    def _cleanup_alert_history(self):
        """æ¸…ç†è¿‡æœŸçš„æŠ¥è­¦è®°å½•"""
        current_time = time.time()
        expired_tokens = [
            token for token, alert_time in self.alert_history.items()
            if current_time - alert_time > self.alert_cooldown
        ]
        for token in expired_tokens:
            del self.alert_history[token]

    def _check_message_status(self, token_address):
        """æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²ç»æ¨é€è¿‡"""
        with self.message_lock:
            if token_address in self.message_status:
                status = self.message_status[token_address]
                # å¦‚æœæ¶ˆæ¯å·²ç»æ¨é€æˆåŠŸï¼Œè¿”å› False
                if status.get('sent', False):
                    return False
                # å¦‚æœæ¶ˆæ¯æ­£åœ¨å¤„ç†ä¸­ï¼Œè¿”å› False
                if status.get('processing', False):
                    return False
            return True

    def _update_message_status(self, token_address, status):
        """æ›´æ–°æ¶ˆæ¯çŠ¶æ€"""
        with self.message_lock:
            self.message_status[token_address] = status

    def _cleanup_message_status(self):
        """æ¸…ç†è¿‡æœŸçš„æ¶ˆæ¯çŠ¶æ€"""
        current_time = time.time()
        with self.message_lock:
            expired_tokens = [
                token for token, status in self.message_status.items()
                if current_time - status.get('timestamp', 0) > self.alert_cooldown
            ]
            for token in expired_tokens:
                del self.message_status[token]

    def on_modified(self, event):
        """å½“æ–‡ä»¶è¢«ä¿®æ”¹æ—¶å¤„ç†æ–°æ•°æ®"""
        if not isinstance(event, FileModifiedEvent):
            return
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ–‡ä»¶
        if event.src_path != str(self.meme_file_path):
            return
            
        # æ£€æŸ¥ä¿®æ”¹æ—¶é—´ï¼Œé¿å…é‡å¤å¤„ç†
        current_mtime = self._get_file_mtime()
        if current_mtime == self.last_modified_time:
            return
            
        self.last_modified_time = current_mtime
        
        # ç­‰å¾…æ–‡ä»¶å®Œå…¨å†™å…¥
        time.sleep(1)
        
        # å¤„ç†æ–°æ•°æ®
        self._process_new_data()

    def _process_new_data(self):
        """å¤„ç†æ–°å¢æ•°æ®è¡Œ"""
        try:
            # æ¸…ç†è¿‡æœŸè®°å½•
            self._cleanup_old_records()
            self._cleanup_alert_history()
            self._cleanup_message_status()  # æ¸…ç†è¿‡æœŸçš„æ¶ˆæ¯çŠ¶æ€
            
            # è¯»å–å½“å‰æ–‡ä»¶
            df = pd.read_excel(self.meme_file_path)
            current_rows = len(df)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°è¡Œ
            if current_rows <= self.last_processed_row:
                logger.info("æ²¡æœ‰æ£€æµ‹åˆ°æ–°çš„æ•°æ®è¡Œ")
                return
                
            # å¤„ç†æ–°å¢çš„è¡Œ
            new_rows = df.iloc[self.last_processed_row:current_rows]
            logger.info(f"æ£€æµ‹åˆ° {len(new_rows)} è¡Œæ–°æ•°æ®")
            
            # ä¸ºæ¯ä¸€ä¸ªæ–°è¡Œå¼‚æ­¥å¤„ç†æ•°æ®
            for idx, row in new_rows.iterrows():
                token_address = row['å†…å®¹']
                if pd.isna(token_address) or not token_address:
                    logger.warning(f"è·³è¿‡ç©ºåœ°å€ï¼Œç´¢å¼• {idx}")
                    continue
                    
                logger.info(f"å¤„ç†æ–°å¢ä»£å¸: {token_address}")
                
                # æ£€æŸ¥ä»£å¸å‡ºç°æ¬¡æ•°å’ŒæŠ¥è­¦å†å²
                if self._check_token_occurrence(token_address) and self._check_alert_history(token_address):
                    # å¼‚æ­¥å¤„ç†æ–°çš„ä»£å¸æ•°æ®
                    asyncio.run(self._analyze_token(token_address))
                else:
                    logger.info(f"ä»£å¸ {token_address} æœªè¾¾åˆ°æŠ¥è­¦é˜ˆå€¼æˆ–åœ¨å†·å´æœŸå†…ï¼Œè·³è¿‡å¤„ç†")
            
            # æ›´æ–°å¤„ç†è¿‡çš„è¡Œæ•°
            self.last_processed_row = current_rows
            logger.info(f"å·²å¤„ç†åˆ°ç¬¬ {self.last_processed_row} è¡Œ")
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶æ›´æ–°æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())

    async def _analyze_token(self, token_address):
        """åˆ†æå•ä¸ªä»£å¸æ•°æ®å¹¶å‘é€åˆ°é£ä¹¦"""
        try:
            # æ£€æŸ¥æ¶ˆæ¯çŠ¶æ€
            if not self._check_message_status(token_address):
                logger.info(f"ä»£å¸ {token_address} çš„æ¶ˆæ¯å·²ç»å¤„ç†è¿‡æˆ–æ­£åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡")
                return

            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            self._update_message_status(token_address, {
                'processing': True,
                'timestamp': time.time()
            })

            twitter_analysis = None
            coingecko_data = None
            
            # 1. Twitter åˆ†æ
            if self.analyzer:
                logger.info(f"å¼€å§‹å¯¹ {token_address} è¿›è¡Œ Twitter åˆ†æ")
                max_retries = 5
                retry_delay = 30
                
                for attempt in range(max_retries):
                    try:
                        tweets = await twitter_api.search_tweets(token_address)
                        if tweets:
                            logger.info(f"æ‰¾åˆ° {len(tweets)} æ¡ç›¸å…³æ¨æ–‡")
                            twitter_analysis = await self.analyzer.analyze_tweets(token_address, tweets)
                            logger.info(f"å·²å®Œæˆ {token_address} çš„ Twitter åˆ†æ")
                            break
                        else:
                            logger.warning(f"æœªæ‰¾åˆ°å…³äº {token_address} çš„æ¨æ–‡")
                            break
                    except Exception as e:
                        if "Rate limit exceeded" in str(e):
                            if attempt < max_retries - 1:
                                current_delay = retry_delay * (2 ** attempt)
                                logger.warning(f"Twitter API é€Ÿç‡é™åˆ¶ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {current_delay} ç§’...")
                                await asyncio.sleep(current_delay)
                                continue
                        logger.error(f"Twitter åˆ†æå‡ºé”™: {str(e)}")
                        break
            
            # 2. CoinGecko åˆ†æ
            if self.coingecko_analyzer:
                logger.info(f"å¼€å§‹å¯¹ {token_address} è¿›è¡Œ CoinGecko åˆ†æ")
                coingecko_data = await self.coingecko_analyzer.analyze_token(token_address)
                if coingecko_data:
                    logger.info(f"å·²å®Œæˆ {token_address} çš„ CoinGecko åˆ†æ")
                else:
                    logger.warning(f"CoinGecko æ— æ³•åˆ†æä»£å¸ {token_address}")
            
            # 3. å‘é€åˆ†æç»“æœåˆ°é£ä¹¦ - è¿™éƒ¨åˆ†éœ€è¦ä¿®æ”¹
            if twitter_analysis or coingecko_data:
                # ä¸å†ç«‹å³å‘é€ï¼Œè€Œæ˜¯åªä¿å­˜åˆ†æç»“æœ
                message = self._build_analysis_message(token_address, twitter_analysis, coingecko_data)
                if message:
                    # æ›´æ–°æ¶ˆæ¯çŠ¶æ€ï¼Œä½†ä¸å‘é€
                    self._update_message_status(token_address, {
                        'processed': True,  # æ ‡è®°ä¸ºå·²å¤„ç†
                        'message': message,  # ä¿å­˜æ¶ˆæ¯å†…å®¹
                        'timestamp': time.time()
                    })
                    
                    logger.info(f"å·²å®Œæˆ {token_address} çš„åˆ†æï¼Œç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆåç»Ÿä¸€å‘é€")
                    
        except Exception as e:
            logger.error(f"åˆ†æä»£å¸ {token_address} æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())

    def _build_analysis_message(self, token_address, twitter_analysis, coingecko_data):
        """æ„å»ºåˆ†æç»“æœæ¶ˆæ¯"""
        try:
            message = f"""ğŸ”é‡‘ç‹—é¢„è­¦

ğŸ“Œ ä»£å¸åœ°å€: {token_address}"""

            if coingecko_data:
                message += f"""
ğŸª™ åç§°: {coingecko_data.get('name', 'N/A')} ({coingecko_data.get('symbol', 'N/A')})
ğŸŒ ç½‘ç»œ: {coingecko_data.get('network', 'N/A')}

ğŸ’° å¸‚åœºæ•°æ®:
â€¢ å¸‚å€¼: {coingecko_api.format_currency(coingecko_data.get('fdv_usd', 'N/A'))}
â€¢ 24å°æ—¶äº¤æ˜“é‡: {coingecko_api.format_currency(coingecko_data.get('volume_usd_24h', 'N/A'))}
â€¢ åˆ›å»ºæ—¶é—´: {coingecko_data.get('pool_created_at', 'N/A')}

ğŸ“ˆ ä»·æ ¼å˜åŠ¨:
â€¢ 5åˆ†é’Ÿ: {coingecko_api.format_percentage(coingecko_data.get('price_change_m5', 'N/A'))}
â€¢ 1å°æ—¶: {coingecko_api.format_percentage(coingecko_data.get('price_change_h1', 'N/A'))}

ğŸ”„ æœ€è¿‘äº¤æ˜“æ¬¡æ•°:
â€¢ 5åˆ†é’Ÿå†…: ä¹°å…¥ {coingecko_data.get('m5_buys', 0)} æ¬¡, å–å‡º {coingecko_data.get('m5_sells', 0)} æ¬¡
â€¢ 15åˆ†é’Ÿå†…: ä¹°å…¥ {coingecko_data.get('m15_buys', 0)} æ¬¡, å–å‡º {coingecko_data.get('m15_sells', 0)} æ¬¡"""

            if twitter_analysis:
                message += f"""

ğŸ“ å™äº‹ä¿¡æ¯:
{twitter_analysis.get('å™äº‹ä¿¡æ¯', 'N/A')}

ğŸŒ¡ï¸ å¯æŒç»­æ€§åˆ†æ:
â€¢ ç¤¾åŒºçƒ­åº¦: {twitter_analysis.get('å¯æŒç»­æ€§_ç¤¾åŒºçƒ­åº¦', 'N/A')}
â€¢ ä¼ æ’­æ½œåŠ›: {twitter_analysis.get('å¯æŒç»­æ€§_ä¼ æ’­æ½œåŠ›', 'N/A')}
â€¢ çŸ­æœŸæŠ•æœºä»·å€¼: {twitter_analysis.get('å¯æŒç»­æ€§_çŸ­æœŸæŠ•æœºä»·å€¼', 'N/A')}"""

            return message
            
        except Exception as e:
            logger.error(f"æ„å»ºåˆ†ææ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
            return None

    def start_watching(self):
        """å¼€å§‹ç›‘æ§æ–‡ä»¶"""
        observer = Observer()
        # ç›‘æ§æ–‡ä»¶æ‰€åœ¨ç›®å½•
        directory = os.path.dirname(self.meme_file_path)
        observer.schedule(self, directory, recursive=False)
        observer.start()
        logger.info(f"å¼€å§‹ç›‘æ§ç›®å½•: {directory}")
        return observer

# ä¿®æ”¹ä¸»å‡½æ•°
async def main():
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        parser = argparse.ArgumentParser(description='Meme å¸åˆ†æå·¥å…·')
        parser.add_argument('--coingecko', action='store_true', help='åªè¿è¡Œ CoinGecko åˆ†æ')
        parser.add_argument('--twitter', action='store_true', help='åªè¿è¡Œ Twitter åˆ†æ')
        parser.add_argument('--start', type=int, default=0, help='CoinGecko åˆ†æçš„èµ·å§‹ç´¢å¼•')
        parser.add_argument('--batch', type=int, default=10, help='CoinGecko æ‰¹å¤„ç†å¤§å°')
        parser.add_argument('--watch', action='store_true', help='å¯ç”¨æ–‡ä»¶ç›‘æ§æ¨¡å¼')
        args = parser.parse_args()
        
        # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å†³å®šè¿è¡Œå“ªäº›åˆ†æ
        run_twitter = not args.coingecko or args.twitter
        run_coingecko = not args.twitter or args.coingecko
        watch_mode = args.watch
        
        # é…ç½®æ•´åˆå™¨æ˜¯å¦éœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆ
        if run_twitter and run_coingecko:
            # éœ€è¦ä¸¤ç§åˆ†æéƒ½å®Œæˆ
            pass
        else:
            # ä¿®æ”¹æ•´åˆå™¨æ–¹æ³•ï¼Œå…è®¸åªæœ‰ä¸€ç§åˆ†æå®Œæˆå°±å‘é€
            integrator._need_both_analyses = lambda: False
        
        # åˆå§‹åŒ–åˆ†æå™¨å®ä¾‹
        twitter_analyzer = None
        coingecko_analyzer = None
        
        if run_twitter:
            logger.info("åˆå§‹åŒ– Twitter Meme åˆ†æå™¨...")
            twitter_analyzer = MemeAnalyzer()
            
        if run_coingecko:
            logger.info("åˆå§‹åŒ– CoinGecko ä»£å¸åˆ†æå™¨...")
            coingecko_analyzer = CoinGeckoAnalyzer()
            
        # å¯ç”¨æ–‡ä»¶ç›‘æ§æ¨¡å¼
        if watch_mode:
            logger.info("å¯åŠ¨æ–‡ä»¶ç›‘æ§æ¨¡å¼...")
            meme_file_path = Path('data') / 'meme.xlsx'
            
            # åˆ›å»ºæ–‡ä»¶ç›‘æ§å™¨
            watcher = MemeFileWatcher(
                meme_file_path=meme_file_path,
                analyzer=twitter_analyzer,
                coingecko_analyzer=coingecko_analyzer
            )
            
            # å¼€å§‹ç›‘æ§
            observer = watcher.start_watching()
            
            # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œ
            try:
                logger.info("æ–‡ä»¶ç›‘æ§å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢...")
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢ç›‘æ§...")
                observer.stop()
            observer.join()
        else:
            # ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼ï¼Œå¤„ç†æ•´ä¸ªæ–‡ä»¶
            if run_twitter and twitter_analyzer:
                logger.info("å¯åŠ¨ Twitter Meme åˆ†æ...")
                await twitter_analyzer.process_history_file()
            
            if run_coingecko and coingecko_analyzer:
                logger.info("å¯åŠ¨ CoinGecko ä»£å¸åˆ†æ...")
                await coingecko_analyzer.process_meme_file(
                    start_index=args.start, 
                    batch_size=args.batch
                )
            
            # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œï¼Œç­‰å¾…ç”¨æˆ·ä¸­æ–­
            logger.info("ç¨‹åºæ­£åœ¨è¿è¡Œä¸­ï¼ŒæŒ‰ Ctrl+C é€€å‡º...")
            try:
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œç¨‹åºé€€å‡º...")
        
    except Exception as e:
        logger.error(f"è¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.exception(e)

# ä¿®æ”¹ __main__ éƒ¨åˆ†
if __name__ == '__main__':
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        # æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨å¼‚æ­¥è¿è¡Œ
        asyncio.run(main())
    else:
        # æ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œé»˜è®¤å¯åŠ¨æ–‡ä»¶ç›‘æ§æ¨¡å¼
        try:
            logger.info("ä½¿ç”¨é»˜è®¤æ¨¡å¼å¯åŠ¨ç¨‹åº - å¯åŠ¨æ–‡ä»¶ç›‘æ§æ¨¡å¼")
            # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°ä¸ºå¯åŠ¨ç›‘æ§æ¨¡å¼
            sys.argv.append('--watch')
            asyncio.run(main())
        except Exception as e:
            logger.error(f"è¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            logger.exception(e)