import json
import time
import lark_oapi as lark
from lark_oapi.api.im.v1 import *
from datetime import datetime, timedelta
import logging
from typing import Optional, Set
import pandas as pd
import os
import sys
from Twitter_historyaianalysis import SiliconFlowClient
from queue import Queue
import threading
from tweet_metrics import TweetMetrics  # å¯¼å…¥TweetMetricsç±»
from feishu_bot import FeishuBot  # æ·»åŠ è¿™è¡Œå¯¼å…¥

# åˆ›å»ºæ•°æ®ä¿å­˜ç›®å½•
DATA_DIR = "data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# é…ç½®æ—¥å¿—åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# ç§»é™¤æ‰€æœ‰å·²å­˜åœ¨çš„å¤„ç†å™¨
for handler in logger.handlers[:]:
    logger.removeHandler(handler)

# æ–‡ä»¶å¤„ç†å™¨ - ä½¿ç”¨utf-8ç¼–ç 
file_handler = logging.FileHandler(os.path.join(DATA_DIR, 'feishu_messages.log'), encoding='utf-8')
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
file_handler.setLevel(logging.INFO)  # åªè®°å½•INFOçº§åˆ«ä»¥ä¸Šçš„æ—¥å¿—
logger.addHandler(file_handler)

# æ§åˆ¶å°å¤„ç†å™¨ - ä½¿ç”¨utf-8ç¼–ç 
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
console_handler.setLevel(logging.INFO)  # åªè®°å½•INFOçº§åˆ«ä»¥ä¸Šçš„æ—¥å¿—
logger.addHandler(console_handler)

# è®¾ç½®larkçš„æ—¥å¿—çº§åˆ«ä¸ºWARNINGï¼Œå‡å°‘APIè°ƒç”¨çš„debugä¿¡æ¯
lark.logger.setLevel(logging.WARNING)

class FeishuMessageMonitor:
    def __init__(self, app_id: str, app_secret: str, deepseek_api_key: str):
        self.client = lark.Client.builder() \
            .app_id(app_id) \
            .app_secret(app_secret) \
            .log_level(lark.LogLevel.INFO) \
            .build()
        self.processed_msgs: Set[str] = set()  # ç”¨äºå­˜å‚¨å·²å¤„ç†çš„æ¶ˆæ¯ID
        self.excel_file = os.path.join(DATA_DIR, 'message_history.xlsx')
        self.analyzed_file = os.path.join(DATA_DIR, 'analyzed_messages.xlsx')
        self.last_timestamp = int(time.time() * 1000)  # æ·»åŠ æ—¶é—´æˆ³è®°å½•
        self.deepseek_client = SiliconFlowClient(deepseek_api_key)
        self.metrics_queue = Queue()  # åˆ›å»ºé˜Ÿåˆ—å­˜å‚¨éœ€è¦è·å–äº’åŠ¨æ•°æ®çš„æ¶ˆæ¯
        self.tweet_metrics = TweetMetrics("your_api_key")  # åˆå§‹åŒ–TweetMetrics
        # å¯åŠ¨äº’åŠ¨æ•°æ®é‡‡é›†çº¿ç¨‹
        self.metrics_thread = threading.Thread(target=self._process_metrics_queue, daemon=True)
        self.metrics_thread.start()
        self.feishu_bot = FeishuBot(app_id, app_secret)  # æ·»åŠ é£ä¹¦æœºå™¨äººå®ä¾‹
        self.alert_chat_id = "oc_24a1bdb222fc850d0049b41022acec47"  # æ·»åŠ ç›®æ ‡ç¾¤èŠID
        
    def monitor_messages(self, chat_id: str, interval: int = 5):
        """
        å®æ—¶ç›‘æ§æ¶ˆæ¯
        :param chat_id: ç¾¤èŠID
        :param interval: è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
        """
        logging.info(f"å¼€å§‹ç›‘æ§ç¾¤èŠ {chat_id}")
        logging.info(f"ç›‘æ§é—´éš”è®¾ç½®ä¸º {interval} ç§’")
        
        while True:
            try:
                # è·å–æœ€æ–°æ¶ˆæ¯
                messages = self._get_latest_messages(chat_id)
                if messages:
                    self._handle_new_messages(messages)
                
                time.sleep(interval)
                
            except Exception as e:
                logging.error(f"ç›‘æ§è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
                time.sleep(interval)
                
    def _get_latest_messages(self, chat_id: str) -> Optional[list]:
        """è·å–æœ€æ–°æ¶ˆæ¯"""
        try:
            # å¢åŠ é¡µé¢å¤§å°ä»¥ç¡®ä¿ä¸ä¼šé—æ¼æ¶ˆæ¯
            request = ListMessageRequest.builder() \
                .container_id_type("chat") \
                .container_id(chat_id) \
                .sort_type("ByCreateTimeDesc") \
                .page_size(30) \
                .build()

            response = self.client.im.v1.message.list(request)

            if not response.success():
                logging.error(
                    f"è·å–æ¶ˆæ¯å¤±è´¥ - é”™è¯¯ç : {response.code}, "
                    f"æ¶ˆæ¯: {response.msg}, "
                    f"æ—¥å¿—ID: {response.get_log_id()}"
                )
                return None

            # åªè¿”å›ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´ä¹‹åçš„æ–°æ¶ˆæ¯
            new_messages = []
            for msg in response.data.items:
                msg_timestamp = int(str(msg.create_time))
                if msg_timestamp > self.last_timestamp:
                    new_messages.append(msg)
            
            # æ›´æ–°æœ€åå¤„ç†çš„æ—¶é—´æˆ³
            if new_messages:
                self.last_timestamp = int(str(new_messages[0].create_time))
                
            return new_messages

        except Exception as e:
            logging.error(f"è·å–æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def _save_to_excel(self, message_data: dict):
        """ä¿å­˜æ¶ˆæ¯åˆ°Excelæ–‡ä»¶"""
        try:
            # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–ç°æœ‰æ•°æ®
            if os.path.exists(self.excel_file):
                df_existing = pd.read_excel(self.excel_file)
                df_new = pd.DataFrame([message_data])
                df_combined = pd.concat([df_existing, df_new], ignore_index=True)
            else:
                df_combined = pd.DataFrame([message_data])
            
            # ä¿å­˜åˆ°Excelï¼ŒåŒ…å«è¡¨å¤´
            df_combined.to_excel(self.excel_file, index=False, engine='openpyxl')
            
        except Exception as e:
            logging.error(f"ä¿å­˜æ¶ˆæ¯åˆ°Excelæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            
    def _process_metrics_queue(self):
        """å¤„ç†äº’åŠ¨æ•°æ®é‡‡é›†é˜Ÿåˆ—çš„çº¿ç¨‹"""
        while True:
            try:
                # ä»é˜Ÿåˆ—ä¸­è·å–æ¶ˆæ¯æ•°æ®
                message_data, create_time = self.metrics_queue.get()
                current_time = datetime.now()
                msg_create_time = datetime.strptime(create_time, '%Y-%m-%d %H:%M:%S')
                
                # è®¡ç®—æ—¶é—´å·®ï¼ˆåˆ†é’Ÿï¼‰
                time_diff = (current_time - msg_create_time).total_seconds() / 60
                
                if time_diff < 3:
                    # å¦‚æœè¿˜ä¸åˆ°3åˆ†é’Ÿï¼Œé‡æ–°æ”¾å…¥é˜Ÿåˆ—å¹¶ç­‰å¾…
                    self.metrics_queue.put((message_data, create_time))
                    time.sleep(10)  # ç­‰å¾…10ç§’åå†æ£€æŸ¥
                    continue
                    
                elif time_diff >= 3 and time_diff < 10:
                    # åˆ°è¾¾3åˆ†é’Ÿï¼Œè·å–3åˆ†é’Ÿæ•°æ®
                    if not self._has_metrics(message_data['message_id'], '3min'):
                        logging.info(f"è·å–3åˆ†é’Ÿæ•°æ®: {message_data['tweet_link']}")
                        metrics_3min = self.tweet_metrics.get_tweet_metrics(message_data['tweet_link'])
                        if metrics_3min:
                            self._update_metrics(message_data['message_id'], metrics_3min, '3min')
                            logging.info(f"3åˆ†é’Ÿæ•°æ®æ›´æ–°æˆåŠŸ: {metrics_3min}")
                        else:
                            logging.error("è·å–3åˆ†é’Ÿæ•°æ®å¤±è´¥")
                    
                    # é‡æ–°æ”¾å…¥é˜Ÿåˆ—ç­‰å¾…10åˆ†é’Ÿ
                    self.metrics_queue.put((message_data, create_time))
                    time.sleep(10)
                    
                else:
                    # åˆ°è¾¾10åˆ†é’Ÿï¼Œè·å–10åˆ†é’Ÿæ•°æ®
                    if not self._has_metrics(message_data['message_id'], '10min'):
                        logging.info(f"è·å–10åˆ†é’Ÿæ•°æ®: {message_data['tweet_link']}")
                        metrics_10min = self.tweet_metrics.get_tweet_metrics(message_data['tweet_link'])
                        if metrics_10min:
                            self._update_metrics(message_data['message_id'], metrics_10min, '10min')
                            logging.info(f"10åˆ†é’Ÿæ•°æ®æ›´æ–°æˆåŠŸ: {metrics_10min}")
                        else:
                            logging.error("è·å–10åˆ†é’Ÿæ•°æ®å¤±è´¥")
                    # å¤„ç†å®Œæˆï¼Œä¸å†æ”¾å›é˜Ÿåˆ—
                
                time.sleep(1)  # é¿å…è¿‡äºé¢‘ç¹çš„æ£€æŸ¥
                
            except Exception as e:
                logging.error(f"å¤„ç†äº’åŠ¨æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                time.sleep(5)  # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´

    def _has_metrics(self, message_id: str, time_point: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å¯¹åº”æ—¶é—´ç‚¹çš„æ•°æ®"""
        try:
            if os.path.exists(self.excel_file):
                df = pd.read_excel(self.excel_file)
                mask = df['message_id'] == message_id
                if mask.any():
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä»»æ„ä¸€ä¸ªè¯¥æ—¶é—´ç‚¹çš„æŒ‡æ ‡
                    metrics_columns = [f"{metric}_{time_point}" 
                                    for metric in ['likes', 'retweets', 'replies', 'quotes']]
                    return any(df.loc[mask, col].notna().any() for col in metrics_columns)
            return False
        except Exception:
            return False

    def _update_metrics(self, message_id: str, metrics: dict, time_point: str):
        """æ›´æ–°æ¶ˆæ¯çš„äº’åŠ¨æ•°æ®"""
        try:
            if os.path.exists(self.excel_file):
                df = pd.read_excel(self.excel_file)
                
                mask = df['message_id'] == message_id
                if mask.any():
                    # æ›´æ–°äº’åŠ¨æ•°æ®
                    for metric_type, value in metrics.items():
                        column_name = f"{metric_type}_{time_point}"
                        df.loc[mask, column_name] = value
                    
                    # å¦‚æœæ˜¯3åˆ†é’Ÿæ•°æ®ï¼Œæ£€æŸ¥äº’åŠ¨æ€»é‡
                    if time_point == '3min':
                        total_interactions = sum(metrics.values())  # è®¡ç®—æ€»äº’åŠ¨é‡
                        if total_interactions > 100:  # ç¬¬ä¸€ä¸ªè§¦å‘æ¡ä»¶ï¼š3åˆ†é’Ÿå†…æ€»äº’åŠ¨é‡è¶…è¿‡100
                            self._send_alert(df.loc[mask].iloc[0], metrics, total_interactions, "3åˆ†é’Ÿ")
                    
                    # å¦‚æœæ˜¯10åˆ†é’Ÿæ•°æ®ï¼Œæ£€æŸ¥å¢é•¿ç‡
                    elif time_point == '10min':
                        total_10min = sum(metrics.values())
                        
                        # è·å–3åˆ†é’Ÿçš„æ•°æ®
                        metrics_3min = {}
                        for metric_type in ['likes', 'retweets', 'replies', 'quotes']:
                            col_3min = f"{metric_type}_3min"
                            if col_3min in df.columns:
                                metrics_3min[metric_type] = df.loc[mask, col_3min].iloc[0]
                        
                        # å¦‚æœæœ‰3åˆ†é’Ÿæ•°æ®ï¼Œè®¡ç®—å¢é•¿ç‡
                        if metrics_3min and all(isinstance(v, (int, float)) for v in metrics_3min.values()):
                            total_3min = sum(metrics_3min.values())
                            
                            if total_3min > 0:
                                growth_rate = total_10min / total_3min
                                
                                # ç¬¬äºŒä¸ªè§¦å‘æ¡ä»¶ï¼š10åˆ†é’Ÿæ€»é‡è¶…è¿‡300ä¸”æ˜¯3åˆ†é’Ÿæ•°æ®çš„3å€ä»¥ä¸Š
                                if total_10min > 300 and growth_rate >= 3:
                                    growth_metrics = {
                                        metric: metrics[metric] - metrics_3min.get(metric, 0)
                                        for metric in metrics
                                    }
                                    
                                    self._send_alert(
                                        df.loc[mask].iloc[0], 
                                        metrics,
                                        total_10min,
                                        "10åˆ†é’Ÿ",
                                        growth_rate=growth_rate,
                                        growth_metrics=growth_metrics
                                    )
                    
                    # ä¿å­˜æ›´æ–°åçš„æ•°æ®
                    df.to_excel(self.excel_file, index=False, engine='openpyxl')
                    logging.info(f"å·²æ›´æ–°æ¶ˆæ¯ {message_id} çš„{time_point}äº’åŠ¨æ•°æ®")
                else:
                    logging.warning(f"æœªæ‰¾åˆ°æ¶ˆæ¯ID {message_id}ï¼Œæ— æ³•æ›´æ–°äº’åŠ¨æ•°æ®")
            else:
                logging.warning(f"Excelæ–‡ä»¶ {self.excel_file} ä¸å­˜åœ¨ï¼Œæ— æ³•æ›´æ–°äº’åŠ¨æ•°æ®")
        
        except Exception as e:
            logging.error(f"æ›´æ–°äº’åŠ¨æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

    def _send_alert(self, row_data, metrics, total_interactions, time_point, growth_rate=None, growth_metrics=None):
        """å‘é€é«˜äº’åŠ¨é‡æé†’"""
        try:
            # æ„é€ åŸºç¡€æ¶ˆæ¯
            alert_msg = (
                f"ğŸ”¥ é«˜äº’åŠ¨æ¨æ–‡æé†’\n\n"
                f"{time_point}å†…æ€»äº’åŠ¨é‡: {total_interactions}\n"
                f"åŸæ–‡: {row_data['content']}\n"
                f"é“¾æ¥: {row_data['tweet_link']}\n\n"
                f"è¯¦ç»†æ•°æ®:\n"
                f"ğŸ‘ ç‚¹èµ: {metrics.get('likes', 0)}\n"
                f"ğŸ”„ è½¬å‘: {metrics.get('retweets', 0)}\n"
                f"ğŸ’¬ è¯„è®º: {metrics.get('replies', 0)}\n"
                f"ğŸ“ å¼•ç”¨: {metrics.get('quotes', 0)}"
            )
            
            # å¦‚æœæœ‰å¢é•¿ç‡æ•°æ®ï¼Œæ·»åŠ å¢é•¿ä¿¡æ¯
            if growth_rate and growth_metrics:
                growth_info = (
                    f"\n\nğŸ“ˆ å¢é•¿æ•°æ®:\n"
                    f"å¢é•¿å€æ•°: {growth_rate:.1f}x\n"
                    f"æ–°å¢ç‚¹èµ: +{growth_metrics.get('likes', 0)}\n"
                    f"æ–°å¢è½¬å‘: +{growth_metrics.get('retweets', 0)}\n"
                    f"æ–°å¢è¯„è®º: +{growth_metrics.get('replies', 0)}\n"
                    f"æ–°å¢å¼•ç”¨: +{growth_metrics.get('quotes', 0)}"
                )
                alert_msg += growth_info
            
            # å‘é€åˆ°é£ä¹¦ç¾¤
            self.feishu_bot.send_message(self.alert_chat_id, alert_msg)
            logging.info(f"å·²å‘é€é«˜äº’åŠ¨é‡æé†’ï¼Œæ¶ˆæ¯ID: {row_data['message_id']}")
            
        except Exception as e:
            logging.error(f"å‘é€æé†’æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

    def _handle_new_messages(self, messages: list):
        """å¤„ç†æ–°æ¶ˆæ¯"""
        analyzed_data = []
        
        for msg in messages:
            if msg.message_id in self.processed_msgs:
                continue
                
            try:
                content = self._get_message_content(msg)
                
                # æ·»åŠ æ–°çš„æ¶ˆæ¯è§£æé€»è¾‘
                kol_name, tweet_content, tweet_link, group_type = self.deepseek_client.process_post(content)
                
                # å¤„ç†æ¶ˆæ¯
                message_data = {
                    'message_id': msg.message_id,
                    'create_time': datetime.fromtimestamp(
                        int(str(msg.create_time))/1000
                    ).strftime('%Y-%m-%d %H:%M:%S'),
                    'sender': self._get_sender_info(msg.sender),
                    'msg_type': msg.msg_type,
                    'content': content,
                    'kol_name': kol_name,
                    'tweet_content': tweet_content,
                    'tweet_link': tweet_link,
                    'group_type': group_type
                }
                
                # å¦‚æœæœ‰æ¨æ–‡é“¾æ¥ï¼Œç«‹å³è·å–ä¸€æ¬¡æ•°æ®
                if tweet_link:
                    logging.info(f"ç«‹å³è·å–æ¨æ–‡æ•°æ®: {tweet_link}")
                    initial_metrics = self.tweet_metrics.get_tweet_metrics(tweet_link, 'initial')
                    if initial_metrics:
                        self._update_metrics(message_data['message_id'], initial_metrics, 'initial')
                        # æ£€æŸ¥åˆå§‹äº’åŠ¨é‡
                        total_interactions = sum(initial_metrics.values())
                        if total_interactions > 100:
                            self._send_alert(message_data, initial_metrics, total_interactions, "åˆå§‹æ•°æ®")
                
                # è®°å½•æ–°æ¶ˆæ¯
                logging.info("=== æ–°æ¶ˆæ¯ ===")
                for key, value in message_data.items():
                    logging.info(f"{key}: {value}")
                logging.info("=============")
                
                # ä¿å­˜æ¶ˆæ¯åˆ°Excel
                self._save_to_excel(message_data)
                
                # ä½¿ç”¨DeepSeek APIåˆ†ææ¶ˆæ¯
                if msg.msg_type == "text" and tweet_content:  # ä¿®æ”¹æ¡ä»¶ï¼Œåªåˆ†ææœ‰æ•ˆçš„æ¨æ–‡å†…å®¹
                    max_retries = 3
                    retry_count = 0
                    
                    while retry_count < max_retries:
                        try:
                            # ä½¿ç”¨è§£æåçš„tweet_contentè¿›è¡Œåˆ†æ
                            summary, category, asset, sentiment, tags = self.deepseek_client.analyze_tweet(tweet_content)
                            
                            # æ£€æŸ¥è¿”å›ç»“æœæ˜¯å¦æœ‰æ•ˆ
                            empty_fields = []
                            if not summary or summary.strip() == "":
                                empty_fields.append("summary")
                            if not category or category.strip() == "":
                                empty_fields.append("category")
                            if not sentiment or sentiment.strip() == "":
                                empty_fields.append("sentiment")
                            
                            if len(empty_fields) >= 2:
                                retry_count += 1
                                if retry_count < max_retries:
                                    logging.warning(f"APIè¿”å›ç»“æœä¸å®Œæ•´ ({', '.join(empty_fields)}ä¸ºç©º)ï¼Œç¬¬{retry_count}æ¬¡é‡è¯•...")
                                    time.sleep(2)
                                    continue
                                else:
                                    logging.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä½¿ç”¨æœ€åä¸€æ¬¡çš„è¿”å›ç»“æœ")
                            
                            analyzed_message = {
                                'Message_ID': message_data['message_id'],
                                'Content': message_data['content'],
                                'Create_Time': message_data['create_time'],
                                'Sender': message_data['sender'],
                                'KOL_Name': kol_name,  # æ·»åŠ KOLåç§°
                                'Tweet_Content': tweet_content,  # æ·»åŠ è§£æåçš„æ¨æ–‡å†…å®¹
                                'Tweet_Link': tweet_link,  # æ·»åŠ æ¨æ–‡é“¾æ¥
                                'Group_Type': group_type,  # æ·»åŠ åˆ†ç»„ç±»å‹
                                'Summary': summary,
                                'Category': category,
                                'Asset': asset,
                                'Sentiment': sentiment,
                                'Tags': tags
                            }
                            analyzed_data.append(analyzed_message)
                            
                            # ä¿å­˜åˆ†æç»“æœ
                            if os.path.exists(self.analyzed_file):
                                df_existing = pd.read_excel(self.analyzed_file)
                                df_new = pd.DataFrame([analyzed_message])
                                df_combined = pd.concat([df_existing, df_new], ignore_index=True)
                            else:
                                df_combined = pd.DataFrame([analyzed_message])
                            
                            df_combined.to_excel(self.analyzed_file, index=False, engine='openpyxl')
                            logging.info(f"æ¶ˆæ¯åˆ†æç»“æœå·²ä¿å­˜åˆ°: {self.analyzed_file}")
                            break
                            
                        except Exception as e:
                            retry_count += 1
                            if retry_count < max_retries:
                                logging.error(f"åˆ†ææ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}ï¼Œç¬¬{retry_count}æ¬¡é‡è¯•...")
                                time.sleep(2)
                            else:
                                logging.error(f"åˆ†ææ¶ˆæ¯å¤±è´¥ï¼Œè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {str(e)}")
                
                # ç¡®è®¤æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if os.path.exists(self.excel_file):
                    logging.info(f"æ¶ˆæ¯å·²ä¿å­˜åˆ°æ–‡ä»¶: {self.excel_file}")
                else:
                    logging.warning("è­¦å‘Šï¼šæ–‡ä»¶æœªèƒ½æˆåŠŸåˆ›å»ºï¼")
                
                # å°†æ¶ˆæ¯IDæ·»åŠ åˆ°å·²å¤„ç†é›†åˆ
                self.processed_msgs.add(msg.message_id)
                
                if len(self.processed_msgs) > 1000:
                    self.processed_msgs = set(list(self.processed_msgs)[-500:])
                
                # å¦‚æœæ¶ˆæ¯åŒ…å«æ¨æ–‡é“¾æ¥ï¼ŒåŠ å…¥äº’åŠ¨æ•°æ®é‡‡é›†é˜Ÿåˆ—ç»§ç»­è·Ÿè¸ª
                if tweet_link:
                    self.metrics_queue.put((message_data, message_data['create_time']))
                
            except Exception as e:
                logging.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

    def _get_sender_info(self, sender):
        """è·å–å‘é€è€…ä¿¡æ¯"""
        if not sender:
            return "æœªçŸ¥ç”¨æˆ·"
        
        for attr in ['id', 'user_id', 'union_id']:
            if hasattr(sender, attr):
                return getattr(sender, attr)
        
        return str(sender)

    def _get_message_content(self, msg):
        """è·å–æ¶ˆæ¯å†…å®¹"""
        if not msg.body:
            return "æ— å†…å®¹"
            
        if msg.msg_type == "text":
            return msg.body.content
        elif msg.msg_type == "system":
            try:
                content_dict = json.loads(msg.body.content)
                return json.dumps(content_dict, ensure_ascii=False)
            except:
                return msg.body.content
        else:
            return f"[{msg.msg_type}] {msg.body.content}"

def main():
    # é…ç½®ä¿¡æ¯
    APP_ID = "cli_a736cea2ff78100d"
    APP_SECRET = "C9FsC6CnJz3CLf0PEz0NQewkuH6uvCdS"
    CHAT_ID = "oc_67aebe4cd9e8dbdccdf292f11eb02e1c"
    DEEPSEEK_API_KEY = "sk-iqpehavwylkwnmxcsbantqhdwceqqhqmvsvlsbtoaegxxaze"
    
    # åˆ›å»ºç›‘æ§å™¨å®ä¾‹
    monitor = FeishuMessageMonitor(APP_ID, APP_SECRET, DEEPSEEK_API_KEY)
    
    # å¼€å§‹ç›‘æ§æ¶ˆæ¯
    monitor.monitor_messages(CHAT_ID, interval=5)

if __name__ == "__main__":
    main() 